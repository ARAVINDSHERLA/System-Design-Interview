* CheatSheet: Data Store In Public Cloud                              :Cloud:
:PROPERTIES:
:type:     database
:export_file_name: cheatsheet-public-db-A4.pdf
:END:

#+BEGIN_HTML
<a href="https://github.com/dennyzhang/cheatsheet.dennyzhang.com/tree/master/cheatsheet-public-db-A4"><img align="right" width="200" height="183" src="https://www.dennyzhang.com/wp-content/uploads/denny/watermark/github.png" /></a>
<div id="the whole thing" style="overflow: hidden;">
<div style="float: left; padding: 5px"> <a href="https://www.linkedin.com/in/dennyzhang001"><img src="https://www.dennyzhang.com/wp-content/uploads/sns/linkedin.png" alt="linkedin" /></a></div>
<div style="float: left; padding: 5px"><a href="https://github.com/dennyzhang"><img src="https://www.dennyzhang.com/wp-content/uploads/sns/github.png" alt="github" /></a></div>
<div style="float: left; padding: 5px"><a href="https://www.dennyzhang.com/slack" target="_blank" rel="nofollow"><img src="https://www.dennyzhang.com/wp-content/uploads/sns/slack.png" alt="slack"/></a></div>
</div>

<br/><br/>
<a href="http://makeapullrequest.com" target="_blank" rel="nofollow"><img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg" alt="PRs Welcome"/></a>
#+END_HTML

- PDF Link: [[https://github.com/dennyzhang/cheatsheet.dennyzhang.com/blob/master/cheatsheet-public-db-A4/cheatsheet-public-db-A4.pdf][cheatsheet-public-db-A4.pdf]], Category: [[https://cheatsheet.dennyzhang.com/category/cloud/][Cloud]]
- Blog URL: https://cheatsheet.dennyzhang.com/cheatsheet-public-db-A4
- Related posts: [[https://cheatsheet.dennyzhang.com/cheatsheet-prometheus-A4][Prometheus CheatSheet]], [[https://cheatsheet.dennyzhang.com/cheatsheet-nagios-A4][Nagios CheatSheet]], [[https://github.com/topics/denny-cheatsheets][#denny-cheatsheets]]

File me [[https://github.com/dennyzhang/cheatsheet.dennyzhang.com/issues][Issues]] or star [[https://github.com/dennyzhang/cheatsheet.dennyzhang.com][this repo]].
** Relation Base Database In Public Cloud
| Name              | Summary                           |
|-------------------+-----------------------------------|
| Open Source RDBMS | Mysql, Mariadb, postgres; AWS RDS |
| Commercial RDBMS  | Oracle, Microsoft SQL Server      |
| Customized RMDB   | AWS Aurora                        |
|                   | Percona XtraDB Cluster            |
** NoSQL Database In Public Cloud
| Name                          | Summary                                       |
|-------------------------------+-----------------------------------------------|
| K/V DB                        | AWS DynamoDB, [[https://cheatsheet.dennyzhang.com/cheatsheet-mongodb-A4][Mongodb]], Cassandra DB, level DB |
| K/V DB - Document based       | [[https://cheatsheet.dennyzhang.com/cheatsheet-mongodb-A4][Mongodb]], Couchbase                            |
| Search engine DB              | Elasticsearch, Lucence, Sphinx                |
| Classic document and BigTable | CouchDB, HBase                                |
| Graph DB                      | Neo4j, OrientDB                               |
| [[https://techcrunch.com/2019/01/09/aws-gives-open-source-the-middle-finger/][AWS DocumentDB]]                | Amazon version of mongodb                     |
| Time series database          | InfluxDB                                      |
|                               | Active directory, LDAP                        |
|                               | Amazon Quantum Ledger Database (QLDB)         |
** Database In Kubernetes
| Name                | Summary                   |
|---------------------+---------------------------|
| Kubernetes Operator | [[https://github.com/operator-framework/awesome-operators][GitHub: awesome-operators]] |
| DB Deployment       | [[https://kubedb.com/][kubedb.com]]                |
** Big Data In Public Cloud
| Name   | Summary |
|--------+---------|
| Hadoop |         |
| Spark  |         |
| Impala |         |

** Caching In Public Cloud
| Name      | Summary                                |
|-----------+----------------------------------------|
| Redis     | In-memory + filesystem support caching |
| Memcached | In-memory caching                      |

** More Resources
https://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis

https://db-engines.com/de/system/Cassandra%3BCouchbase%3BElasticsearch%3BMongoDB%3BRedis
License: Code is licensed under [[https://www.dennyzhang.com/wp-content/mit_license.txt][MIT License]].

#+BEGIN_HTML
<a href="https://cheatsheet.dennyzhang.com"><img align="right" width="201" height="268" src="https://raw.githubusercontent.com/USDevOps/mywechat-slack-group/master/images/denny_201706.png"></a>

<a href="https://cheatsheet.dennyzhang.com"><img align="right" src="https://raw.githubusercontent.com/dennyzhang/cheatsheet.dennyzhang.com/master/images/cheatsheet_dns.png"></a>
#+END_HTML
* org-mode configuration                                           :noexport:
#+STARTUP: overview customtime noalign logdone showall
#+DESCRIPTION:
#+KEYWORDS:
#+LATEX_HEADER: \usepackage[margin=0.6in]{geometry}
#+LaTeX_CLASS_OPTIONS: [8pt]
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage{lastpage}
#+LATEX_HEADER: \usepackage{fancyhdr}
#+LATEX_HEADER: \pagestyle{fancy}
#+LATEX_HEADER: \fancyhf{}
#+LATEX_HEADER: \rhead{Updated: \today}
#+LATEX_HEADER: \rfoot{\thepage\ of \pageref{LastPage}}
#+LATEX_HEADER: \lfoot{\href{https://github.com/dennyzhang/cheatsheet.dennyzhang.com/tree/master/cheatsheet-public-db-A4}{GitHub: https://github.com/dennyzhang/cheatsheet.dennyzhang.com/tree/master/cheatsheet-public-db-A4}}
#+LATEX_HEADER: \lhead{\href{https://cheatsheet.dennyzhang.com/cheatsheet-public-db-A4}{Blog URL: https://cheatsheet.dennyzhang.com/cheatsheet-public-db-A4}}
#+AUTHOR: Denny Zhang
#+EMAIL:  denny@dennyzhang.com
#+TAGS: noexport(n)
#+PRIORITIES: A D C
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_EXCLUDE_TAGS: exclude noexport
#+SEQ_TODO: TODO HALF ASSIGN | DONE BYPASS DELEGATE CANCELED DEFERRED
#+LINK_UP:
#+LINK_HOME:
* #  --8<-------------------------- separator ------------------------>8-- :noexport:
* neo4j                                                            :noexport:
* [#A] redis                                                       :noexport:
集群部署
1.配置文件/etc/redis/redis.conf: 必须要有的配置项: cluster-enabled yes
2.启动方式: /etc/init.d/redis start
3.集群初始化: /usr/local/src/redis-trib.rb create --replicas 1 172.17.0.12:6379 172.17.0.15:6379 172.17.0.17:6379 172.17.0.18:6379 172.17.0.19:6379 172.17.0.20:6379 (此处只支持ip, 且初始化只做一次) 默认前面三个为master, 后面三个为slave

集群添加新的节点	redis-trib.rb add-node $host1:$port1 $host2:$port2 : 把节点$host1加入$host2集群

http://redis.io
** install
apt-get install build-essential

tar -xf redis-3.0.2.tar.gz
wget http://download.redis.io/releases/redis-3.0.2.tar.gz
cd redis-3.0.2
make
** TODO [#A] How the cluster is setup to avoid SPOF?
** run redis
To run Redis with the default configuration just type:

    % cd src
    % ./redis-server

If you want to provide your redis.conf, you have to run it using an additional
parameter (the path of the configuration file):

    % cd src
    % ./redis-server /path/to/redis.conf
It is possible to alter the Redis configuration passing parameters directly
as options using the command line. Examples:

    % ./redis-server --port 9999 --slaveof 127.0.0.1 6379
    % ./redis-server /etc/redis/6379.conf --loglevel debug

Playing with Redis
------------------

You can use redis-cli to play with Redis. Start a redis-server instance,
then in another terminal try the following:

    % cd src
    % ./redis-cli
    redis> ping
    PONG
    redis> set foo bar
    OK
    redis> get foo
    "bar"
    redis> incr mycounter
    (integer) 1
    redis> incr mycounter
    (integer) 2
    redis>

You can find the list of all the available commands here:

http://redis.io/commands
** redis cookbook: https://supermarket.chef.io/cookbooks/redis2
** DONE redis initscript
  CLOSED: [2015-07-21 Tue 00:20]
http://www.linuxidc.com/Linux/2011-10/45184.htm
http://www.cnblogs.com/xsi640/p/3756130.html
http://www.2cto.com/os/201406/307712.html
** [question] 熟悉Redis的索引机制和异步写机制
** web page: 应用监控-Redis状态监控 - 运维社区          
https://www.unixhot.com/article/20
*** webcontent                                                     :noexport:
#+begin_example
Location: https://www.unixhot.com/article/20                                                                                  
[no-js]

你的浏览器禁用了JavaScript, 请开启后刷新浏览器获得更好的体验!

[                    ]

输入关键字进行搜索

搜索:

发起问题
  * 发现
  * 话题
  * 知识库
  * · · ·

登录注册
Zabbix

应用监控-Redis状态监控

Redis可以使用INFO命令,进行状态监控.

以一种易于解释（parse）且易于阅读的格式,返回关于 Redis 服务器的各种信息和统计数值.
通过给定可选的参数 section ,可以让命令只返回某一部分的信息:
    server : 一般 Redis 服务器信息,包含以下域:
            redis_version : Redis 服务器版本
            redis_git_sha1 : Git SHA1
            redis_git_dirty : Git dirty flag
            os : Redis 服务器的宿主操作系统
            arch_bits : 架构（32 或 64 位）
            multiplexing_api : Redis 所使用的事件处理机制
            gcc_version : 编译 Redis 时所使用的 GCC 版本
            process_id : 服务器进程的 PID
            run_id : Redis 服务器的随机标识符（用于 Sentinel 和集群）
            tcp_port : TCP/IP 监听端口
            uptime_in_seconds : 自 Redis 服务器启动以来,经过的秒数
            uptime_in_days : 自 Redis 服务器启动以来,经过的天数
            lru_clock : 以分钟为单位进行自增的时钟,用于 LRU 管理

    clients : 已连接客户端信息,包含以下域:
            connected_clients : 已连接客户端的数量（不包括通过从属服务器连接的客户端）
            client_longest_output_list : 当前连接的客户端当中,最长的输出列表
            client_longest_input_buf : 当前连接的客户端当中,最大输入缓存
            blocked_clients : 正在等待阻塞命令（BLPOP`BRPOP`BRPOPLPUSH）的客户端的数量
    memory : 内存信息,包含以下域:
            used_memory : 由 Redis 分配器分配的内存总量,以字节（byte）为单位
            used_memory_human : 以人类可读的格式返回 Redis 分配的内存总量
            used_memory_rss : 从操作系统的角度,返回 Redis 已分配的内存总量（俗称常驻集大小）.这个
值和 top ` ps 等命令的输出一致.
            used_memory_peak : Redis 的内存消耗峰值（以字节为单位）
            used_memory_peak_human : 以人类可读的格式返回 Redis 的内存消耗峰值
            used_memory_lua : Lua 引擎所使用的内存大小（以字节为单位）
            mem_fragmentation_ratio : used_memory_rss 和 used_memory 之间的比率
            mem_allocator : 在编译时指定的, Redis 所使用的内存分配器.可以是 libc ` jemalloc 或者
tcmalloc .
        在理想情况下, used_memory_rss 的值应该只比 used_memory 稍微高一点儿.
        当 rss > used ,且两者的值相差较大时,表示存在（内部或外部的）内存碎片.
        内存碎片的比率可以通过 mem_fragmentation_ratio 的值看出.
        当 used > rss 时,表示 Redis 的部分内存被操作系统换出到交换空间了,在这种情况下,操作可能会
产生明显的延迟.
        Because Redis does not have control over how its allocations are mapped to memory pages,
high used_memory_rss is often the result of a spike in memory usage.
        当 Redis 释放内存时,分配器可能会,也可能不会,将内存返还给操作系统.
        如果 Redis 释放了内存,却没有将内存返还给操作系统,那么 used_memory 的值可能和操作系统显示
的 Redis 内存占用并不一致.
        查看 used_memory_peak 的值可以验证这种情况是否发生.

    persistence : RDB 和 AOF 的相关信息
    stats : 一般统计信息
    replication : 主/从复制信息
    cpu : CPU 计算量统计信息
    commandstats : Redis 命令统计信息
    cluster : Redis 集群信息
    keyspace : 数据库相关的统计信息

除上面给出的这些值以外,参数还可以是下面这两个:
    all : 返回所有信息
    default : 返回默认选择的信息
当不带参数直接调用 INFO 命令时,使用 default 作为默认参数.
1
分享

  * 微博
  * QZONE
  * 微信

2015-10-29
悠久之翼

1 个评论

朗哥

朗哥

"SEO工具*souyida.net* 搜易达SEO发帖软件*网页权重:
除了以上再次抓取策略外,网页权重也是决定抓取频率的重要因素.用户体验策略在一定程度上反映了网页权重
的影响.在网页类型相同,历史更新频率也差不多的情况下,是权重越高的页面被抓取的频
率越高."搜易达发帖软件"souyida.net
比如百度首页.在搜索引擎Spider的实际作业中,不会单独使用某一种再次抓取策略,而是会综合参考网页的用
户体验,更新频率,网页类型和网页权重.并且对不同类型的页面,着重参考的更新内容主体
也是不同的,比如,列表页只有一篇新的文章进入可能就算更新了,"搜易达SEO软件"souyida.net文章页主体内
容没有变,主体内容周围的所有推荐链接,广告,内容都变了,可能也不会算是更新.

在SEO工作中为了提高某一网站的抓取频率,"搜易达SEO工具"souyida.net一般会重点为该页面导入链接提高权重
.努力加大该页面的更新频率,其实在用户体验和网页类型方面也是有工作可做的,用标题和描述吸引点击不仅
可以提升排
名,也能够间接增加页面被Spider抓取的频率,同时对于不同定位的关键词可以使用不同的网页类型（列表页,
专题页,内容页等）,"搜易达 SEO软件"souyida.net这在设计页面内容和网站架构时就应该仔细地考虑,并且网
页类型这部
分有很多工作值得做.比如,有少网站把整站都做了列表页,全站没有普通意义上的内容页,内容页主体内容下
方或周围也有大量和主题相关的文本内容,一般是类列表形式.不过这种手法有效时间不长,或
者损害用户体验后会降低被抓取的频率,不论怎样,"搜易达发帖软件"souyida.net优秀的网站架构设计应该合理
地利用Spider抓取策略的各种特性.

以上讨论的是Spider正常抓取策略,有抓取并不代表一定有更新,当页面内容的变化值得搜索引擎更新索引时才
会更新,比如,上面提到的文章主体内容但是推荐链接全变了,一般搜索引擎也不会浪费资源做
无意义的更新,当Spider发现已经索引的页面突然被删除,也就是服务器突然返回404状态码时,也会在短时间内
增加对该页面的抓取频率,有的SEO人员就利用这一点增加Spider对自己网站的抓取,并做一些
其他手脚,不过这是一个走钢丝的人小聪明行为,不见得长期有效,"搜易达 SEO软件"souyida.net不值得冒险使
用.

另外,有不少门户网站习惯对即时新闻先发布一个标题,然后再编辑补充内容,甚至还会修改标题,但是这种网
页一般都是文章页,不论从用户体验,更新频率,网页类型和网站权重哪方面来看都不会得到比
较高的抓取频率,也就造成这些网站不断在报怨百度不更新如此操作的新闻网页,百度搜索官方已经表示希望以
后通过百度站长平台来解决一下这个问题,但是作为SEO人员不能只是指望搜索引擎做出什么样的
动作,仔细研究Spider的抓取和更新策略,虽然或许不能将问题解决得那么完美,"搜易达 SEO外链"souyida.net
但是一般都会找到比较适合自己的方法,当然最终还是期望百度官方尽快推出相应的机制或工具来解决这个问题
.

SEO服务热线:13302966002,联系人:彭技术
本文原创首发于:"搜易达外贸SEO"souyida.net
版权所有,转载请保留出处,谢谢
2015-11-12 12:45

                                     要回复文章请先登录或注册                                      

发起人

赵班长
    赵班长
   
    不忘初心,方得始终！
   
擅长话题 :   DevOps jenkins Redis ELKStack

 1. SaltStack中国用户组
 2. 速云科技
 3. 绿肥
 4. 郭冬
 5. 运维进行时
 6. 架构师之路
 7. 徐亮伟
 8. 运维服务
 9. 老男孩
10. chunk
11. 赵班长视频
12. 上海蝎子
13. 安生
14. GitLab中文社区

Copyright © 2016 - 京ICP备12049721号, All Rights Reserved Powered By WeCenter

[piwik]

#+end_example

** HALF redis clustering solution
 https://redis.io/topics/cluster-tutorial

 Every Redis Cluster node requires two TCP connections open.
 - port 6379: serve clients

 - port 16379: data port. This second high port is used for the Cluster
   bus, that is a node-to-node communication channel using a binary
   protocol. The Cluster bus is used by nodes for failure detection,
   configuration update, failover authorization and so forth.
*** DONE [#A] DB scalability: handle too many data: has slots
    CLOSED: [2017-07-03 Mon 22:05]
 https://redis.io/topics/cluster-tutorial
 #+BEGIN_EXAMPLE
 Redis Cluster data sharding
 Redis Cluster does not use consistent hashing, but a different form of sharding where every key is conceptually part of what we call an hash slot.
 There are 16384 hash slots in Redis Cluster, and to compute what is the hash slot of a given key, we simply take the CRC16 of the key modulo 16384.
 Every node in a Redis Cluster is responsible for a subset of the hash slots, so for example you may have a cluster with 3 nodes, where:
 Node A contains hash slots from 0 to 5500.
 Node B contains hash slots from 5501 to 11000.
 Node C contains hash slots from 11001 to 16383.

 Because moving hash slots from a node to another does not require to stop operations, adding and removing nodes, or changing the percentage of hash slots hold by nodes, does not require any downtime.
 #+END_EXAMPLE
*** DONE [#A] DB availability: avoid SPOF: master-slave
    CLOSED: [2017-07-03 Mon 22:05]
 https://redis.io/topics/cluster-tutorial
 #+BEGIN_EXAMPLE
 Redis Cluster master-slave model
 In order to remain available when a subset of master nodes are failing or are not able to communicate with the majority of nodes, Redis Cluster uses a master-slave model where every hash slot has from 1 (the master itself) to N replicas (N-1 additional slaves nodes).
 In our example cluster with nodes A, B, C, if node B fails the cluster is not able to continue, since we no longer have a way to serve hash slots in the range 5501-11000.
 However when the cluster is created (or at a latter time) we add a slave node to every master, so that the final cluster is composed of A, B, C that are masters nodes, and A1, B1, C1 that are slaves nodes, the system is able to continue if node B fails.
 Node B1 replicates B, and B fails, the cluster will promote node B1 as the new master and will continue to operate correctly.
 However note that if nodes B and B1 fail at the same time Redis Cluster is not able to continue to operate.
 #+END_EXAMPLE
*** DONE Redis Cluster is not able to guarantee strong consistency: it uses asynchronous replication.
    CLOSED: [2017-07-03 Mon 22:06]
 https://redis.io/topics/cluster-tutorial
*** DONE Redis Cluster has support for synchronous writes when absolutely needed, implemented via the WAIT command
    CLOSED: [2017-07-03 Mon 22:19]
 https://redis.io/topics/cluster-tutorial
 Redis Cluster has support for synchronous writes when absolutely needed, implemented via the WAIT command, this makes losing writes a lot less likely, however note that Redis Cluster does not implement strong consistency even when synchronous replication is used: it is always possible under more complex failure scenarios that a slave that was not able to receive the write is elected as master.
*** #  --8<-------------------------- separator ------------------------>8--
*** DONE In order to make Docker compatible with Redis Cluster you need to use the host networking mode
    CLOSED: [2017-07-03 Mon 21:55]
 https://redis.io/topics/cluster-tutorial
 #+BEGIN_EXAMPLE
 Currently Redis Cluster does not support NATted environments and in general environments where IP addresses or TCP ports are remapped.
 Docker uses a technique called port mapping: programs running inside Docker containers may be exposed with a different port compared to the one the program believes to be using. This is useful in order to run multiple containers using the same ports, at the same time, in the same server.
 In order to make Docker compatible with Redis Cluster you need to use the host networking mode of Docker. Please check the --net=host option in the Docker documentation for more information.
 #+END_EXAMPLE
*** useful link
 https://redis.io/topics/cluster-tutorial
 https://redis.io/topics/cluster-spec
* [#A] squid: setup proxy server                         :noexport:IMPORTANT:
mac squid logfile: /usr/local/squid/var/logs

export http_proxy='http://192.168.1.190:3128/'

sudo http_proxy='http://user:pass@proxy.example.com:8080/' apt-get install package-name

export http_proxy='http://192.168.50.10:15628/'
export https_proxy='http://192.168.50.10:15628/'
curl -I http://www.baidu.com
curl -I http://www.google.com

| Name                | summary                                                 |
|---------------------+---------------------------------------------------------|
| Install package     | sudo apt-get install squid                              |
| squid conf          | /etc/squid3/squid.conf                                  |
| process             | /usr/sbin/squid3                                        |
| logfile             | /var/log/squid3/access.log                              |
| restart             | /etc/init.d/squid3 restart                              |
| http proxy password | sudo htdigest -c /etc/squid3/passwords proxy dennysquid |
| Start squid daemon  | /usr/sbin/squid3 -f /etc/squid3/squid.conf              |
|---------------------+---------------------------------------------------------|
| mac squid logfile   | /usr/local/squid/var/logs                               |
| mac squid cache     | /usr/local/squid/var/cache                              |

./configure --with-large-files

# maximum_object_size 4096 KB

1G: 1024*1024=1048576

ls -lth  /usr/local/squid/var/logs
sudo tail -f  /usr/local/squid/var/logs/access.log
sudo ls -lth /usr/local/squid/var/cache
cat /usr/local/squid/etc/squid.conf
cat /etc/squid3/squid.conf

sudo launchctl load -w /Library/LaunchDaemons/squid.plist

sudo ls -lth  /usr/local/squid/var/cache/00/00/
** package to confirm squid works
export http_proxy=http://172.17.0.68:3128
export https_proxy=http://172.17.0.68:3128

wget http://apache.cs.utah.edu/tomcat/tomcat-8/v8.0.23/bin/apache-tomcat-8.0.23.tar.gz
wget http://archive.ubuntu.com/ubuntu/pool/main/v/vim/vim_7.4.052-1ubuntu3_amd64.deb
wget http://tsung.erlang-projects.org/dist/tsung-1.4.2.tar.gz
** DONE [#A] squid: flush cache                                   :IMPORTANT:
  CLOSED: [2014-11-22 Sat 12:21]
grep cache_dir /etc/squid3/squid.conf

service squid3 stop
ps -ef | grep squid
ls -lth /var/spool/squid3/ | head
rm -rf /var/spool/squid3/*
ls -lth /var/spool/squid3/ | head
# create cache_dir
squid3 -z
ls -lth /var/spool/squid3/ | head
service squid3 start
http://www.cyberciti.biz/faq/linux-unix-clearing-squid-proxy-cache-and-recreate-dirs/

http://wiki.squid-cache.org/SquidFaq/OperatingSquid#head-23466fef7b7d2e1e43f4a3b83564029116e1faef
** DONE test squid
   CLOSED: [2015-03-24 Tue 11:45]
export http_proxy='http://192.168.1.167:3128/'

curl http://www.google.com

tail -f /usr/local/squid/var/logs/access.log

#+BEGIN_EXAMPLE
macs-MacBook-Air:~ mac$ curl -I http://www.google.com
HTTP/1.0 200 OK
Expires: -1
Date: Tue, 24 Mar 2015 15:44:24 GMT
Content-Type: text/html; charset=ISO-8859-1
Server: gws
Accept-Ranges: none
Cache-Control: private, max-age=0
Set-Cookie: PREF=ID=98c927babb2cc4e4:FF=0:TM=1427211864:LM=1427211864:S=qGgtfP7KrsGKuac2; expires=Thu, 23-Mar-2017 15:44:24 GMT; path=/; domain=.google.com
Set-Cookie: NID=67=HhOrD89itB8hkkxrcuWgecGDmt0KXfTtI-YdikmuJlp4G2zEvrf3FG8_otkLnZXnyMXEqbkSB2huetUX7XsbgIT7MNx9gmwZ4Y4gQqby8HDrwNsB7AfvExesWWgS6gJF; expires=Wed, 23-Sep-2015 15:44:24 GMT; path=/; domain=.google.com; HttpOnly
P3P: CP="This is not a P3P policy! See http://www.google.com/support/accounts/bin/answer.py?hl=en&answer=151657 for more info."
X-XSS-Protection: 1; mode=block
X-Frame-Options: SAMEORIGIN
Alternate-Protocol: 80:quic,p=0.5
Vary: Accept-Encoding
X-Cache: MISS from macs-macbook-air.local
Via: 1.1 AZRAEL, 1.1 macs-macbook-air.local:3128 (squid/2.7.STABLE9)
Connection: keep-alive
Proxy-Connection: keep-alive
#+END_EXAMPLE
** install squid
apt-get install squid

Change: /etc/squid3/squid.conf
acl localnet src 10.0.0.0/8	# RFC1918 possible internal network
acl localnet src 172.16.0.0/12	# RFC1918 possible internal network
acl localnet src 192.168.0.0/16	# RFC1918 possible internal network
acl localnet src 172.17.0.0/16	# docker possible internal network
acl localnet src fc00::/7       # RFC 4193 local private network range
acl localnet src fe80::/10      # RFC 4291 link-local (directly plugged) machines

acl all src all
http_access allow all

restart squid:
/etc/init.d/squid3 restart

Test:
export http_proxy='http://192.168.1.184:3128/'
export https_proxy='http://192.168.1.184:3128/'
curl -I http://www.google.com

tail -f /var/log/squid3/access.log
** #  --8<-------------------------- separator ------------------------>8--
** BYPASS squid proxy can't resolve dns: 重启一下又好了
  CLOSED: [2013-12-28 Sat 09:49]
#+begin_example
bash-3.2$ diff /usr/local/squid/etc/squid.conf		/usr/local/squid/etc/squid.conf.default
2295c2295
< # log_fqdn on
---
> # log_fqdn off
5000,5007d4999
< # Add this to the auth_param section
< auth_param basic program /usr/local/squid/libexec/ncsa_auth /usr/local/squid/etc/squid_passwd
<
< # Add this to the bottom of the ACL section
< acl ncsa_users proxy_auth REQUIRED
<
< # Add this at the top of the http_access section
< http_access allow ncsa_users
bash-3.2$
#+end_example

#+begin_example
➜  /tmp  curl -I http://www.rarlab.com/rar/rarlinux-3.8.0.tar.gz


HTTP/1.0 504 Gateway Time-out
Server: squid/2.7.STABLE9
Date: Sat, 28 Dec 2013 15:25:00 GMT
Content-Type: text/html
Content-Length: 1184
X-Squid-Error: ERR_DNS_FAIL 0
X-Cache: MISS from localhost
Via: 1.0 localhost:3128 (squid/2.7.STABLE9)
Connection: close

➜  /tmp  curl -I http://192.168.1.190
HTTP/1.0 200 OK
Date: Sat, 28 Dec 2013 15:26:47 GMT
Server: Apache/2.2.24 (Unix) DAV/2 mod_ssl/2.2.24 OpenSSL/0.9.8y
Content-Location: index.html.en
Vary: negotiate
TCN: choice
Last-Modified: Thu, 24 Oct 2013 20:25:51 GMT
ETag: "fd8be0-2c-4e9827099f1c0"
Accept-Ranges: bytes
Content-Length: 44
Content-Type: text/html
Content-Language: en
Age: 14
X-Cache: HIT from localhost
Via: 1.1 localhost:3128 (squid/2.7.STABLE9)
Connection: keep-alive
Proxy-Connection: keep-alive

#+end_example
** 让wget用squid
** 让yum/apt-get用squid
http://www.centos.org/docs/5/html/yum/sn-yum-proxy-server.html
If you define a proxy server in /etc/yum.conf, all users connect to the proxy server with those details when using yum.

#+begin_example
# The Web proxy server, with the username and password for this account
http_proxy="http://yum-user:qwerty@mycache.mydomain.com:3128"
export http_proxy
#+end_example
** 让apt-get用squid
http://itkia.com/using-squid-to-cache-apt-updates-for-debian-and-ubuntu/
http://askubuntu.com/questions/89437/how-to-install-packages-with-apt-get-on-a-system-connected-via-proxy

#+begin_example
/etc/apt/apt.conf.d/ :

Acquire {
        Retries "0";
        HTTP {
                Proxy "http://address-or-URL-of-squid-proxy.example.tld:3128/";
        };
};
#+end_example

#+begin_example
check the file /etc/apt/apt.conf

The contents were,

Acquire::http::proxy "http://<proxy>:<port>/";
Acquire::ftp::proxy "ftp://<proxy>:<port>/";
Acquire::https::proxy "https://<proxy>:<port>/";
#+end_example
** DONE install squid on mac OSX step by step
   CLOSED: [2015-02-27 Fri 00:15]

mkdir -p /usr/local/src/squid
cd /usr/local/src/squid
wget http://www.squid-cache.org/Versions/v2/2.7/squid-2.7.STABLE9.tar.gz
tar -xf squid-2.7.STABLE9.tar.gz
cd ./squid-2.7.STABLE9/
./configure
make
sudo make install
sudo chown -R nobody /usr/local/squid/var
sudo /usr/local/squid/sbin/squid -z
cd /usr/local/squid/etc

sudo touch squid_passwd
sudo chmod o+r squid_passwd
sudo htpasswd squid_passwd denny

* Active directory                                                 :noexport:
- domain, schema, and configuration
** Active directory VS Ldap
Active Directory isn't just an implementation of LDAP by Microsoft,
that is only a small part of what AD is. Active Directory is (in an
overly simplified way) a service that provides LDAP based
authentication with Kerberos based Authorization

LDAP is a protocol specification for directory data.

Active Directory is Microsoft's Implementation of an LDAP based directory server.

AD also has custom extensions ontop of the LDAP v3 spec such as account lockout, password expiration, etc.

LDAP is a standard, AD is Microsoft's (proprietary) implementation (and more).
http://stackoverflow.com/questions/663402/what-are-the-differences-between-ldap-and-active-directory

ftp://ftp.uni-duisburg.de/LDAP/Adam-Eval1-0.pdf

http://www.differencebetween.net/technology/difference-between-ldap-and-acitve-directory/

* ldap                                                             :noexport:
** TODO [#A] ldap server: 389 directory
http://directory.fedoraproject.org
https://www.rosehosting.com/blog/how-to-install-ldap-389-directory-server-on-a-centos-6-vps/
https://github.com/RiotGamesCookbooks/dirsrv-cookbook
*** install 389 directory in ubuntu
http://novint.blogspot.com/2013/10/installation-of-389-directory-server-on.html

http://directory.fedoraproject.org/docs/389ds/howto/howto-debianubuntu.html

ssh root@mdmlab

docker run -t -d --privileged -p 5022:22 -p 1389:1389 denny/sshd:latest /usr/sbin/sshd -D
ssh -p 5022 root@127.0.0.1

sudo apt-get install -y software-properties-common

sudo add-apt-repository ppa:ubuntu-389-directory-server/ppa

sudo apt-get update

sudo apt-get install -y 389-admin 389-ds-base 389-ds-console

cat > /etc/hosts << EOF
172.17.1.63 e1921f1bd293.test.com
127.0.0.1 localhost
::1 localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
EOF

cat > /etc/hostname <<EOF
e1921f1bd293.test.com
EOF

hostname -F /etc/hostname

sudo /usr/sbin/setup-ds-admin
*** DONE [#A] setup 389 directory
  CLOSED: [2015-08-03 Mon 13:08]
ldap.test.com

1389

password1
dc=jingantech,dc=com

#+BEGIN_EXAMPLE
root@66b8e1be75ba:~# sudo /usr/sbin/setup-ds-admin
sudo /usr/sbin/setup-ds-admin
sysctl: cannot stat /proc/sys/net/ipv4/tcp_keepalive_time: No such file or directory

==============================================================================
This program will set up the 389 Directory and Administration Servers.

It is recommended that you have "root" privilege to set up the software.
Tips for using this program:
  - Press "Enter" to choose the default and go to the next screen
  - Type "Control-B" then "Enter" to go back to the previous screen
  - Type "Control-C" to cancel the setup program

Would you like to continue with set up? [yes]:


==============================================================================
Your system has been scanned for potential problems, missing patches,
etc.  The following output is a report of the items found that need to
be addressed before running this software in a production
environment.

389 Directory Server system tuning analysis version 23-FEBRUARY-2012.

NOTICE : System is x86_64-unknown-linux3.13.0-32-generic (4 processors).

NOTICE : /sbin/sysctl -n net.ipv4.tcp_keepalive_time failed
Would you like to continue? [yes]:


==============================================================================
Choose a setup type:

   1. Express
       Allows you to quickly set up the servers using the most
       common options and pre-defined defaults. Useful for quick
       evaluation of the products.

   2. Typical
       Allows you to specify common defaults and options.

   3. Custom
       Allows you to specify more advanced options. This is
       recommended for experienced server administrators only.

To accept the default shown in brackets, press the Enter key.

Choose a setup type [2]:


==============================================================================
Enter the fully qualified domain name of the computer
on which you're setting up server software. Using the form
<hostname>.<domainname>
Example: eros.example.com.

To accept the default shown in brackets, press the Enter key.

Warning: This step may take a few minutes if your DNS servers
can not be reached or if DNS is not configured correctly.  If
you would rather not wait, hit Ctrl-C and run this program again
with the following command line option to specify the hostname:

    General.FullMachineName=your.hostname.domain.name

Computer name [66b8e1be75ba]: ldap.test.com
ldap.test.com

==============================================================================
The servers must run as a specific user in a specific group.
It is strongly recommended that this user should have no privileges
on the computer (i.e. a non-root user).  The setup procedure
will give this user/group some permissions in specific paths/files
to perform server-specific operations.

If you have not yet created a user and group for the servers,
create this user and group using your native operating
system utilities.

System User [dirsrv]:

System Group [dirsrv]:


==============================================================================
Server information is stored in the configuration directory server.
This information is used by the console and administration server to
configure and manage your servers.  If you have already set up a
configuration directory server, you should register any servers you
set up or create with the configuration server.  To do so, the
following information about the configuration server is required: the
fully qualified host name of the form
<hostname>.<domainname>(e.g. hostname.example.com), the port number
(default 389), the suffix, the DN and password of a user having
permission to write the configuration information, usually the
configuration directory administrator, and if you are using security
(TLS/SSL).  If you are using TLS/SSL, specify the TLS/SSL (LDAPS) port
number (default 636) instead of the regular LDAP port number, and
provide the CA certificate (in PEM/ASCII format).

If you do not yet have a configuration directory server, enter 'No' to
be prompted to set up one.

Do you want to register this software with an existing
configuration directory server? [no]:


==============================================================================
Please enter the administrator ID for the configuration directory
server.  This is the ID typically used to log in to the console.  You
will also be prompted for the password.

Configuration directory server
administrator ID [admin]:

Password:
Password (confirm): password1


==============================================================================
The information stored in the configuration directory server can be
separated into different Administration Domains.  If you are managing
multiple software releases at the same time, or managing information
about multiple domains, you may use the Administration Domain to keep
them separate.

If you are not using administrative domains, press Enter to select the
default.  Otherwise, enter some descriptive, unique name for the
administration domain, such as the name of the organization
responsible for managing the domain.

Administration Domain [test.com]:


==============================================================================
The standard directory server network port number is 389.  However, if
you are not logged as the superuser, or port 389 is in use, the
default value will be a random unused port number greater than 1024.
If you want to use port 389, make sure that you are logged in as the
superuser, that port 389 is not in use.

Directory server network port [389]: 1389
1389

==============================================================================
Each instance of a directory server requires a unique identifier.
This identifier is used to name the various
instance specific files and directories in the file system,
as well as for other uses as a server instance identifier.

Directory server identifier [ldap]:


==============================================================================
The suffix is the root of your directory tree.  The suffix must be a valid DN.
It is recommended that you use the dc=domaincomponent suffix convention.
For example, if your domain is example.com,
you should use dc=example,dc=com for your suffix.
Setup will create this initial suffix for you,
but you may have more than one suffix.
Use the directory server utilities to create additional suffixes.

Suffix [dc=test, dc=com]: dc=jingantech,dc=com
dc=jingantech,dc=com

==============================================================================
Certain directory server operations require an administrative user.
This user is referred to as the Directory Manager and typically has a
bind Distinguished Name (DN) of cn=Directory Manager.
You will also be prompted for the password for this user.  The password must
be at least 8 characters long, and contain no spaces.
Press Control-B or type the word "back", then Enter to back up and start over.

Directory Manager DN [cn=Directory Manager]:

Password:
Password (confirm): password1


==============================================================================
The Administration Server is separate from any of your web or application
servers since it listens to a different port and access to it is
restricted.

Pick a port number between 1024 and 65535 to run your Administration
Server on. You should NOT use a port number which you plan to
run a web or application server on, rather, select a number which you
will remember and which will not be used for anything else.

Administration port [9830]:


==============================================================================
The interactive phase is complete.  The script will now set up your
servers.  Enter No or go Back if you want to change something.

Are you ready to set up your servers? [yes]:

Creating directory server . . .
Your new DS instance 'ldap' was successfully created.
Creating the configuration directory server . . .
Beginning Admin Server creation . . .
Creating Admin Server files and directories . . .
Updating adm.conf . . .
Updating admpw . . .
Registering admin server with the configuration directory server . . .
Updating adm.conf with information from configuration directory server . . .
Updating the configuration for the httpd engine . . .
Starting admin server . . .
output: AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 172.17.0.13. Set the 'ServerName' directive globally to suppress this message
The admin server was successfully started.
Admin server was successfully created, configured, and started.
Exiting . . .
Log file is '/tmp/setupsagcRK.log'
#+END_EXAMPLE
*** DONE [#B] 389 directory setup-ds-admin: slient setup inf configuration file
  CLOSED: [2015-08-03 Mon 17:21]
http://www.centos.org/docs/5/html/CDS/install/8.0/Installation_Guide-Advanced_Configuration-Silent.html

http://www.centos.org/docs/5/html/CDS/install/8.0/Installation_Guide-about-setup-ds-admin.pl.html

ssh -p 4022 root@50.198.76.249
# docker server
ssh -i /home/denny/denny root@192.168.1.185
tmux attach
ssh -p 33095 root@127.0.0.1

lsof -i tcp:1389
rm -rf /etc/dirsrv/slapd-ldap

/usr/sbin/setup-ds-admin --silent --file=ldap_setup.ini
*** DONE connect to 389 directory server by apple studio
  CLOSED: [2015-08-03 Mon 14:44]
https://directory.apache.org/studio/

123.57.240.189
1389
*** TODO ldapmodify
ldapmodify: wrong attributeType at line 11, entry "cn=encryption,cn=config"

#+BEGIN_EXAMPLE
ldapmodify -x -h localhost -p $ldapport -D "cn=directory manager" -W "password1" <<EOF
dn: cn=encryption,cn=config
changetype: modify
replace: nsSSL3
nsSSL3: on
-
replace: nsSSLClientAuth
nsSSLClientAuth: allowed
-
add: nsSSL3Ciphers
nsSSL3Ciphers: -rsa_null_md5,+rsa_rc4_128_md5,+rsa_rc4_40_md5,+rsa_rc2_40_md5,
 +rsa_des_sha,+rsa_fips_des_sha,+rsa_3des_sha,+rsa_fips_3des_sha,+fortezza,
 +fortezza_rc4_128_sha,+fortezza_null,+tls_rsa_export1024_with_rc4_56_sha,
 +tls_rsa_export1024_with_des_cbc_sha

dn: cn=config
changetype: modify
add: nsslapd-security
nsslapd-security: on
-
replace: nsslapd-ssl-check-hostname
nsslapd-ssl-check-hostname: off
-
replace: nsslapd-secureport
nsslapd-secureport: 636

dn: cn=RSA,cn=encryption,cn=config
changetype: add
objectclass: top
objectclass: nsEncryptionModule
cn: RSA
nsSSLPersonalitySSL: Server-Cert
nsSSLToken: internal (software)
nsSSLActivation: on

EOF
#+END_EXAMPLE
** TODO [#A] ldap: 389 directory: service fail to start, after machine reboot
** #  --8<-------------------------- separator ------------------------>8--
** [#B] OpenDJ project: open source directory services
http://opendj.forgerock.org

| Name                          | Summary      |
|-------------------------------+--------------|
| cd ./opendj; ./setup --cli    | setup        |
| /usr/local/opendj/bin/status  | check status |
| /usr/local/opendj/bin/stop-ds |              |
| /usr/local/ldap/errors        |              |
*** TODO root user DN??
What would you like to use as the initial root user DN for the Directory
Server? [cn=Directory Manager]:
*** install
http://opendj.forgerock.org/opendj-server/doc/bootstrap/install-guide/index.html

#+BEGIN_EXAMPLE
root@a3c83d971fd8:~/opendj# ./setup --cli
./setup --cli
READ THIS SOFTWARE LICENSE AGREEMENT CAREFULLY. BY DOWNLOADING OR INSTALLING
THE FORGEROCK SOFTWARE, YOU, ON BEHALF OF YOURSELF AND YOUR COMPANY, AGREE TO
BE BOUND BY THIS SOFTWARE LICENSE AGREEMENT. IF YOU DO NOT AGREE TO THESE
TERMS, DO NOT DOWNLOAD OR INSTALL THE FORGEROCK SOFTWARE.

1. Software License.

1.1. Development Right to Use. If Company intends to or does use the ForgeRock
Software only for the purpose(s) of developing, testing, prototyping and
demonstrating its application software, then ForgeRock hereby grants Company a
nonexclusive, nontransferable, limited license to use the ForgeRock Software
only for those purposes, solely at Company???s facilities and only in a
non-production environment. ForgeRock may audit Company???s use of the ForgeRock
Software to confirm that a production license is not required upon reasonable
written notice to Company. If Company intends to use the ForgeRock Software in
a live environment, Company must purchase a production license and may only use
the ForgeRock Software licensed thereunder in accordance with the terms and
conditions of that subscription agreement.

1.2. Restrictions. Except as expressly set forth in this ForgeRock Software
License Agreement (the ???Agreement???), Company shall not, directly or indirectly:
(a) sublicense, resell, rent, lease, distribute or otherwise transfer rights or
usage in the ForgeRock Software, including without limitation to Company
subsidiaries and affiliates; (b) remove or alter any copyright, trademark or
proprietary notices in the ForgeRock Software; or (c) use the ForgeRock
Software in any way that would subject the ForgeRock Software, in whole in or
in part, to a Copyleft License. As used herein, ???Copyleft License??? means a
software license that requires that information necessary for reproducing and
modifying such software must be made available publicly to recipients of
executable versions of such software (see, e.g., GNU General Public License and
http://www.gnu.org/copyleft/).

2. Proprietary Rights.

2.1. ForgeRock Intellectual Property. Title to and ownership of all copies of
the ForgeRock Software whether in machine-readable (source, object code or
other format) or printed form, and all related technical know-how and all
rights therein (including without limitation all intellectual property rights
applicable thereto), belong to ForgeRock and its licensors and shall remain the
exclusive property thereof. ForgeRock???s name, logo, trade names and trademarks
are owned exclusively by ForgeRock and no right is granted to Company to use
any of the foregoing except as expressly permitted herein. All rights not
expressly granted to Company are reserved by ForgeRock and its licensors.

2.2. Suggestions. Company hereby grants to ForgeRock a royalty-free, worldwide,
transferable, sublicensable and irrevocable right and license to use, copy,
modify and distribute, including by incorporating into any product or service
owned by ForgeRock, any suggestions, enhancements, recommendations or other
feedback provided by Company relating to any product or service owned or
offered by ForgeRock.

2.3. Source Code. The source code underlying the ForgeRock Software is
available at www.forgerock.org.

3. Term and Termination. The terms of this Agreement shall commence on the
Effective Date and shall continue in force unless earlier terminated in
accordance this Section. This Agreement shall terminate without notice to
Company in the event Company is in material breach of any of the terms and
conditions of this Agreement. As used herein, ???Effective Date??? means the date
on which Company first accepted this Agreement and downloads the ForgeRock
Software.

4. Disclaimer of Warranties. THE FORGEROCK SOFTWARE LICENSED HEREUNDER IS
LICENSED ???AS IS??? AND WITHOUT WARRANTY OF ANY KIND. FORGEROCK AND IT???S LICENSORS
EXPRESSLY DISCLAIM ALL WARRANTIES, WHETHER EXPRESS, IMPLIED OR STATUTORY,
INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND ANY WARRANTY OF NON-INFRINGEMENT.

5. General Indemnification. Company shall defend, indemnify and hold ForgeRock
harmless from and against any and all liabilities, damages, losses, costs and
expenses (including but not limited to reasonable fees of attorneys and other
professionals) payable to third parties based upon any claim arising out of or
related to the use of Company???s products, provided that ForgeRock: (a) promptly
notifies Company of the claim; (b) provides Company with all reasonable
information and assistance, at Company's expense, to defend or settle such a
claim; and (c) grants Company authority and control of the defense or
settlement of such claim. Company shall not settle any such claim, without
ForgeRock's prior written consent, if such settlement would in any manner
effect ForgeRock's rights in the ForgeRock Software or otherwise. ForgeRock
reserves the right to retain counsel, at ForgeRock's expense, to participate in
the defense and settlement of any such claim.

6. Limitation of Liability. IN NO EVENT SHALL FORGEROCK BE LIABLE FOR THE COST
OF PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES, ANY LOST PROFITS, REVENUE, OR
DATA, INTERRUPTION OF BUSINESS OR FOR ANY INCIDENTAL, SPECIAL, CONSEQUENTIAL OR
INDIRECT DAMAGES OF ANY KIND, AND WHETHER ARISING OUT OF BREACH OF WARRANTY,
BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE,
EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE OR IF SUCH DAMAGE COULD HAVE
BEEN REASONABLY FORESEEN. IN NO EVENT SHALL FORGEROCK???S LIABILITY ARISING OUT
OF OR RELATED TO THIS AGREEMENT WHETHER IN CONTRACT, TORT OR UNDER ANY OTHER
THEORY OF LIABILITY, EXCEED IN THE AGGREGATE $1,000 USD.

7. General.

7.1. Governing Law. This Agreement shall be governed by and interpreted in
accordance with the laws of the State of California without reference to its
conflicts of law provisions.

7.2. Assignment. Company may not assign any of its rights or obligations under
this Agreement without the prior written consent of ForgeRock, which consent
shall not be unreasonably withheld. Any assignment not in conformity with this
Section shall be null and void.

7.3. Waiver. A waiver on one occasion shall not be construed as a waiver of any
right on any future occasion. No delay or omission by a party in exercising any
of its rights hereunder shall operate as a waiver of such rights.

7.4. Compliance with Law. The ForgeRock Software is subject to U.S. export
control laws, including the U.S. Export Administration Act and its associated
regulations, and may be subject to export or import regulations in other
countries. Company agrees to comply with all laws and regulations of the United
States and other countries (???Export Laws???) to assure that neither the ForgeRock
Software, nor any direct products thereof are; (a) exported, directly or
indirectly, in violation of Export Laws, either to any countries that are
subject to U.S. export restrictions or to any end user who has been prohibited
from participating in the U.S. export transactions by any federal agency of the
U.S. government or (b) intended to be used for any purpose prohibited by Export
Laws, including, without limitation, nuclear, chemical, or biological weapons
proliferation.

7.5. US Government Restrictions. Company acknowledges that the ForgeRock
Software consists of ???commercial computer software??? and ???commercial computer
software documentation??? as such terms are defined in the Code of Federal
Regulations. No Government procurement regulations or contract clauses or
provisions shall be deemed a part of any transaction between the parties unless
its inclusion is required by law, or mutually agreed in writing by the parties
in connection with a specific transaction. Use, duplication, reproduction,
release, modification, disclosure or transfer of the ForgeRock Software is
restricted in accordance with the terms of this Agreement.

7.6. Provision Severability. In the event that it is determined by a court of
competent jurisdiction that any provision of this Agreement is invalid,
illegal, or otherwise unenforceable, such provision shall be enforced as nearly
as possible in accordance with the stated intention of the parties, while the
remainder of this Agreement shall remain in full force and effect and bind the
parties according to its terms. To the extent any provision cannot be enforced
in accordance with the stated intentions of the parties, such terms and
conditions shall be deemed not to be a part of this Agreement.

7.7. Entire Agreement. This Agreement constitutes the entire and exclusive
agreement between the parties with respect to the subject matter hereof and
supersede any prior agreements between the parties with respect to such subject
matter


Please read the License Agreement above.
You must accept the terms of the agreement before continuing with the
installation.
Accept the license (Yes/No) [No]:Yes
Yes

What would you like to use as the initial root user DN for the Directory
Server? [cn=Directory Manager]:

Please provide the password to use for the initial root user: password1

Please re-enter the password for confirmation:

Provide the fully-qualified directory server host name that will be used when
generating self-signed certificates for LDAP SSL/StartTLS, the administration
connector, and replication [a3c83d971fd8]:


On which port would you like the Directory Server to accept connections from
LDAP clients? [389]:


On which port would you like the Administration Connector to accept
connections? [4444]:


Do you want to create base DNs in the server? (yes / no) [yes]: no
no

Do you want to enable SSL? (yes / no) [no]: no
no

Do you want to enable Start TLS? (yes / no) [no]: no
no

Do you want to start the server when the configuration is completed? (yes /
no) [yes]: yes
yes


Setup Summary
=============
LDAP Listener Port:            389
Administration Connector Port: 4444
LDAP Secure Access:            disabled
Root User DN:                  cn=Directory Manager
Directory Data:                Do not Create a Base DN

Start Server when the configuration is completed


What would you like to do?

    1)  Set up the server with the parameters above
    2)  Provide the setup parameters again
    3)  Print equivalent non-interactive command-line
    4)  Cancel and exit

Enter choice [1]: 1
1

See /tmp/opendj-setup-2250796791804245225.log for a detailed log of this operation.

Configuring Directory Server ..... Done.
Starting Directory Server ....................................................................
#+END_EXAMPLE
*** TODO If you plan to install OpenDJ DSML gateway or OpenDJ REST LDAP gateway, make sure you have an appropriate application server installed.
http://opendj.forgerock.org/opendj-server/doc/bootstrap/install-guide/index.html#chap-install-cli
*** TODO fail to check status
#+BEGIN_EXAMPLE
root@cf5ef4413d3b:~/opendj# ./bin/status
./bin/status
Exception in thread "main" java.lang.NoClassDefFoundError: Could not initialize class org.opends.server.util.Platform
	at org.opends.admin.ads.util.ApplicationTrustManager.<init>(ApplicationTrustManager.java:118)
	at org.opends.guitools.controlpanel.datamodel.ControlPanelInfo.getInstance(ControlPanelInfo.java:148)
	at org.opends.server.tools.status.StatusCli.execute(StatusCli.java:329)
	at org.opends.server.tools.status.StatusCli.mainCLI(StatusCli.java:264)
	at org.opends.server.tools.status.StatusCli.main(StatusCli.java:191)
#+END_EXAMPLE
*** TODO fail to start directory server
https://community.oracle.com/thread/3550651
https://forgerock.org/topic/opendj-2-6-does-not-compatible-with-java-8/

#+BEGIN_EXAMPLE
root@a3c83d971fd8:~# tail -n 100 /tmp/opendj-setup-2250796791804245225.log
tail -n 100 /tmp/opendj-setup-2250796791804245225.log
INFO: copying file '/root/opendj/template/config/messages/account-permanently-locked.template' to '/root/opendj/./config/messages/account-permanently-locked.template'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/messages/password-changed.template' to '/root/opendj/./config/messages/password-changed.template'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/admin-backend.ldif' to '/root/opendj/./config/admin-backend.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/java.properties' to '/root/opendj/./config/java.properties'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/01-pwpolicy.ldif' to '/root/opendj/./config/schema/01-pwpolicy.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/03-rfc2714.ldif' to '/root/opendj/./config/schema/03-rfc2714.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/03-rfc2713.ldif' to '/root/opendj/./config/schema/03-rfc2713.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/05-rfc4876.ldif' to '/root/opendj/./config/schema/05-rfc4876.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/05-samba.ldif' to '/root/opendj/./config/schema/05-samba.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/03-rfc2926.ldif' to '/root/opendj/./config/schema/03-rfc2926.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/03-uddiv3.ldif' to '/root/opendj/./config/schema/03-uddiv3.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/00-core.ldif' to '/root/opendj/./config/schema/00-core.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/06-compat.ldif' to '/root/opendj/./config/schema/06-compat.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/03-rfc3112.ldif' to '/root/opendj/./config/schema/03-rfc3112.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/03-rfc2739.ldif' to '/root/opendj/./config/schema/03-rfc2739.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/03-changelog.ldif' to '/root/opendj/./config/schema/03-changelog.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/02-config.ldif' to '/root/opendj/./config/schema/02-config.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/04-rfc2307bis.ldif' to '/root/opendj/./config/schema/04-rfc2307bis.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/05-solaris.ldif' to '/root/opendj/./config/schema/05-solaris.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/root/opendj/template/config/schema/03-rfc3712.ldif' to '/root/opendj/./config/schema/03-rfc3712.ldif'
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.Utils supportsOption
INFO: Checking if options -Xms8m -client are supported with java home: /usr/lib/jvm/jdk1.8.0_40/jre
Jun 26, 2015 5:41:44 AM org.opends.quicksetup.util.Utils supportsOption
INFO: launching [/root/opendj/lib/_script-util.sh] with env: {PATH=/bin:/usr/bin, SCRIPT_NAME_ARG=-Dorg.opends.server.scriptName=setup, LESSCLOSE=/usr/bin/lesspipe %s %s, OPENDJ_JAVA_HOME=/usr/lib/jvm/jdk1.8.0_40/jre, JAVA_HOME=/usr/lib/jvm/java-8-oracle-amd64, TERM=dumb, XFILESEARCHPATH=/usr/dt/app-defaults/%L/Dt, OPENDJ_JAVA_ARGS=-Xms8m -client, SCRIPT_UTIL_CMD=set-full-environment-and-test-java, INSTALL_ROOT=/root/opendj, MAIL=/var/mail/root, INSTANCE_ROOT=/root/opendj, LD_LIBRARY_PATH=, SCRIPT_NAME=setup, LOGNAME=root, LD_PRELOAD_64=, PWD=/root/opendj, LD_LIBRARY_PATH_64=, _=./setup, SHELL=/bin/bash, LESSOPEN=| /usr/bin/lesspipe %s, OLDPWD=/root/opendj, USER=root, CLASSPATH=/root/opendj/classes:/root/opendj/resources/*.jar:/root/opendj/lib/bootstrap.jar, NLSPATH=/usr/dt/lib/nls/msg/%L/%N.cat, LD_PRELOAD=, LD_PRELOAD_32=, LS_COLORS=, HOME=/root, SHLVL=1, LD_LIBRARY_PATH_32=}
Jun 26, 2015 5:41:45 AM org.opends.quicksetup.util.Utils supportsOption
INFO: returnCode: 0
Jun 26, 2015 5:41:45 AM org.opends.quicksetup.util.Utils supportsOption
INFO: supported: true
Jun 26, 2015 5:41:45 AM org.opends.quicksetup.util.Utils supportsOption
INFO: Checking if options -server are supported with java home: /usr/lib/jvm/jdk1.8.0_40/jre
Jun 26, 2015 5:41:45 AM org.opends.quicksetup.util.Utils supportsOption
INFO: launching [/root/opendj/lib/_script-util.sh] with env: {PATH=/bin:/usr/bin, SCRIPT_NAME_ARG=-Dorg.opends.server.scriptName=setup, LESSCLOSE=/usr/bin/lesspipe %s %s, OPENDJ_JAVA_HOME=/usr/lib/jvm/jdk1.8.0_40/jre, JAVA_HOME=/usr/lib/jvm/java-8-oracle-amd64, TERM=dumb, XFILESEARCHPATH=/usr/dt/app-defaults/%L/Dt, OPENDJ_JAVA_ARGS=-server, SCRIPT_UTIL_CMD=set-full-environment-and-test-java, INSTALL_ROOT=/root/opendj, MAIL=/var/mail/root, INSTANCE_ROOT=/root/opendj, LD_LIBRARY_PATH=, SCRIPT_NAME=setup, LOGNAME=root, LD_PRELOAD_64=, PWD=/root/opendj, LD_LIBRARY_PATH_64=, _=./setup, SHELL=/bin/bash, LESSOPEN=| /usr/bin/lesspipe %s, OLDPWD=/root/opendj, USER=root, CLASSPATH=/root/opendj/classes:/root/opendj/resources/*.jar:/root/opendj/lib/bootstrap.jar, NLSPATH=/usr/dt/lib/nls/msg/%L/%N.cat, LD_PRELOAD=, LD_PRELOAD_32=, LS_COLORS=, HOME=/root, SHLVL=1, LD_LIBRARY_PATH_32=}
Jun 26, 2015 5:41:45 AM org.opends.quicksetup.util.Utils supportsOption
INFO: returnCode: 0
Jun 26, 2015 5:41:45 AM org.opends.quicksetup.util.Utils supportsOption
INFO: supported: true
Jun 26, 2015 5:41:45 AM org.opends.quicksetup.util.Utils supportsOption
INFO: Checking if options -Xms64m -Xmx128m -client are supported with java home: /usr/lib/jvm/jdk1.8.0_40/jre
Jun 26, 2015 5:41:45 AM org.opends.quicksetup.util.Utils supportsOption
INFO: launching [/root/opendj/lib/_script-util.sh] with env: {PATH=/bin:/usr/bin, SCRIPT_NAME_ARG=-Dorg.opends.server.scriptName=setup, LESSCLOSE=/usr/bin/lesspipe %s %s, OPENDJ_JAVA_HOME=/usr/lib/jvm/jdk1.8.0_40/jre, JAVA_HOME=/usr/lib/jvm/java-8-oracle-amd64, TERM=dumb, XFILESEARCHPATH=/usr/dt/app-defaults/%L/Dt, OPENDJ_JAVA_ARGS=-Xms64m -Xmx128m -client, SCRIPT_UTIL_CMD=set-full-environment-and-test-java, INSTALL_ROOT=/root/opendj, MAIL=/var/mail/root, INSTANCE_ROOT=/root/opendj, LD_LIBRARY_PATH=, SCRIPT_NAME=setup, LOGNAME=root, LD_PRELOAD_64=, PWD=/root/opendj, LD_LIBRARY_PATH_64=, _=./setup, SHELL=/bin/bash, LESSOPEN=| /usr/bin/lesspipe %s, OLDPWD=/root/opendj, USER=root, CLASSPATH=/root/opendj/classes:/root/opendj/resources/*.jar:/root/opendj/lib/bootstrap.jar, NLSPATH=/usr/dt/lib/nls/msg/%L/%N.cat, LD_PRELOAD=, LD_PRELOAD_32=, LS_COLORS=, HOME=/root, SHLVL=1, LD_LIBRARY_PATH_32=}
Jun 26, 2015 5:41:46 AM org.opends.quicksetup.util.Utils supportsOption
INFO: returnCode: 0
Jun 26, 2015 5:41:46 AM org.opends.quicksetup.util.Utils supportsOption
INFO: supported: true
Jun 26, 2015 5:41:46 AM org.opends.quicksetup.installer.Installer configureServer
INFO: configure DS cmd: -C org.opends.server.extensions.ConfigFileHandler -c /root/opendj/config/config.ldif -h a3c83d971fd8 -p 389 --adminConnectorPort 4444 -D cn=Directory Manager -w {rootUserPassword} -R /root/opendj
Jun 26, 2015 5:41:47 AM org.opends.quicksetup.Application$ApplicationPrintStream println
INFO: Successfully wrote the updated Directory Server configuration
Jun 26, 2015 5:41:47 AM org.opends.quicksetup.Application$ApplicationPrintStream println
INFO:

Jun 26, 2015 5:41:47 AM org.opends.quicksetup.util.ServerController startServer
INFO: starting server
Jun 26, 2015 5:41:49 AM org.opends.quicksetup.util.ServerController$StartReader$1 run
INFO: server: [26/Jun/2015:05:41:49 +0000] category=EXTENSIONS severity=NOTICE msgID=1507899 msg=Loaded extension from file '/root/opendj/lib/extensions/snmp-mib2605.jar' (build 2.6.0, revision 9086)
Jun 26, 2015 5:41:49 AM org.opends.quicksetup.util.ServerController$StartReader$1 run
INFO: server: [26/Jun/2015:05:41:49 +0000] category=CORE severity=NOTICE msgID=458886 msg=OpenDJ 2.6.0 (build 20130626200626Z, R9086) starting up
Jun 26, 2015 5:41:50 AM org.opends.quicksetup.util.ServerController$StartReader$1 run
INFO: server: [26/Jun/2015:05:41:50 +0000] category=RUNTIME_INFORMATION severity=NOTICE msgID=20381717 msg=Installation Directory:  /root/opendj
Jun 26, 2015 5:41:50 AM org.opends.quicksetup.util.ServerController$StartReader$1 run
INFO: server: [26/Jun/2015:05:41:50 +0000] category=RUNTIME_INFORMATION severity=NOTICE msgID=20381719 msg=Instance Directory:      /root/opendj
Jun 26, 2015 5:41:50 AM org.opends.quicksetup.util.ServerController$StartReader$1 run
INFO: server: [26/Jun/2015:05:41:50 +0000] category=RUNTIME_INFORMATION severity=NOTICE msgID=20381713 msg=JVM Information: 1.8.0_40-b26 by Oracle Corporation, 64-bit architecture, 3732406272 bytes heap size
Jun 26, 2015 5:41:50 AM org.opends.quicksetup.util.ServerController$StartReader$1 run
INFO: server: [26/Jun/2015:05:41:50 +0000] category=RUNTIME_INFORMATION severity=NOTICE msgID=20381714 msg=JVM Host: a3c83d971fd8, running Linux 3.13.0-53-generic amd64, 16787423232 bytes physical memory size, number of processors available 8
Jun 26, 2015 5:41:50 AM org.opends.quicksetup.util.ServerController$StartReader$1 run
INFO: server: [26/Jun/2015:05:41:50 +0000] category=RUNTIME_INFORMATION severity=NOTICE msgID=20381715 msg=JVM Arguments: "-Dorg.opends.server.scriptName=start-ds"
Jun 26, 2015 5:41:50 AM org.opends.quicksetup.util.ServerController$StartReader$1 run
INFO: server: Exception in thread "main" java.lang.ExceptionInInitializerError: A security class cannot be found in this JVM because of the following reason: sun.security.x509.CertAndKeyGen
Jun 26, 2015 5:41:50 AM org.opends.quicksetup.util.ServerController$StartReader$1 run
INFO: server: 	at org.opends.server.util.Platform$PlatformIMPL.<clinit>(Platform.java:127)
Jun 26, 2015 5:41:50 AM org.opends.quicksetup.util.ServerController$StartReader$1 run
INFO: server: 	at org.opends.server.util.Platform.<clinit>(Platform.java:80)
Jun 26, 2015 5:41:50 AM org.opends.quicksetup.util.ServerController$StartReader$1 run
INFO: server: 	at org.opends.server.util.CertificateManager.generateSelfSignedCertificate(CertificateManager.java:283)
Jun 26, 2015 5:41:50 AM org.opends.quicksetup.util.ServerController$StartReader$1 run
INFO: server: 	at org.opends.server.admin.AdministrationConnector.createSelfSignedCertificateIfNeeded(AdministrationConnector.java:698)
Jun 26, 2015 5:41:50 AM org.opends.quicksetup.util.ServerController$StartReader$1 run
INFO: server: 	at org.opends.server.core.DirectoryServer.startServer(DirectoryServer.java:1353)
Jun 26, 2015 5:41:50 AM org.opends.quicksetup.util.ServerController$StartReader$1 run
INFO: server: 	at org.opends.server.core.DirectoryServer.main(DirectoryServer.java:9651)
root@a3c83d971fd8:~#
#+END_EXAMPLE
*** DONE opendj status
  CLOSED: [2015-06-26 Fri 10:09]
#+BEGIN_EXAMPLE
root@60168a2aac56:~# /opt/opendj/bin/status
/opt/opendj/bin/status


>>>> Specify OpenDJ LDAP connection parameters

Administrator user bind DN [cn=Directory Manager]:


Password for user 'cn=Directory Manager':

          --- Server Status ---
Server Run Status:        Started
Open Connections:         1

          --- Server Details ---
Host Name:                60168a2aac56
Administrative Users:     cn=Directory Manager
Installation Path:        /opt/opendj
Version:                  OpenDJ 2.6.0
Java Version:             1.7.0_79
Administration Connector: Port 4444 (LDAPS)

          --- Connection Handlers ---
Address:Port : Protocol : State
-------------:----------:---------
--           : LDIF     : Disabled
0.0.0.0:161  : SNMP     : Disabled
0.0.0.0:389  : LDAP     : Enabled
0.0.0.0:636  : LDAPS    : Disabled
0.0.0.0:1689 : JMX      : Disabled
0.0.0.0:8080 : HTTP     : Disabled

          --- Data Sources ---
-No LDAP Databases Found-

root@60168a2aac56:~# lsof -i tcp:4444
lsof -i tcp:4444
COMMAND  PID USER   FD   TYPE   DEVICE SIZE/OFF NODE NAME
java    8079 root   82u  IPv6 46924510      0t0  TCP *:4444 (LISTEN)
root@60168a2aac56:~#
#+END_EXAMPLE
*** DONE mail: OpenDJ 2.6.0 doesn't work with Java8                :noexport:
  CLOSED: [2015-06-26 Fri 16:25]
[[gnus:nnfolder%2Barchive:mail.sent.mail#m2egkzovh5.fsf@gmail.com][Email from Denny Zhang (Fri, 26 Jun 2015 01:05:26 -0500): Official OpenDJ doesn't work w]]
#+begin_example
From: Denny Zhang <filebat.mark@gmail.com>
Subject: Official OpenDJ doesn't work with Java8
To: Brandon Chen <bchen.osc@gmail.com>
Date: Fri, 26 Jun 2015 01:05:26 -0500
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/24.4 (darwin)

Hi Bradon

In my test, I found OpenDJ doesn't work with JAVA8.

It's also confirmed by below link.
https://forgerock.org/topic/opendj-2-6-does-not-compatible-with-java-8/

Do you have any recommendation for OpenDJ version and JDK version?

--
Denny Zhang(张巍)
Email: filebat.mark@gmail.com
Website: https://www.dennyzhang.com/

Watch out for useless worries, like the past, the future, the nothing
you cannot change.

Ｏｏ.°ｏＯｏ.Ｏｏ.°〇ｏ〇

#+end_example
*** TODO start opendj failed
#+BEGIN_EXAMPLE
root@60168a2aac56:/usr/local/opendj# tail -n 100 /tmp/opendj-setup-4128481801537249804.log
<l/opendj# tail -n 100 /tmp/opendj-setup-4128481801537249804.log
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/._03-rfc3712.ldif' to '/usr/local/opendj-1/./config/schema/._03-rfc3712.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/03-rfc2926.ldif' to '/usr/local/opendj-1/./config/schema/03-rfc2926.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/03-uddiv3.ldif' to '/usr/local/opendj-1/./config/schema/03-uddiv3.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/00-core.ldif' to '/usr/local/opendj-1/./config/schema/00-core.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/06-compat.ldif' to '/usr/local/opendj-1/./config/schema/06-compat.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/._06-compat.ldif' to '/usr/local/opendj-1/./config/schema/._06-compat.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/03-rfc3112.ldif' to '/usr/local/opendj-1/./config/schema/03-rfc3112.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/._03-rfc2714.ldif' to '/usr/local/opendj-1/./config/schema/._03-rfc2714.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/._03-changelog.ldif' to '/usr/local/opendj-1/./config/schema/._03-changelog.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/03-rfc2739.ldif' to '/usr/local/opendj-1/./config/schema/03-rfc2739.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/03-changelog.ldif' to '/usr/local/opendj-1/./config/schema/03-changelog.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/02-config.ldif' to '/usr/local/opendj-1/./config/schema/02-config.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/._05-solaris.ldif' to '/usr/local/opendj-1/./config/schema/._05-solaris.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/._03-rfc2713.ldif' to '/usr/local/opendj-1/./config/schema/._03-rfc2713.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/04-rfc2307bis.ldif' to '/usr/local/opendj-1/./config/schema/04-rfc2307bis.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/05-solaris.ldif' to '/usr/local/opendj-1/./config/schema/05-solaris.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/._00-core.ldif' to '/usr/local/opendj-1/./config/schema/._00-core.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/._05-samba.ldif' to '/usr/local/opendj-1/./config/schema/._05-samba.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/03-rfc3712.ldif' to '/usr/local/opendj-1/./config/schema/03-rfc3712.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/config/schema/._03-rfc3112.ldif' to '/usr/local/opendj-1/./config/schema/._03-rfc3112.ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/._classes' to '/usr/local/opendj-1/./._classes'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.FileManager$CopyOperation apply
INFO: copying file '/usr/local/opendj-1/template/._ldif' to '/usr/local/opendj-1/./._ldif'
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.Utils supportsOption
INFO: Checking if options -Xms8m -client are supported with java home: /usr/lib/jvm/java-7-openjdk-amd64/jre
Jun 26, 2015 1:02:22 PM org.opends.quicksetup.util.Utils supportsOption
INFO: launching [/usr/local/opendj-1/lib/_script-util.sh] with env: {TERM=dumb, SHLVL=1, LD_PRELOAD=, LESSCLOSE=/usr/bin/lesspipe %s %s, SCRIPT_UTIL_CMD=set-full-environment-and-test-java, MAIL=/var/mail/root, LD_PRELOAD_64=, PWD=/usr/local/opendj, LOGNAME=root, _=./setup, SCRIPT_NAME_ARG=-Dorg.opends.server.scriptName=setup, LD_LIBRARY_PATH=, LD_LIBRARY_PATH_32=, OLDPWD=/usr/local/opendj, SHELL=/bin/bash, SCRIPT_NAME=setup, CLASSPATH=/usr/local/opendj/classes:/usr/local/opendj/resources/*.jar:/usr/local/opendj/lib/bootstrap.jar, PATH=/bin:/usr/bin, USER=root, LD_PRELOAD_32=, HOME=/root, LD_LIBRARY_PATH_64=, OPENDJ_JAVA_ARGS=-Xms8m -client, LESSOPEN=| /usr/bin/lesspipe %s, INSTANCE_ROOT=/usr/local/opendj, OPENDJ_JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/jre, LS_COLORS=, INSTALL_ROOT=/usr/local/opendj}
Jun 26, 2015 1:02:23 PM org.opends.quicksetup.util.Utils supportsOption
INFO: returnCode: 0
Jun 26, 2015 1:02:23 PM org.opends.quicksetup.util.Utils supportsOption
INFO: supported: true
Jun 26, 2015 1:02:23 PM org.opends.quicksetup.util.Utils supportsOption
INFO: Checking if options -server are supported with java home: /usr/lib/jvm/java-7-openjdk-amd64/jre
Jun 26, 2015 1:02:23 PM org.opends.quicksetup.util.Utils supportsOption
INFO: launching [/usr/local/opendj-1/lib/_script-util.sh] with env: {TERM=dumb, SHLVL=1, LD_PRELOAD=, LESSCLOSE=/usr/bin/lesspipe %s %s, SCRIPT_UTIL_CMD=set-full-environment-and-test-java, MAIL=/var/mail/root, LD_PRELOAD_64=, PWD=/usr/local/opendj, LOGNAME=root, _=./setup, SCRIPT_NAME_ARG=-Dorg.opends.server.scriptName=setup, LD_LIBRARY_PATH=, LD_LIBRARY_PATH_32=, OLDPWD=/usr/local/opendj, SHELL=/bin/bash, SCRIPT_NAME=setup, CLASSPATH=/usr/local/opendj/classes:/usr/local/opendj/resources/*.jar:/usr/local/opendj/lib/bootstrap.jar, PATH=/bin:/usr/bin, USER=root, LD_PRELOAD_32=, HOME=/root, LD_LIBRARY_PATH_64=, OPENDJ_JAVA_ARGS=-server, LESSOPEN=| /usr/bin/lesspipe %s, INSTANCE_ROOT=/usr/local/opendj, OPENDJ_JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/jre, LS_COLORS=, INSTALL_ROOT=/usr/local/opendj}
Jun 26, 2015 1:02:23 PM org.opends.quicksetup.util.Utils supportsOption
INFO: returnCode: 0
Jun 26, 2015 1:02:23 PM org.opends.quicksetup.util.Utils supportsOption
INFO: supported: true
Jun 26, 2015 1:02:23 PM org.opends.quicksetup.util.Utils supportsOption
INFO: Checking if options -Xms64m -Xmx128m -client are supported with java home: /usr/lib/jvm/java-7-openjdk-amd64/jre
Jun 26, 2015 1:02:23 PM org.opends.quicksetup.util.Utils supportsOption
INFO: launching [/usr/local/opendj-1/lib/_script-util.sh] with env: {TERM=dumb, SHLVL=1, LD_PRELOAD=, LESSCLOSE=/usr/bin/lesspipe %s %s, SCRIPT_UTIL_CMD=set-full-environment-and-test-java, MAIL=/var/mail/root, LD_PRELOAD_64=, PWD=/usr/local/opendj, LOGNAME=root, _=./setup, SCRIPT_NAME_ARG=-Dorg.opends.server.scriptName=setup, LD_LIBRARY_PATH=, LD_LIBRARY_PATH_32=, OLDPWD=/usr/local/opendj, SHELL=/bin/bash, SCRIPT_NAME=setup, CLASSPATH=/usr/local/opendj/classes:/usr/local/opendj/resources/*.jar:/usr/local/opendj/lib/bootstrap.jar, PATH=/bin:/usr/bin, USER=root, LD_PRELOAD_32=, HOME=/root, LD_LIBRARY_PATH_64=, OPENDJ_JAVA_ARGS=-Xms64m -Xmx128m -client, LESSOPEN=| /usr/bin/lesspipe %s, INSTANCE_ROOT=/usr/local/opendj, OPENDJ_JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/jre, LS_COLORS=, INSTALL_ROOT=/usr/local/opendj}
Jun 26, 2015 1:02:24 PM org.opends.quicksetup.util.Utils supportsOption
INFO: returnCode: 0
Jun 26, 2015 1:02:24 PM org.opends.quicksetup.util.Utils supportsOption
INFO: supported: true
Jun 26, 2015 1:02:24 PM org.opends.quicksetup.installer.Installer configureServer
INFO: configure DS cmd: -C org.opends.server.extensions.ConfigFileHandler -c /usr/local/opendj-1/config/config.ldif -h 60168a2aac56 -p 389 --adminConnectorPort 4444 -D cn=Directory Manager -w {rootUserPassword} -R /usr/local/opendj-1
Jun 26, 2015 1:02:25 PM org.opends.quicksetup.Application$ApplicationPrintStream println
INFO: An error occurred while attempting to process the Directory Server
configuration file /usr/local/opendj-1/config/config.ldif:  The Directory
Server jar file ._snmp-mib2605.jar in directory
/usr/local/opendj-1/lib/extensions cannot be loaded because an unexpected
error occurred while trying to open the file for reading:  error in opening
zip file (ZipFile.java:-2 ZipFile.java:215 ZipFile.java:145 JarFile.java:154
JarFile.java:118 ClassLoaderProvider.java:817 ClassLoaderProvider.java:412
ClassLoaderProvider.java:602 ClassLoaderProvider.java:337
ClassLoaderProvider.java:283 DirectoryServer.java:1131
DirectoryServer.java:1108 ConfigureDS.java:518 InstallerHelper.java:112
Installer.java:1069)
Jun 26, 2015 1:02:25 PM org.opends.quicksetup.Application$ApplicationPrintStream println
INFO:

Jun 26, 2015 1:02:25 PM org.opends.quicksetup.installer.Installer invokeLongOperation
SEVERE: Error: Error Configuring Directory Server.
Error Configuring Directory Server.
	at org.opends.quicksetup.installer.Installer$2.run(Installer.java:1071)

Jun 26, 2015 1:02:25 PM org.opends.quicksetup.installer.offline.OfflineInstaller run
SEVERE: Caught exception: Error Configuring Directory Server.
Error Configuring Directory Server.
	at org.opends.quicksetup.installer.Installer$2.run(Installer.java:1071)

Jun 26, 2015 1:02:25 PM org.opends.quicksetup.installer.offline.OfflineInstaller run
SEVERE: Error installing.
Error Configuring Directory Server.
	at org.opends.quicksetup.installer.Installer$2.run(Installer.java:1071)
#+END_EXAMPLE
*** DONE OpenDJ: setup by command
   CLOSED: [2015-06-29 Mon 00:14]
echo "password1" > /tmp/passwd
./setup -n --acceptLicense -j /tmp/passwd

#+BEGIN_EXAMPLE
root@1ac9c7ba20a9:/usr/local/opendj# ./setup --help
./setup --help
Usage:  setup  {options}

This utility can be used to setup the Directory Server

Command options:

-a, --addBaseEntry
    Indicates whether to create the base entry in the Directory Server database
--acceptLicense
    Automatically accepts the product license (if present)
--adminConnectorPort {port}
    Port on which the Administration Connector should listen for communication
    Default value: 5444
-b, --baseDN {baseDN}
    Base DN for user information in the Directory Server.  Multiple base DNs
    may be provided by using this option multiple times
-d, --sampleData {numEntries}
    Specifies that the database should be populated with the specified number
    of sample entries
    Default value: 0
-D, --rootUserDN {rootUserDN}
    DN for the initial root user for the Directory Server
    Default value: cn=Directory Manager
--generateSelfSignedCertificate
    Generate a self-signed certificate that the server should use when
    accepting SSL-based connections or performing StartTLS negotiation
-h, --hostname {host}
    The fully-qualified directory server host name that will be used when
    generating self-signed certificates for LDAP SSL/StartTLS, the
    administration connector, and replication
    Default value: 1ac9c7ba20a9
-i, --cli
    Use the command line install. If not specified the graphical interface
    will be launched.  The rest of the options (excluding help and version)
    will only be taken into account if this option is specified
-j, --rootUserPasswordFile {rootUserPasswordFile}
    Path to a file containing the password for the initial root user for the
    Directory Server
-l, --ldifFile {ldifFile}
    Path to an LDIF file containing data that should be added to the Directory
    Server database. Multiple LDIF files may be provided by using this option
    multiple times
-N, --certNickname {nickname}
    Nickname of the certificate that the server should use when accepting
    SSL-based connections or performing StartTLS negotiation
-O, --doNotStart
    Do not start the server when the configuration is completed
-p, --ldapPort {port}
    Port on which the Directory Server should listen for LDAP communication
    Default value: 1389
-q, --enableStartTLS
    Enable StartTLS to allow secure communication with the server using the
    LDAP port
-R, --rejectFile {rejectFile}
    Write rejected entries to the specified file
-S, --skipPortCheck
    Skip the check to determine whether the specified ports are usable
--skipFile {skipFile}
    Write skipped entries to the specified file
-t, --backendType {backendType}
    The type of the userRoot backend
    Default value: local-db
-u, --keyStorePasswordFile {keyStorePasswordFile}
    Certificate key store PIN file.  A PIN is required when you specify to use
    an existing certificate (JKS, JCEKS, PKCS#12 or PKCS#11) as server
    certificate
--useJavaKeystore {keyStorePath}
    Path of a Java Key Store (JKS) containing a certificate to be used as the
    server certificate
--useJCEKS {keyStorePath}
    Path of a JCEKS containing a certificate to be used as the server
    certificate
--usePkcs11Keystore
    Use a certificate in a PKCS#11 token that the server should use when
    accepting SSL-based connections or performing StartTLS negotiation
--usePkcs12keyStore {keyStorePath}
    Path of a PKCS#12 key store containing the certificate that the server
    should use when accepting SSL-based connections or performing StartTLS
    negotiation
-w, --rootUserPassword {rootUserPassword}
    Password for the initial root user for the Directory Server
-W, --keyStorePassword {keyStorePassword}
    Certificate key store PIN.  A PIN is required when you specify to use an
    existing certificate (JKS, JCEKS, PKCS#12 or PKCS#11) as server
    certificate
-x, --jmxPort {jmxPort}
    Port on which the Directory Server should listen for JMX communication
    Default value: 1689
-Z, --ldapsPort {port}
    Port on which the Directory Server should listen for LDAPS communication.
    The LDAPS port will be configured and SSL will be enabled only if this
    argument is explicitly specified
    Default value: 1636

Utility input/output options:

-n, --no-prompt
    Use non-interactive mode.  If data in the command is missing, the user is
    not prompted and the tool will fail
--noPropertiesFile
    No properties file will be used to get default command line argument values
--propertiesFilePath {propertiesFilePath}
    Path to the file containing default property values used for command line
    arguments
-Q, --quiet
    Use quiet mode
-v, --verbose
    Use verbose mode

General options:

-V, --version
    Display Directory Server version information
-?, -H, --help
    Display this usage information
root@1ac9c7ba20a9:/usr/local/opendj#
#+END_EXAMPLE
*** DONE OpenDJ: check status by command
   CLOSED: [2015-06-29 Mon 00:14]
/usr/local/opendj/bin/status

/usr/local/opendj/bin/status -D "Directory Manager" -j /opt/authright/config/opendj_passwd

#+BEGIN_EXAMPLE
root@default-ubuntu-1404:~/opendj# /root/opendj/bin/status
/root/opendj/bin/status -w password1


>>>> Specify OpenDJ LDAP connection parameters

Administrator user bind DN [cn=Directory Manager]: password1
password1

          --- Server Status ---
Server Run Status:        Started
Open Connections:         0

          --- Server Details ---
Host Name:                default-ubuntu-1404
Administrative Users:     cn=Directory Manager
Installation Path:        /root/opendj
Version:                  OpenDJ 3.0.0-SNAPSHOT
Java Version:             <not available> (*)
Administration Connector: Port 4444 (LDAPS)

          --- Connection Handlers ---
Address:Port : Protocol : State
-------------:----------:---------
--           : LDIF     : Disabled
0.0.0.0:161  : SNMP     : Disabled
0.0.0.0:389  : LDAP     : Enabled
0.0.0.0:636  : LDAPS    : Disabled
0.0.0.0:1689 : JMX      : Disabled
0.0.0.0:8080 : HTTP     : Disabled

          --- Data Sources ---
<not available> (*)

 * Information only available if you provide valid authentication information
when launching the status command.

#+END_EXAMPLE

/usr/local/opendj/bin/status -n -j /tmp/passwd

#+BEGIN_EXAMPLE
root@2df7aed68ccc:/usr/local/opendj# /usr/local/opendj/bin/status --help
/usr/local/opendj/bin/status --help
This utility can be used to display basic server information

Usage:  status {options}
Command options:

--connectTimeout {timeout}
    Maximum length of time (in milliseconds) that can be taken to establish a
    connection.  Use '0' to specify no time out
    Default value: 30000

LDAP connection options:

-D, --bindDN {bindDN}
    DN to use to bind to the server
    Default value: cn=Directory Manager
-w, --bindPassword {bindPassword}
    Password to use to bind to the server. Use -w - to ensure that the command
    prompts for the password, rather than entering the password as a command
    argument
-j, --bindPasswordFile {bindPasswordFile}
    Bind password file
-o, --saslOption {name=value}
    SASL bind options
-X, --trustAll
    Trust all server SSL certificates
-P, --trustStorePath {trustStorePath}
    Certificate trust store path
    Default value: /usr/local/opendj-1/config/admin-truststore
-T, --trustStorePassword {trustStorePassword}
    Certificate trust store PIN
-U, --trustStorePasswordFile {path}
    Certificate trust store PIN file
-K, --keyStorePath {keyStorePath}
    Certificate key store path
-W, --keyStorePassword {keyStorePassword}
    Certificate key store PIN.  A PIN is required when you specify to use an
    existing certificate as server certificate
-u, --keyStorePasswordFile {keyStorePasswordFile}
    Certificate key store PIN file.  A PIN is required when you specify to use
    an existing certificate as server certificate
-N, --certNickname {nickname}
    Nickname of the certificate that the server should use when accepting
    SSL-based connections or performing StartTLS negotiation

Utility input/output options:

-n, --no-prompt
    Use non-interactive mode.  If data in the command is missing, the user is
    not prompted and the tool will fail
-s, --script-friendly
    Use script-friendly mode
--propertiesFilePath {propertiesFilePath}
    Path to the file containing default property values used for command line
    arguments
--noPropertiesFile
    No properties file will be used to get default command line argument values
-r, --refresh {period}
    When this argument is specified, the status command will display its
    contents periodically.  Used to specify the period (in seconds) between two
    displays of the status

General options:

-V, --version
    Display Directory Server version information
-?, -H, --help
    Display this usage information
#+END_EXAMPLE
*** TODO [#A] opendj setup output
#+BEGIN_EXAMPLE
root@a3c4549d51b9:/tmp# cd /usr/local/ldap
./setup --cli

cd /usr/local/ldap
root@a3c4549d51b9:/usr/local/ldap# ./setup --cli

What would you like to use as the initial root user DN for the Directory
Server? [cn=Directory Manager]:

Please provide the password to use for the initial root user: password1

Please re-enter the password for confirmation:

Provide the fully-qualified directory server host name that will be used when
generating self-signed certificates for LDAP SSL/StartTLS, the administration
connector, and replication [a3c4549d51b9]:
On which port would you like the Directory Server to accept connections from
LDAP clients? [389]: 1389
1389

On which port would you like the Administration Connector to accept
connections? [4444]: 10004
10004

Do you want to create base DNs in the server? (yes / no) [yes]: yes
yes
Provide the backend type:

    1)  Local DB Backend
    2)  PDB Backend

Enter choice [1]: 1
1

Provide the base DN for the directory data: [dc=example,dc=com]: dc=jingantech,dc=com
dc=jingantech,dc=com
Options for populating the database:

    1)  Leave the database empty
    2)  Only create the base entry
    3)  Import data from an LDIF file
    4)  Load automatically-generated sample data

Enter choice [1]: 4
4
Please specify the number of user entries to generate: [2000]: 10
10

Do you want to enable SSL? (yes / no) [no]:


Do you want to enable Start TLS? (yes / no) [no]:


Do you want to start the server when the configuration is completed? (yes /
no) [yes]:



Setup Summary
=============
LDAP Listener Port:            1389
Administration Connector Port: 10004
JMX Listener Port:
LDAP Secure Access:            disabled
Root User DN:                  cn=Directory Manager
Directory Data:                Backend Type: Local DB Backend
                               Create New Base DN dc=jingantech,dc=com
Base DN Data: Import Automatically-Generated Data (10 Entries)

Start Server when the configuration is completed


What would you like to do?

    1)  Set up the server with the parameters above
    2)  Provide the setup parameters again
    3)  Print equivalent non-interactive command-line
    4)  Cancel and exit

Enter choice [1]:


See /tmp/opendj-setup-541727443143343641.log for a detailed log of this
operation.

Configuring Directory Server ..... Done.
Importing Automatically-Generated Data (10 Entries) ...... Done.
Starting Directory Server ....... Done.

To see basic server configuration status and configuration you can launch
/usr/local/ldap-1/bin/status

root@a3c4549d51b9:/usr/local/ldap# /usr/local/ldap-1/bin/status
/usr/local/ldap-1/bin/status


>>>> Specify OpenDJ LDAP connection parameters

Administrator user bind DN [cn=Directory Manager]:


Password for user 'cn=Directory Manager':

          --- Server Status ---
Server Run Status:        Started
Open Connections:         1

          --- Server Details ---
Host Name:                a3c4549d51b9
Administrative Users:     cn=Directory Manager
Installation Path:        /usr/local/ldap-1
Version:                  OpenDJ 3.0.0-SNAPSHOT
Java Version:             1.8.0_40
Administration Connector: Port 10004 (LDAPS)

          --- Connection Handlers ---
Address:Port : Protocol : State
-------------:----------:---------
--           : LDIF     : Disabled
0.0.0.0:161  : SNMP     : Disabled
0.0.0.0:636  : LDAPS    : Disabled
0.0.0.0:1389 : LDAP     : Enabled
0.0.0.0:1689 : JMX      : Disabled
0.0.0.0:8080 : HTTP     : Disabled

          --- Data Sources ---
Base DN:     dc=jingantech,dc=com
Backend ID:  userRoot
Entries:     12
Replication:
#+END_EXAMPLE
*** TODO How to verify opendj service
#+BEGIN_EXAMPLE
[8/24/15, 9:35:15 AM] Jason Zheng: I updated the following REST API, so we can list directory users based on userName or email:
[8/24/15, 9:36:17 AM] Jason Zheng: list all users under a directory: /account/directories/<directory_id>/users
[8/24/15, 9:37:32 AM] Jason Zheng: list users under a directory with userName: /account/directories/<directory_id>/users?userName=<user_name>
[8/24/15, 9:37:53 AM] Jason Zheng: list users under a directory with email: /account/directories/<directory_id>/users?email=<email>
#+END_EXAMPLE
*** DONE opendj /usr/local/opendj/bin/list-backends
  CLOSED: [2015-07-27 Mon 23:17]
#+BEGIN_EXAMPLE
root@506931afcbee:/usr/local/opendj/bin# ./list-backends
./list-backends
Backend ID     : Base DN
---------------:------------------
adminRoot      : cn=admin data
ads-truststore : cn=ads-truststore
backup         : cn=backups
config         : cn=config
monitor        : cn=monitor
schema         : cn=schema
tasks          : cn=tasks
root@506931afcbee:/usr/local/opendj/bin#
#+END_EXAMPLE

* hornetq                                                          :noexport:
** DONE Ubuntu install hornetq
  CLOSED: [2014-08-07 Thu 11:07]
http://hornetq.jboss.org
http://hornetq.jboss.org/downloads

apt-get install libaio
| Name          | Summary                                                                       |
|---------------+-------------------------------------------------------------------------------|
| start hornetq | cd /opt/hornetq/bin; nohup ./run.sh >> /data/fluigidentity-logs/hornetq.log & |
*** How to install
http://docs.jboss.org/hornetq/2.2.14.Final/quickstart-guide/en/html/index.html
Dowload hornetq-2.2.14.Final.tar.gz

HornetQ only runs on Java 6 or later.

After downloading the distribution, unzip it into your chosen directory.

cd /opt/hornetq/bin
./run.sh
*** TODO apt-get install libaio: fail to install libaio
*** configuration
#+begin_example
root@testmini1:/cloudpass/backend/build/bin# tree /opt/couchbase/etc
/opt/couchbase/etc
├── couchbase
│   ├── config
│   ├── init.sql
│   └── static_config
├── couchbase_init.d
├── couchbase_init.d.tmpl
├── couchdb
│   ├── default.d
│   │   ├── capi.ini
│   │   └── geocouch.ini
│   ├── default.ini
│   ├── local.d
│   └── local.ini
├── default
│   └── couchdb
├── init.d
│   └── couchdb
├── logrotate.d
│   └── couchdb
└── runtime.ini

7 directories, 13 files
#+end_example
*** TODO get hornetq version
* [#A] elk: log analysis                                 :noexport:IMPORTANT:
http://www.elasticsearch.org/overview/elkdownloads/

| Name          | Summary                                       |
|---------------+-----------------------------------------------|
| logstash      | bin/logstash -f logstash.conf                 |
| elasticsearch | ./bin/elasticsearch                           |
| elasticsearch | http://$elastic_search_ip:9200/_search?pretty |
| kibana4       | ./bin/kibana                                  |
| kibana        | http://yourhost.com:5601                      |

- logstash
| Name              | Summary                                                                               |
|-------------------+---------------------------------------------------------------------------------------|
| inputs            | How events get into LogStash                                                          |
| codecs            | convert an incoming format into an internal representation                            |
| Codecs: json      | encode / decode data in JSON format                                                   |
| filters           | Process actions on events: modifiy events or drop events                              |
| filters: grok     | parses arbitrary text and structure it.                                               |
| Codecs: multiline | Merge multiple-line text events into one, e.g. java exception and stacktrace messages |
| outputs           | How output events from logstash                                                 |
|-------------------+---------------------------------------------------------------------------------------|
| simple test       | bin/logstash -e 'input { stdin { } } output { stdout {} }'                            |
| Index             | default index name is like logstash-YYYY.MM.DD, creating one index per day.           |

- logstash event
| Name      | Summary            |
|-----------+--------------------|
| timestamp | ISO 8601 timestamp |
| message   | data               |
| version   |                    |
| host      | the host of sender |
| type      | syslog, irc, etc   |

- elasticsearch
| Name                        | Summary                                    |
|-----------------------------+--------------------------------------------|
| elasticsearch check version | http://$elasticsearch_ip:9200/_nodes       |
| elasticsearch               | http://192.241.202.107:9200/_search?pretty |

- Video
| Name                         | Summary                                 | Link                                        |
|------------------------------+-----------------------------------------+---------------------------------------------|
| Introduction and Demo to ELK | First 5 min for the values and problems | https://www.youtube.com/watch?v=GrdzX9BNfkg |
| UI of Kibana 3               |                                         | https://www.youtube.com/watch?v=hXiBe8NcLPA |
| Visualizing Logs Using ELK   |                                         | https://www.youtube.com/watch?v=Kqs7UcCJquM                                            |
** DONE kibana 4.0.0b2 elasticsearch version problem
  CLOSED: [2015-01-29 Thu 23:06]
Check version for http://$elasticsearch_ip:9200/_nodes
http://awstaiwan.blogspot.com/2015/01/kibana-this-version-of-kibana-requires.html

https://github.com/elasticsearch/kibana/issues/1629

http://blog.keithkim.com/2015/01/kibana-elasticsearch-logstash-issue.html
https://github.com/elasticsearch/kibana/issues/2513

#+BEGIN_EXAMPLE
FYI, I managed to resolve this by upgrading the version of ElasticSearch which my version of logstash uses.

Download ElasticSearch 1.4.0.Beta1 from wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.4.0.Beta1.zip
Stop logstash
Unzip, and place into the directory <logstash installation directory>/vendor/jar/
Remove (backup) the directory named <logstash installation directory>/vendor/jar/elasticsearch-1.1.1
Start logstash
So far, so good. No errors in the logstash console, messages from my pre-existing shippers are being received and Kibana 4 now loads fine (as does my Kibana 3 instance).
#+END_EXAMPLE

#+BEGIN_EXAMPLE

Monday, January 12, 2015

Kibana: This version of Kibana requires Elasticsearch 1.4.0 or higher on all nodes. I found the following incompatible nodes in your cluster: Elasticsearch 1.1.1 @ inet[/10.0.2.15:9301] (172.16.255.250)
If your Kibana 4.0 complains with your elasticsearch version 1.1.1. Even though you has install elasticsearch 1.4.2.  It's because an elasticsearch running embedded in logstash 1.4.2 (under vendor/jar/elasticearch-1.1.1). You should disable it if you install another elasticsearch 1.4.2 on the same or a different node.

#+END_EXAMPLE
** DONE kibana 4 setup: no need for vhost
  CLOSED: [2015-01-29 Thu 17:03]
http://www.elasticsearch.org/overview/kibana/installation/

wget https://download.elasticsearch.org/kibana/kibana/kibana-4.0.0-beta3.tar.gz
** DONE install logstatsh
   CLOSED: [2015-01-29 Thu 23:07]
sudo apt-get install openjdk-7-jdk

wget https://download.elasticsearch.org/logstash/logstash/logstash-1.4.2.zip
wget https://download.elasticsearch.org/logstash/logstash/logstash-contrib-1.4.2.tar.gz
Download and unzip the latest Logstash release

2 Prepare a logstash.conf config file

3 Run bin/logstash agent -f logstash.conf
** DONE elasticsearch
   CLOSED: [2015-01-30 Fri 14:41]
http://logstash.net/docs/1.4.2/tutorials/getting-started-with-logstash
curl -O https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.4.2.tar.gz
tar zxvf elasticsearch-1.4.2.tar.gz
cd elasticsearch-1.4.2/
./bin/elasticsearch
** #  --8<-------------------------- separator ------------------------>8--
** DONE logstash.conf example
   CLOSED: [2015-01-29 Thu 23:07]
#+BEGIN_EXAMPLE
input {
  file {
    path => "/var/log/apache2/error.log"
    type => "apache-error"
  }
}

filter {
       grok {
           type => "apache-error"
           pattern => "%{COMBINEDAPACHELOG}"
           }
}

output {
  elasticsearch {
    host => localhost
  }
  stdout { codec => rubydebug }
}
#+END_EXAMPLE
** DONE logstash monitor multiple files
   CLOSED: [2015-01-30 Fri 09:56]
#+BEGIN_EXAMPLE
input {
  file {
    path => "/data/fluigidentity-logs/adsync.log"
    type => "adsync-log"
  },

  file {
    path => "/data/fluigidentity-logs/search.log"
    type => "search-log"
  },

  file {
    path => "/data/fluigidentity-logs/rest.log"
    type => "rest-log"
  },

  file {
    path => "/data/fluigidentity-logs/cloudpass_logs/racagent01.log"
    type => "racagent-log"
  },

  file {
    path => "/data/fluigidentity-logs/cloudpass_logs/server.log"
    type => "server-log"
  }
}

filter {
       grok {
           type => "adsync-log"
           pattern => "%{COMBINEDAPACHELOG}"
           },

       grok {
           type => "search-log"
           pattern => "%{COMBINEDAPACHELOG}"
           },

       grok {
           type => "rest-log"
           pattern => "%{COMBINEDAPACHELOG}"
           },

       grok {
           type => "racagent-log"
           pattern => "%{COMBINEDAPACHELOG}"
           },

       grok {
           type => "server-log"
           pattern => "%{COMBINEDAPACHELOG}"
           }
}

output {
  elasticsearch {
    host => "192.241.202.107"
  }
  stdout { codec => rubydebug }
}
#+END_EXAMPLE
** #  --8<-------------------------- separator ------------------------>8--
** TODO how to query use log data which is already been collected
** TODO [#B] kibana take a long time to start
** TODO How to avoid log files are generating too fast in logstash client
** #  --8<-------------------------- separator ------------------------>8--
** TODO [#B] kibana only search in same file of all machines in the env
** TODO [#B] kibana only search in a given machine
** #  --8<-------------------------- separator ------------------------>8--
** TODO elasticsearch data retention to remove old log files
http://logstash.net/docs/1.4.2/tutorials/getting-started-with-logstash
Of course, you can always archive (or re-index) your data to an alternate location, where you are able to query further into the past. If you'd like to simply delete old indices after a certain time period, you can use the Elasticsearch Curator tool.
** [#A] kibanan auto configure useful dashboard                   :IMPORTANT:
** TODO kibanba time chart
https://github.com/elasticsearch/kibana/blob/master/README.md
** #  --8<-------------------------- separator ------------------------>8--
** TODO kibana visualize design
https://github.com/elasticsearch/kibana/blob/master/README.md
** TODO when logstash process die, how to make it up?
** #  --8<-------------------------- separator ------------------------>8--
** DONE [#A] kibana: search all rest.log across different machines: mark different logfile by type, then filter matches by type
  CLOSED: [2015-01-30 Fri 15:10]
http://192.241.202.107:5601
** DONE [#A] logstash build customized grok                       :IMPORTANT:
  CLOSED: [2015-01-31 Sat 10:42]
http://grokdebug.herokuapp.com/patterns#
** DONE [#A] logstatsh parse Apache error log                     :IMPORTANT:
  CLOSED: [2015-01-31 Sat 11:02]
http://stackoverflow.com/questions/17331593/parse-apache2-error-logs-with-grok-for-logstash

remember to refresh indice
#+BEGIN_EXAMPLE
root@sf-fi-qa-01:~/logstash-1.4.2# cat /var/lib/logstash/etc/grok
HTTPERRORDATE %{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}
APACHEERRORLOG \[%{HTTPERRORDATE:timestamp}\] \[%{WORD:severity}\] \[client %{IPORHOST:clientip}\] %{GREEDYDATA:message_remainder}
#+END_EXAMPLE

#+BEGIN_EXAMPLE
input {
  file {
    path => "/data/fluigidentity-logs/adsync.log"
    type => "adsync-log"
  }

  file {
    path => "/data/fluigidentity-logs/search.log"
    type => "search-log"
  }

  file {
    path => "/data/fluigidentity-logs/rest.log"
    type => "rest-log"
  }

  file {
    path => "/data/fluigidentity-logs/cloudpass_logs/racagent01.log"
    type => "racagent-log"
  }

  file {
    path => "/data/fluigidentity-logs/cloudpass_logs/racagent02.log"
    type => "racagent-log"
  }

  file {
    path => "/data/fluigidentity-logs/cloudpass_logs/server.log"
    type => "server-log"
  }

  file {
    path => "/var/log/apache2/*_access.log"
    type => "apache-access-log"
  }

  file {
    path => "/var/log/apache2/*_error.log"
    type => "apache-error-log"
  }
}

filter {
       grok {
           type => "adsync-log"
           pattern => "%{COMBINEDAPACHELOG}"
           }

       grok {
           type => "search-log"
           pattern => "%{COMBINEDAPACHELOG}"
           }

       grok {
           type => "rest-log"
           pattern => "%{COMBINEDAPACHELOG}"
           }

       grok {
           type => "racagent-log"
           pattern => "%{COMBINEDAPACHELOG}"
           }

       grok {
           type => "server-log"
           pattern => "%{COMBINEDAPACHELOG}"
           }

       grok {
           type => "apache-access-log"
           pattern => "%{COMBINEDAPACHELOG}"
           }

       grok {
           type => "apache-error-log"
           match => { "message" => "%{APACHEERRORLOG}" }
           patterns_dir => ["/var/lib/logstash/etc/grok"]
           #match => [ "message", "\[%{HTTPDATE:timestamp}\] \[%{WORD:severity}\] \[client %{IPORHOST:clientip}\] %{GREEDYDATA:message_remainder}" ]
           #add_field => { "message_remainder" => "%{message_remainder}" }
           }
}

output {
  elasticsearch {
    host => "192.241.202.107"
  }
  stdout { codec => rubydebug }
}
#+END_EXAMPLE
** #  --8<-------------------------- separator ------------------------>8--
** DONE graph: Kibana get the error counts per hours
   CLOSED: [2015-01-31 Sat 11:21]
** DONE [#A] enable logstash for kibana
  CLOSED: [2015-01-30 Fri 22:56]
cd /root/
wget https://download.elasticsearch.org/logstash/logstash/logstash-1.4.2.zip
unzip logstash-1.4.2.zip

# workaround for logstash emebed version of elasticsearch
mv /root/logstash-1.4.2/vendor/jar/elasticsearch-1.1.1  /root/

cd /root/
curl -O https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.4.2.tar.gz
tar zxvf elasticsearch-1.4.2.tar.gz
cp -r elasticsearch-1.4.2  /root/logstash-1.4.2/vendor/jar/

# /root/logstash-1.4.2
cat > /root/logstash-1.4.2/logstash.conf <<EOF
input {
  file {
    path => "/data/fluigidentity-logs/adsync.log"
    type => "adsync-log"
  }

  file {
    path => "/data/fluigidentity-logs/search.log"
    type => "search-log"
  }

  file {
    path => "/data/fluigidentity-logs/rest.log"
    type => "rest-log"
  }

  file {
    path => "/data/fluigidentity-logs/cloudpass_logs/racagent01.log"
    type => "racagent-log"
  }

  file {
    path => "/data/fluigidentity-logs/cloudpass_logs/racagent02.log"
    type => "racagent-log"
  }

  file {
    path => "/data/fluigidentity-logs/cloudpass_logs/server.log"
    type => "server-log"
  }

  file {
    path => "/var/log/apache2/*.log"
    type => "apache-log"
  }
}

filter {
       grok {
           type => "adsync-log"
           pattern => "%{COMBINEDAPACHELOG}"
           }

       grok {
           type => "search-log"
           pattern => "%{COMBINEDAPACHELOG}"
           }

       grok {
           type => "rest-log"
           pattern => "%{COMBINEDAPACHELOG}"
           }

       grok {
           type => "racagent-log"
           pattern => "%{COMBINEDAPACHELOG}"
           }

       grok {
           type => "server-log"
           pattern => "%{COMBINEDAPACHELOG}"
           }

       grok {
           type => "apache-log"
           pattern => "%{COMBINEDAPACHELOG}"
           }
}

output {
  elasticsearch {
    host => "192.241.202.107"
  }
  stdout { codec => rubydebug }
}
EOF

source /etc/profile
cd /root/logstash-1.4.2
nohup bin/logstash -f logstash.conf &
** DONE [#A] kibana filter fields manually: path:www.fluigidentity.com_ssl_error.log: manually filter out popular one
   CLOSED: [2015-01-31 Sat 08:43]
** DONE logstash debug tool: http://grokdebug.herokuapp.com
   CLOSED: [2015-01-31 Sat 11:03]
** DONE [#A] elk search: kibana search filter out pattern         :IMPORTANT:
  CLOSED: [2015-01-31 Sat 15:43]
!message:Amazon && !message:registerUser
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html

http://stackoverflow.com/questions/25720741/kibana-query-exact-match

kibana search filter out fake web login
** DONE [#A] send data by elk api
  CLOSED: [2016-07-11 Mon 19:40]
https://www.elastic.co/blog/little-logstash-lessons-part-using-grok-mutate-type-data

If I were to send this document to Elasticsearch to be indexed:

curl -XPOST 'localhost:9200/logstash-2016.07.11/logs/1' -d '
{
    "@timestamp": "2016-07-11T11:35:45.000Z",
    "@version": "1",
    "count": 2048,
    "average": 1523.33,
    "host": "denny.com"
}'
** DONE elk check mapping
  CLOSED: [2016-07-11 Mon 19:44]
curl localhost:9200/logstash-2016.07.11/_mapping?pretty
https://www.elastic.co/blog/little-logstash-lessons-part-using-grok-mutate-type-data

#+BEGIN_EXAMPLE
]0;root@mytest: /root@mytest:/# curl localhost:9200/logstash-2016.07.11/_mapping?pretty
curl localhost:9200/logstash-2016.07.11/_mapping?pretty
{
  "logstash-2016.07.11" : {
    "mappings" : {
      "_default_" : {
        "dynamic_templates" : [ {
          "message_field" : {
            "mapping" : {
              "index" : "analyzed",
              "omit_norms" : true,
              "type" : "string"
            },
            "match" : "message",
            "match_mapping_type" : "string"
          }
        }, {
          "string_fields" : {
            "mapping" : {
              "index" : "analyzed",
              "omit_norms" : true,
              "type" : "string",
              "fields" : {
                "raw" : {
                  "ignore_above" : 256,
                  "index" : "not_analyzed",
                  "type" : "string"
                }
              }
            },
            "match" : "*",
            "match_mapping_type" : "string"
          }
        } ],
        "_all" : {
          "enabled" : true,
          "omit_norms" : true
        },
        "properties" : {
          "@version" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "geoip" : {
            "dynamic" : "true",
            "properties" : {
              "location" : {
                "type" : "geo_point"
              }
            }
          }
        }
      },
      "logs" : {
        "dynamic_templates" : [ {
          "message_field" : {
            "mapping" : {
              "index" : "analyzed",
              "omit_norms" : true,
              "type" : "string"
            },
            "match" : "message",
            "match_mapping_type" : "string"
          }
        }, {
          "string_fields" : {
            "mapping" : {
              "index" : "analyzed",
              "omit_norms" : true,
              "type" : "string",
              "fields" : {
                "raw" : {
                  "ignore_above" : 256,
                  "index" : "not_analyzed",
                  "type" : "string"
                }
              }
            },
            "match" : "*",
            "match_mapping_type" : "string"
          }
        } ],
        "_all" : {
          "enabled" : true,
          "omit_norms" : true
        },
        "properties" : {
          "@timestamp" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "@version" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "agent" : {
            "type" : "string",
            "norms" : {
              "enabled" : false
            },
            "fields" : {
              "raw" : {
                "type" : "string",
                "index" : "not_analyzed",
                "ignore_above" : 256
              }
            }
          },
          "auth" : {
            "type" : "string",
            "norms" : {
              "enabled" : false
            },
            "fields" : {
              "raw" : {
                "type" : "string",
                "index" : "not_analyzed",
                "ignore_above" : 256
              }
            }
          },
          "average" : {
            "type" : "double"
          },
          "bytes" : {
            "type" : "string",
            "norms" : {
              "enabled" : false
            },
            "fields" : {
              "raw" : {
                "type" : "string",
                "index" : "not_analyzed",
                "ignore_above" : 256
              }
            }
          },
          "clientip" : {
            "type" : "string",
            "norms" : {
              "enabled" : false
            },
            "fields" : {
              "raw" : {
                "type" : "string",
                "index" : "not_analyzed",
                "ignore_above" : 256
              }
            }
...
...
#+END_EXAMPLE
** DONE [#A] inject data by explictly configure timestamp field
  CLOSED: [2016-07-11 Mon 20:14]
http://stackoverflow.com/questions/32337881/grok-what-is-the-difference-between-grok-pattern-timestamp-and-date-filter-of-l
http://stackoverflow.com/questions/17319760/logstash-converting-date-to-valid-joda-time-timestamp
https://discuss.elastic.co/t/converting-string-to-date/26748/5

item_name property_name property_value

echo "[11/Jul/2016:08:13:45 +0000] mdm-master diskUsed 117314535" >> /home/denny/data_volume.log
echo "[11/Jul/2016:09:13:45 +0000] mdm-master diskUsed 117314535" >> /home/denny/data_volume.log
echo "[11/Jul/2016:10:13:45 +0000] mdm-master diskUsed 117314535" >> /home/denny/data_volume.log

cat > /etc/logstash/conf.d/data_volume.conf <<EOF
input {
  file {
    path => "/home/denny/data_volume.log"
    start_position => beginning
  }
}

filter {
    if [path] =~ "data_volume" {
        grok {
            match => {"message" => "\[%{HTTPDATE:log_timestamp}\] %{NOTSPACE:item_name} %{NOTSPACE:property_name} %{NOTSPACE:property_value:float}"
            }
       }
       date {
          match => [ "log_timestamp", "dd/MMM/YYYY:MM:mm:ss Z" ]
       }
   }
}

output {
  elasticsearch {
    host => localhost
  }
  stdout { codec => rubydebug }
}
EOF

service logstash restart
service logstash status
** TODO docker image: long number display: 104147191
kibana metric visualization number formatter: JSON formatted properties: long number display: 104147191
https://www.elastic.co/guide/en/kibana/current/metric-chart.html
https://discuss.elastic.co/t/kibana-4-1-metric-visualization-round-decimal-value/24098/4
https://www.elastic.co/blog/kibana-4-1-field-formatters
https://github.com/elastic/kibana/issues/4035
https://github.com/elastic/kibana/issues/4035
{ "script" : "doc['grade'].value * 1.2" }
** DONE check logstash version: bin/logstash --version
   CLOSED: [2015-08-09 Sun 08:31]
** DONE kibana fail to start: kibana 4.1.3 does not work with 2.0+
  CLOSED: [2016-09-29 Thu 16:11]
https://github.com/elastic/kibana/issues/5637
#+BEGIN_EXAMPLE
root@449c96b42635:/var/log/supervisor# /opt/kibana/bin/kibana
{"name":"Kibana","hostname":"449c96b42635","pid":2521,"level":50,"err":{"message":"unknown error","name":"Error","stack":"Error: unknown error\n    at respond (/opt/kibana-4.1.2-linux-x64/src/node_modules/elasticsearch/src/lib/transport.js:237:15)\n    at checkRespForFailure (/opt/kibana-4.1.2-linux-x64/src/node_modules/elasticsearch/src/lib/transport.js:203:7)\n    at HttpConnector.<anonymous> (/opt/kibana-4.1.2-linux-x64/src/node_modules/elasticsearch/src/lib/connectors/http.js:156:7)\n    at IncomingMessage.bound (/opt/kibana-4.1.2-linux-x64/src/node_modules/elasticsearch/node_modules/lodash-node/modern/internals/baseBind.js:56:17)\n    at IncomingMessage.emit (events.js:117:20)\n    at _stream_readable.js:944:16\n    at process._tickCallback (node.js:442:13)"},"msg":"","time":"2016-09-29T07:39:41.632Z","v":0}
{"name":"Kibana","hostname":"449c96b42635","pid":2521,"level":60,"err":{"message":"unknown error","name":"Error","stack":"Error: unknown error\n    at respond (/opt/kibana-4.1.2-linux-x64/src/node_modules/elasticsearch/src/lib/transport.js:237:15)\n    at checkRespForFailure (/opt/kibana-4.1.2-linux-x64/src/node_modules/elasticsearch/src/lib/transport.js:203:7)\n    at HttpConnector.<anonymous> (/opt/kibana-4.1.2-linux-x64/src/node_modules/elasticsearch/src/lib/connectors/http.js:156:7)\n    at IncomingMessage.bound (/opt/kibana-4.1.2-linux-x64/src/node_modules/elasticsearch/node_modules/lodash-node/modern/internals/baseBind.js:56:17)\n    at IncomingMessage.emit (events.js:117:20)\n    at _stream_readable.js:944:16\n    at process._tickCallback (node.js:442:13)"},"msg":"","time":"2016-09-29T07:39:41.635Z","v":0}
#+END_EXAMPLE
** DONE kibana 4.0.1 fail to start: index_not_found_exception .kibana: incompatible version between elasticsearch and kibana
  CLOSED: [2016-09-29 Thu 16:11]
root@449c96b42635:/var/log/supervisor# /opt/kibana/bin/kibana
{"@timestamp":"2016-09-29T07:47:49.508Z","level":"error","message":{"root_cause":[{"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":".kibana","index":".kibana"}],"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":".kibana","index":".kibana"},"node_env":"production","error":{"message":{"root_cause":[{"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":".kibana","index":".kibana"}],"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":".kibana","index":".kibana"},"name":"Error","stack":"Error: [object Object]\n  at respond (/opt/kibana-4.0.1-linux-x64/src/node_modules/elasticsearch/src/lib/transport.js:235:15)\n  at checkRespForFailure (/opt/kibana-4.0.1-linux-x64/src/node_modules/elasticsearch/src/lib/transport.js:203:7)\n  at HttpConnector.<anonymous> (/opt/kibana-4.0.1-linux-x64/src/node_modules/elasticsearch/src/lib/connectors/http.js:156:7)\n  at IncomingMessage.bound (/opt/kibana-4.0.1-linux-x64/src/node_modules/elasticsearch/node_modules/lodash-node/modern/internals/baseBind.js:56:17)\n  at IncomingMessage.emit (events.js:117:20)\n  at _stream_readable.js:944:16\n  at process._tickCallback (node.js:442:13)\n"}}
{"@timestamp":"2016-09-29T07:47:49.511Z","level":"fatal","message":{"root_cause":[{"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":".kibana","index":".kibana"}],"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":".kibana","index":".kibana"},"node_env":"production","error":{"message":{"root_cause":[{"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":".kibana","index":".kibana"}],"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":".kibana","index":".kibana"},"name":"Error","stack":"Error: [object Object]\n  at respond (/opt/kibana-4.0.1-linux-x64/src/node_modules/elasticsearch/src/lib/transport.js:235:15)\n  at checkRespForFailure (/opt/kibana-4.0.1-linux-x64/src/node_modules/elasticsearch/src/lib/transport.js:203:7)\n  at HttpConnector.<anonymous> (/opt/kibana-4.0.1-linux-x64/src/node_modules/elasticsearch/src/lib/connectors/http.js:156:7)\n  at IncomingMessage.bound (/opt/kibana-4.0.1-linux-x64/src/node_modules/elasticsearch/node_modules/lodash-node/modern/internals/baseBind.js:56:17)\n  at IncomingMessage.emit (events.js:117:20)\n  at _stream_readable.js:944:16\n  at process._tickCallback (node.js:442:13)\n"}}

** #  --8<-------------------------- separator ------------------------>8--
** DONE [#A] logstash verify configuration file
  CLOSED: [2016-09-29 Thu 17:12]
cd /etc/logstash/conf.d/
/opt/logstash/bin/logstash -f *.conf --configtest

/opt/logstash/bin/logstash -f /etc/logstash/conf.d/data_report.conf --configtest

When logstash runs, it combines all the files in your config directory into one file.

cat /etc/logstash/conf.d/* > /tmp/total.conf

 /opt/logstash/bin/logstash --config /etc/logstash/conf.d/data_report.conf
** DONE logstash error: Error: Expected one of #, input, filter, output at line 1, column 1 (byte 1) after
  CLOSED: [2016-09-29 Thu 18:05]
https://discuss.elastic.co/t/logstash-configtest-error/54547
https://github.com/elastic/logstash/issues/2571
http://stackoverflow.com/questions/34164603/error-expected-one-of-input-filter-output-at-line-24-column-1-byte-528
http://stackoverflow.com/questions/28546289/expected-one-of-input-filter-output-at-line-2-column-1-byte-2-logstash

try this to debug: /opt/logstash/bin/logstash --config /etc/logstash/conf.d/data_report.conf

- incompatible version: logstash config file
- /opt/logstash/bin/logstash agent -f /etc/logstash/ --> /opt/logstash/bin/logstash agent -f /etc/logstash/conf.d

cd /etc/logstash/conf.d/
/opt/logstash/bin/logstash -f *.conf --configtest   

Probably you have some unsupported non-printed character in the original file. Incorrect EOL or whatever.

/opt/logstash/bin/logstash agent -f /etc/logstash/

/opt/logstash/bin/logstash  -f /etc/logstash/

/opt/logstash/bin/logstash --config /etc/logstash/conf.d/data_report.conf

#+BEGIN_EXAMPLE
root@cfac095b55b2:/var/log/logstash# tail -f ./stdout.log
{:timestamp=>"2016-09-29T09:03:56.246000+0000", :message=>"Error: Expected one of #, input, filter, output at line 1, column 1 (byte 1) after ", :level=>:error}
{:timestamp=>"2016-09-29T09:04:07.268000+0000", :message=>"Error: Expected one of #, input, filter, output at line 1, column 1 (byte 1) after ", :level=>:error}
{:timestamp=>"2016-09-29T09:04:18.045000+0000", :message=>"Error: Expected one of #, input, filter, output at line 1, column 1 (byte 1) after ", :level=>:error}
{:timestamp=>"2016-09-29T09:04:29.669000+0000", :message=>"Error: Expected one of #, input, filter, output at line 1, column 1 (byte 1) after ", :level=>:error}
{:timestamp=>"2016-09-29T09:04:41.271000+0000", :message=>"Error: Expected one of #, input, filter, output at line 1, column 1 (byte 1) after ", :level=>:error}
{:timestamp=>"2016-09-29T09:04:53.445000+0000", :message=>"Error: Expected one of #, input, filter, output at line 1, column 1 (byte 1) after ", :level=>:error}
{:timestamp=>"2016-09-29T09:05:03.876000+0000", :message=>"Error: Expected one of #, input, filter, output at line 1, column 1 (byte 1) after ", :level=>:error}
{:timestamp=>"2016-09-29T09:05:14.630000+0000", :message=>"Error: Expected one of #, input, filter, output at line 1, column 1 (byte 1) after ", :level=>:error}
{:timestamp=>"2016-09-29T09:05:25.452000+0000", :message=>"Error: Expected one of #, input, filter, output at line 1, column 1 (byte 1) after ", :level=>:error}
{:timestamp=>"2016-09-29T09:05:38.959000+0000", :message=>"Error: Expected one of #, input, filter, output at line 1, column 1 (byte 1) after ", :level=>:error}
#+END_EXAMPLE
** DONE logstash: plugin elasticsearch is obsolete
  CLOSED: [2016-09-29 Thu 18:04]
https://discuss.elastic.co/t/error-the-setting-host-in-plugin-elasticsearch-is-obsolete-and-is-no-longer-available-please-use-the-hosts-setting-instead-you-can-specify-multiple-entries-separated-by-comma-in-host-port-format/33489/3
https://github.com/elastic/beats/issues/535

root@cfac095b55b2:/var/log/logstash# /opt/logstash/bin/logstash --config /etc/logstash/conf.d/data_report.conf
Settings: Default pipeline workers: 8
Error: The setting `host` in plugin `elasticsearch` is obsolete and is no longer available. Please use the 'hosts' setting instead. You can specify multiple entries separated by comma in 'host:port' format. If you have any questions about this, you are invited to visit https://discuss.elastic.co/c/logstash and ask. {:level=>:error}
*** before
cat > /etc/logstash/conf.d/data_report.conf <<EOF
input {
  file {
    path => "/var/log/data_report.log"
    start_position => beginning
  }
}

filter {
    if [path] =~ "data_report" {
        grok {
            match => {"message" => "\[%{HTTPDATE:log_timestamp}\] %{NOTSPACE:item_name} %{NOTSPACE:property_name} %{NOTSPACE:property_value:float}"
            }
       }
       date {
          match => [ "log_timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
       }
   }
}

output {
  elasticsearch {
    host => localhost
  }
  stdout { codec => rubydebug }
}

EOF

*** after
cat > /etc/logstash/conf.d/data_report.conf <<EOF
input {
  file {
    path => "/var/log/data_report.log"
    start_position => beginning
  }
}

filter {
    if [path] =~ "data_report" {
        grok {
            match => {"message" => "\[%{HTTPDATE:log_timestamp}\] %{NOTSPACE:item_name} %{NOTSPACE:property_name} %{NOTSPACE:property_value:float}"
            }
       }
       date {
          match => [ "log_timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
       }
   }
}

output {
  elasticsearch {
    hosts => ['localhost:9200']
  }
  stdout { codec => rubydebug }
}
EOF
** DONE use logstash to convert format
  CLOSED: [2016-09-30 Fri 11:39]
http://blog.terminal.com/elk-for-log-analysis/

input {  
  file {
    path => "/var/log/apache2/*.log" # absolute path of your apache logs
    start_position => beginning # Live data: use 'end'. Existing data: use 'beginning'
  }
}
filter {  
  if [path] =~ "access" {
    mutate { replace => { "type" => "apache_access" } }
    grok {
      # Depends on the format of your log file
      match => { "message" => "%{COMBINEDAPACHELOG}" }
      #match => { "message" => "%{COMMONAPACHELOG}" }
    }
  } else if [path] =~ "error" {
    mutate { replace => { type => "apache_error" } }
  }
  date {
    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
    locale => "en"
  }
}
output {  
  # elasticsearch { host => localhost }
  file { path => "/tmp/logstash.out"}
}

** TODO logstash how to generate _id for elasticsearch
https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-document_id
https://www.elastic.co/guide/en/logstash/current/event-dependent-configuration.html#sprintf

http://stackoverflow.com/questions/34878609/how-do-i-replicate-the-id-and-type-of-elasticsearch-index-when-dumping-data-th
http://stackoverflow.com/questions/30391898/change-id-in-elasticsearch
http://stackoverflow.com/questions/25037289/how-to-deduplicate-documents-while-indexing-into-elasticsearch-from-logstash

document_id => "%{[@metadata][_id]}"

In the elasticsearch output you can set the document_id for the event
you are shipping. This will end up being the _id in elasticsearch. You
can use all sort of parameters / field references / ... that are
available in logstash config. Like so:

elasticsearch { 
    host => yourEsHost
    cluster => "yourCluster"
    index => "logstash-%{+YYYY.MM.dd}"
    document_id => "%{someFieldOfMyEvent}"
} 
In this example someFieldOfMyEvent ends up being the _id of this event in ES.
** DONE [#A] Programmatically set Kibana's default index pattern
  CLOSED: [2016-10-04 Tue 07:40]
http://stackoverflow.com/questions/36871862/programmatically-set-kibanas-default-index-pattern
https://github.com/elastic/kibana/issues/3709

https://www.elastic.co/guide/en/kibana/current/settings.html
https://www.elastic.co/guide/en/kibana/current/setup.html
https://www.elastic.co/guide/en/kibana/current/tutorial-define-index.html
https://github.com/elastic/kibana/issues/5447

** #  --8<-------------------------- separator ------------------------>8--
** DONE inject data directly to elasticsearch
   CLOSED: [2016-10-04 Tue 10:25]
curl -XPUT localhost:9200/logstash-2016.08.01/logs/AVd51KVF1vucSY-abfda -d '
{
    "message": "[01/Aug/2016:00:26:02 +0000] master-index-e4010da4110ba377d100f050cb4440db ESItemNum 961",
    "@version": "1",
    "@timestamp": "2016-08-01T00:26:02.000Z",
    "path": "/var/log/data_report.log",
    "host": "mytest",
    "log_timestamp": "01/Aug/2016:00:26:02 +0000",
    "item_name": "master-denny",
    "property_name": "ESItemNum",
    "property_value": 486
}'
** DONE enhance kibana authentication: password protection
  CLOSED: [2016-10-06 Thu 11:53]
Kibana itself doesn't support authentication or restricting access to dashboards.

- Reverse proxy by apache or nginx
- Kibana users have to log in when Shield is installed on your cluster
*** nginx vhost setup
http://www.ragingcomputer.com/2014/02/securing-elasticsearch-kibana-with-nginx
http://shairosenfeld.blogspot.sg/2011/03/authorization-header-in-nginx-for.html

apt-get install -y nginx apache2-utils

# create an apache user
htpasswd -c /etc/nginx/conf.d/kibana.htpasswd totvslabs

fluigdata

rm /etc/nginx/sites-enabled/default

service nginx stop
service nginx start
service nginx status
**** vim /etc/nginx/sites-enabled/kibana
# Nginx proxy for Kibana
#
# In this setup, we are password protecting the saving of dashboards. You may
# wish to extend the password protection to all paths.
#
# Even though these paths are being called as the result of an ajax request, the
# browser will prompt for a username/password on the first request
#
# If you use this, you'll want to point config.js at http://FQDN:80/ instead of
# http://FQDN:9200
#
 
server {
  listen                *:80 ;
 
  # server_name           mykibana.example.com;
  access_log            /var/log/nginx/kibana.access.log;
 
  location / {
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_pass http://127.0.0.1:5601;
    auth_basic "Restricted";
    auth_basic_user_file /etc/nginx/conf.d/kibana.htpasswd;
   }
}

*** useful link
http://www.ragingcomputer.com/2014/02/securing-elasticsearch-kibana-with-nginx

https://www.elastic.co/guide/en/x-pack/current/kibana.html
http://stackoverflow.com/questions/30138936/how-to-set-authentication-in-kibana
https://www.elastic.co/guide/en/kibana/current/production.html
http://blog.trifork.com/2015/03/05/shield-your-kibana-dashboards/
http://www.itzgeek.com/how-tos/linux/centos-how-tos/configure-kibana-4-with-nginx-securing-kibana-4-centos-7.html
https://github.com/elastic/kibana/issues/1559

** BYPASS kibana: create metric based on calculation of other metric: kibana/es don't support this
  CLOSED: [2016-10-13 Thu 07:53]
https://github.com/elastic/kibana/issues/3505
http://stackoverflow.com/questions/29174113/how-can-i-do-scripted-aggregation-in-kibana-elasticsearch
http://stackoverflow.com/questions/29652992/performing-calculations-on-multiple-metrics-counts-in-kibana-4

** BYPASS kibana limitation: can't show number of 0.0000001 so far
   CLOSED: [2016-10-13 Thu 08:42]

** TODO kibana dashboard takes quite a while to pull data

** DONE [#A] create your own document_id in logstash              :IMPORTANT:
  CLOSED: [2016-10-13 Thu 12:49]
https://discuss.elastic.co/t/how-to-create-my-own-document-id-in-logstash/1416/2

/opt/logstash/bin/logstash -f /opt/logstash/data_report.conf

cat /tmp/db_summary_report.txt >> /var/log/data_report.log

tail -f /var/log/logstash.log

> /opt/logstash/data_report.conf && vim /opt/logstash/data_report.conf

input {
  file {
    path => "/var/log/data_report.log"
    start_position => beginning
  }
}

filter {
    if [path] =~ "data_report" {
        grok {
            match => {"message" => "\[%{HTTPDATE:log_timestamp}\] %{NOTSPACE:item_name} %{NOTSPACE:property_name} %{NOTSPACE:property_value:float}"
            }
       }
       date {
          match => [ "log_timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
       }
       ruby {
          code => "require 'digest/md5';
          event['@metadata']['computed_id'] = Digest::MD5.hexdigest(event['message'])"
        }
   }
}

output {
  elasticsearch {
    hosts => ['localhost:9200']
    document_id => "%{[@metadata][computed_id]}"
  }
  stdout { codec => rubydebug }
}

* Sphinx                                                 :noexport:IMPORTANT:
http://sphinxsearch.com/docs/

sudo /usr/local/sphinx/bin/search -q 1 -c /usr/local/sphinx/etc/sphinx.conf -i 'idx_delta_customer_contactuser' 1

| Name                 | Summary                                                                                       |
|----------------------+-----------------------------------------------------------------------------------------------|
| 重建Index            | sudo /usr/local/sphinx/bin/indexer --config /usr/local/sphinx/etc/sphinx.conf --all           |
| 启动daemon           | /usr/local/sphinx/bin/searchd --config /usr/local/sphinx/etc/sphinx.conf                      |
| 关闭                 | /usr/local/sphinx/bin/searchd --config /usr/local/sphinx/etc/sphinx.conf --stop               |
| 对index,按关键字搜索 | /usr/local/sphinx/bin/search -i 'idx_delta_customer_contactuser' 18606871727                  |
| 查看状态             | /usr/local/sphinx/bin/searchd --status                                                        |
| 增量索引             | /usr/local/sphinx/bin/indexer  --config /usr/local/sphinx/etc/sphinx.conf idx_active --rotate |
| log文件              | tail -f /usr/local/sphinx/var/log/query.log                                                   |

./configure --prefix=/usr/local/sphinx --with-mysql=/usr/local/mysql
** install sphinx on mac
http://blog.locoy.com/read-30.html
brew install mysql
brew install mysql-connector-c
brew install sphinx
** ERROR: cannot find MySQL include files.
#+begin_example
checking for mysql_real_connect... no
checking for mysql_real_connect... no
checking MySQL include files... configure: error: missing include files.


ERROR: cannot find MySQL include files.

Check that you do have MySQL include files installed.
The package name is typically 'mysql-devel'.

If include files are installed on your system, but you are still getting
this message, you should do one of the following:

1) either specify includes location explicitly, using --with-mysql-includes;
2) or specify MySQL installation root location explicitly, using --with-mysql;
3) or make sure that the path to 'mysql_config' program is listed in
   your PATH environment variable.

To disable MySQL support, use --without-mysql option.
#+end_example
** DONE index --all fail: mkdir /var/data
   CLOSED: [2013-05-06 Mon 11:06]
#+begin_example
bash-3.2$ indexer --all
indexer --all
Sphinx 2.0.6-release (r3473)
Copyright (c) 2001-2012, Andrew Aksyonoff
Copyright (c) 2008-2012, Sphinx Technologies Inc (http://sphinxsearch.com)

using config file '/usr/local/etc/sphinx.conf'...
indexing index 'test1'...
FATAL: failed to open /var/data/test1.spl: No such file or directory, will not index. Try --rotate option.
#+end_example
** search test fail
#+begin_example
bash-3.2$ search test
search test
Sphinx 2.0.6-release (r3473)
Copyright (c) 2001-2012, Andrew Aksyonoff
Copyright (c) 2008-2012, Sphinx Technologies Inc (http://sphinxsearch.com)

using config file '/usr/local/etc/sphinx.conf'...
index 'test1': search error: .
#+end_example
* H2O: Fast Scalable Machine Learning                              :noexport:
http://www.h2o.ai/product/recommended-systems-for-h2o/
* apache spark: Lightning-fast cluster computing                   :noexport:
http://spark.apache.org

Spark is clearly being very much adopted for a specific set of use
cases, including pipelined data processing and parallelizable
data-science workloads. At the same time, SQL-on-Hadoop engines
(including Spark SQL, Impala, Presto, and Drill) are very much
critical and growing.

Hadoop will continue to be used as a platform for scale-out data
storage, parallel processing, and clustered workload management. Spark
will continue to be used for both batch-oriented and interactive
scale-out data-processing needs.

Spark is a fast data processing engine that works in the Hadoop
ecosystem, replacing MapReduce. It is designed to perform both batch
processing (similar to MapReduce) and new workloads like streaming,
interactive queries, and iterative algorithms, like those commonly
found in machine learning and graph processing.

Spark is written in Scala and it is pulling people towards Scala.

Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.

Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.
** TODO relationship between spark and hadoop
** install spark
cd /home/denny/spark
wget http://mirror.nus.edu.sg/apache/spark/spark-1.6.0/spark-1.6.0.tgz
* hadoop                                                 :noexport:IMPORTANT:
MapR   ---- Cloudera ---- Hortonworks
Proprietary             Mix                  OpenSource

hadoop is very helpful in storing and managing vast amounts of data cheaply and efficiently.

It's designed to be robust, in that your Big Data applications will
continue to run even when individual servers - or clusters - fail. And
it's also designed to be efficient, because it doesn't require your
applications to shuttle huge volumes of data across your network.

It has two main parts - a data processing framework and a distributed filesystem for data storage.

MapReduce runs as a series of jobs, with each job essentially a
separate Java application that goes out into the data and starts
pulling out information as needed.

- Items
| Name                       | Summary                                                                           |
|----------------------------+-----------------------------------------------------------------------------------|
| CDH                        | Cloudera's Distribution For Hadoop                                                |
| HDP                        | Hortonworks Hadoop                                                                |
|----------------------------+-----------------------------------------------------------------------------------|
| Blocks                     | 64 MB by default.                                                                 |
|                            | Unlike filesystem, a file in HDFS that is smaller than a single block             |
|                            | doesn't occupy a full block's worth of underlying storage                         |
|----------------------------+-----------------------------------------------------------------------------------|
| namenode                   | manages filesystem namespace                                                      |
| datanodes                  | Store and retrieve blocks, and they report back to namenode                       |
|                            | periodically with lists of blocks that they are storing                           |
| secondary namenode         | Periodically merge the namespace image with edit log to                           |
|                            | prevent the it becoming too large                                                 |
|----------------------------+-----------------------------------------------------------------------------------|
| Hadoop Archives(HAR files) | a file archiving facility that packs files into HDFS blocks more efficiently      |
| codec                      | implementation of a compression-decompression algorithm                           |
|----------------------------+-----------------------------------------------------------------------------------|
| hive                       | convert query language into MapReduce jobs                                        |
| pig                        | A high-level data-flow language and execution framework for parallel computation. |
| sqoop                      |                                                                                   |
| HCatalog                   |                                                                                   |
| yarn                       | for resource allocation and management                                            |

- Components
| Name                          | Summary                 |
|-------------------------------+-------------------------|
| jps                           |                         |
| NameNode                      | http://localhost:50070/ |
| JobTracker                    | http://localhost:50030/ |
| ./bin/hadoop dfs -ls /        |                         |
| ./bin/hadoop namenode -format |                         |
** hdfs pros and cons
Design for:
- Very large files
- Streaming data access
- Commodity hardware

Not good for:
- Low-latency data access
  HDFS is optimized for delievering a high throughput of data, and
  this may be at the expense of latency. HBase is currently a better
  choice for low-latency access

- Lots of small files
  Since the namenode holds filesystem metadata in memory, the limit to
  the number of files in a filesystem is governed by the amount of
  memory on the namenode.

  As a rule of thumb, each file, directory, and block takes about 150
  bytes.

- Multiple writers, abitrary file modfications
  Files in HDFS may be written to a single writer. Writes are always
  made at the end of the file.
** MapReduce
When a request for information comes in, MapReduce uses two components
- JobTracker that sits on the Hadoop master node
- TaskTrackers that sit out on each node within the Hadoop network.
** #  --8<-------------------------- separator ------------------------>8--
** Hadoop on OSX "Unable to load realm info from SCDynamicStore"
http://blog.btnotes.com/articles/390.html
#+begin_example
在conf/hadoop-env.sh文件中加上

export HADOOP_OPTS="-Djava.security.krb5.realm=OX.AC.UK -Djava.security.krb5.kdc=kdc0.ox.ac.uk:kdc1.ox.ac.uk"

#+end_example
** conf/hadoop-env.sh
export JAVA_HOME="/Library/Java/Home"
export HADOOP_OPTS="-Djava.security.krb5.realm=OX.AC.UK -Djava.security.krb5.kdc=kdc0.ox.ac.uk:kdc1.ox.ac.uk"
** vim ./conf/core-site.xml
#+begin_example
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!-- Put site-specific property overrides in this file. -->
<configuration>
<property>
<name>hadoop.tmp.dir</name>
<value>/Users/hadoop/hadoop-1.2.0/hadoop-${user.name}</value>
<description>A base for other temporary directories.</description>
</property>
<property>
<name>fs.default.name</name>
<value>hdfs://localhost:9000</value>
<description>The name of the default file system. A URI whose
scheme and authority determine the FileSystem implementation. The
uri's scheme determines the config property (fs.SCHEME.impl) naming
the FileSystem implementation class. The uri's authority is used to
determine the host, port, etc. for a filesystem.</description>
</property>
<property>
<name>mapred.job.tracker</name>
<value>localhost:9001</value>
<description>The host and port that the MapReduce job tracker runs
at. If "local", then jobs are run in-process as a single map
and reduce task.
</description>
</property>
<property>
<name>mapred.tasktracker.tasks.maximum</name>
<value>8</value>
<description>The maximum number of tasks that will be run simultaneously by a
a task tracker
</description>
</property>
<property>
<name>dfs.replication</name>
<value>1</value>
<description>Default block replication.
The actual number of replications can be specified when the file is created.
The default is used if replication is not specified in create time.
</description>
</property>
</configuration>

#+end_example
** #  --8<-------------------------- separator ------------------------>8--
** DONE [#A] format namenode: hdfs namenode -format
  CLOSED: [2016-04-21 Thu 18:47]
http://stackoverflow.com/questions/30688011/hadoop-name-node-format-fails

hdfs namenode -format
service hadoop-hdfs-namenode status

su hdfs
rm -rf /app/
mkdir -p /app/hadoop/tmp/dfs/name
chown hdfs:hdfs -R /app

hdfs namenode -format

service hadoop-hdfs-namenode status
service hadoop-hdfs-namenode stop
service hadoop-hdfs-namenode start

#+BEGIN_EXAMPLE
root@ccccce86c70d:/var/log/hadoop/hdfs# tail -n 20 hadoop-hdfs-namenode-ccccce86c70d.log
<hadoop/hdfs# tail -n 20 hadoop-hdfs-namenode-ccccce86c70d.log
2016-04-21 08:40:57,787 INFO org.mortbay.log: Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2016-04-21 08:40:57,789 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...
2016-04-21 08:40:57,790 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system stopped.
2016-04-21 08:40:57,790 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system shutdown complete.
2016-04-21 08:40:57,791 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
java.io.IOException: NameNode is not formatted.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:225)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:983)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:688)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:662)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:722)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:951)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:935)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1641)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1707)
2016-04-21 08:40:57,794 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2016-04-21 08:40:57,797 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at ccccce86c70d/172.17.0.5
************************************************************/
root@ccccce86c70d:/var/log/hadoop/hdfs#
#+END_EXAMPLE

** [#A] hdfs help
#+BEGIN_EXAMPLE
]0;root@myhadoopaio: /usr/local/hadoop/binroot@myhadoopaio:/usr/local/hadoop/bin# ./hdfs --help
./hdfs --help
Usage: hdfs [--config confdir] [--loglevel loglevel] COMMAND
       where COMMAND is one of:
  dfs                  run a filesystem command on the file systems supported in Hadoop.
  classpath            prints the classpath
  namenode -format     format the DFS filesystem
  secondarynamenode    run the DFS secondary namenode
  namenode             run the DFS namenode
  journalnode          run the DFS journalnode
  zkfc                 run the ZK Failover Controller daemon
  datanode             run a DFS datanode
  dfsadmin             run a DFS admin client
  haadmin              run a DFS HA admin client
  fsck                 run a DFS filesystem checking utility
  balancer             run a cluster balancing utility
  jmxget               get JMX exported values from NameNode or DataNode.
  mover                run a utility to move block replicas across
                       storage types
  oiv                  apply the offline fsimage viewer to an fsimage
  oiv_legacy           apply the offline fsimage viewer to an legacy fsimage
  oev                  apply the offline edits viewer to an edits file
  fetchdt              fetch a delegation token from the NameNode
  getconf              get config values from configuration
  groups               get the groups which users belong to
  snapshotDiff         diff two snapshots of a directory or diff the
                       current directory contents with a snapshot
  lsSnapshottableDir   list all snapshottable dirs owned by the current user
						Use -help to see options
  portmap              run a portmap service
  nfs3                 run an NFS version 3 gateway
  cacheadmin           configure the HDFS cache
  crypto               configure HDFS encryption zones
  storagepolicies      list/get/set block storage policies
  version              print the version

Most commands print help when invoked w/o parameters.
]0;root@myhadoopaio: /usr/local/hadoop/binroot@myhadoopaio:/usr/local/hadoop/bin# pwd
pwd
/usr/local/hadoop/bin
#+END_EXAMPLE
** hdfs helloworld
#+BEGIN_EXAMPLE
# Name node service: http://$server_ip:50070/dfshealth.html#tab-overview
# Hadoop configuration
# ls /etc/hadoop

# try hdfs
su hdfs
hdfs dfs -mkdir /user/denny
hdfs dfs -touchz /user/denny/abc
hdfs dfs -put /etc/hosts /user/denny/hosts
hdfs dfs -tail /user/denny/hosts
hdfs dfs -ls /user/denny

#+END_EXAMPLE
** #  --8<-------------------------- separator ------------------------>8--
** ./bin/hadoop namenode -format
#+begin_example
bash-3.2$ ./bin/hadoop namenode -format
13/07/29 00:18:14 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = localhost/127.0.0.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 1.2.0
STARTUP_MSG:   build = https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.2 -r 1479473; compiled by 'hortonfo' on Mon May  6 06:59:37 UTC 2013
STARTUP_MSG:   java = 1.6.0_51
************************************************************/
13/07/29 00:18:14 INFO util.GSet: Computing capacity for map BlocksMap
13/07/29 00:18:14 INFO util.GSet: VM type       = 64-bit
13/07/29 00:18:14 INFO util.GSet: 2.0% max memory = 1039859712
13/07/29 00:18:14 INFO util.GSet: capacity      = 2^21 = 2097152 entries
13/07/29 00:18:14 INFO util.GSet: recommended=2097152, actual=2097152
13/07/29 00:18:15 INFO namenode.FSNamesystem: fsOwner=hadoop
13/07/29 00:18:15 INFO namenode.FSNamesystem: supergroup=supergroup
13/07/29 00:18:15 INFO namenode.FSNamesystem: isPermissionEnabled=true
13/07/29 00:18:15 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
13/07/29 00:18:15 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
13/07/29 00:18:15 INFO namenode.FSEditLog: dfs.namenode.edits.toleration.length = 0
13/07/29 00:18:15 INFO namenode.NameNode: Caching file names occuring more than 10 times
13/07/29 00:18:15 INFO common.Storage: Image file of size 112 saved in 0 seconds.
13/07/29 00:18:15 INFO namenode.FSEditLog: closing edit log: position=4, editlog=/Users/hadoop/hadoop-1.2.0/hadoop-hadoop/dfs/name/current/edits
13/07/29 00:18:15 INFO namenode.FSEditLog: close success: truncate to 4, editlog=/Users/hadoop/hadoop-1.2.0/hadoop-hadoop/dfs/name/current/edits
13/07/29 00:18:15 INFO common.Storage: Storage directory /Users/hadoop/hadoop-1.2.0/hadoop-hadoop/dfs/name has been successfully formatted.
13/07/29 00:18:15 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at localhost/127.0.0.1
************************************************************/

#+end_example
** TODO ./bin/hadoop dfs -copyFromLocal conf input
http://comments.gmane.org/gmane.comp.jakarta.lucene.hadoop.user/17088
#+begin_example
bash-3.2$ ./bin/hadoop dfs -copyFromLocal conf input
copyFromLocal: org.apache.hadoop.security.AccessControlException: Permission denied: user=hadoop, access=WRITE, inode="":root:supergroup:rwxr-xr-x
#+end_example

<property>
<name>dfs.permissions</name>
<value>false</value>
</property>
** useful link
http://readwrite.com/2013/05/23/hadoop-what-it-is-and-how-it-works/
Hadoop: What It Is And How It Works
http://readwrite.com/2015/12/02/spark-hadoop-business-intelligence/
Why Spark And Hadoop Are Both Here To Stay
** DONE hadoop help usage
  CLOSED: [2016-04-18 Mon 18:32]
#+BEGIN_EXAMPLE
]0;root@myhadoopaio: /usr/local/hadoop/binroot@myhadoopaio:/usr/local/hadoop/bin# /usr/local/hadoop/bin/hadoop --help
/usr/local/hadoop/bin/hadoop --help
Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]
  CLASSNAME            run the class named CLASSNAME
 or
  where COMMAND is one of:
  fs                   run a generic filesystem user client
  version              print the version
  jar <jar>            run a jar file
                       note: please use "yarn jar" to launch
                             YARN applications, not this command.
  checknative [-a|-h]  check native hadoop and compression libraries availability
  distcp <srcurl> <desturl> copy file or directories recursively
  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive
  classpath            prints the class path needed to get the
  credential           interact with credential providers
                       Hadoop jar and the required libraries
  daemonlog            get/set the log level for each daemon
  trace                view and modify Hadoop tracing settings

Most commands print help when invoked w/o parameters.
]0;root@myhadoopaio: /usr/local/hadoop/binroot@myhadoopaio:/usr/local/hadoop/bin#
#+END_EXAMPLE
** DONE hdfs help usage
  CLOSED: [2016-04-18 Mon 18:33]
#+BEGIN_EXAMPLE
]0;root@myhadoopaio: /usr/local/hadoop/binroot@myhadoopaio:/usr/local/hadoop/bin# /usr/local/hadoop/bin/hdfs --help
/usr/local/hadoop/bin/hdfs --help
Usage: hdfs [--config confdir] [--loglevel loglevel] COMMAND
       where COMMAND is one of:
  dfs                  run a filesystem command on the file systems supported in Hadoop.
  classpath            prints the classpath
  namenode -format     format the DFS filesystem
  secondarynamenode    run the DFS secondary namenode
  namenode             run the DFS namenode
  journalnode          run the DFS journalnode
  zkfc                 run the ZK Failover Controller daemon
  datanode             run a DFS datanode
  dfsadmin             run a DFS admin client
  haadmin              run a DFS HA admin client
  fsck                 run a DFS filesystem checking utility
  balancer             run a cluster balancing utility
  jmxget               get JMX exported values from NameNode or DataNode.
  mover                run a utility to move block replicas across
                       storage types
  oiv                  apply the offline fsimage viewer to an fsimage
  oiv_legacy           apply the offline fsimage viewer to an legacy fsimage
  oev                  apply the offline edits viewer to an edits file
  fetchdt              fetch a delegation token from the NameNode
  getconf              get config values from configuration
  groups               get the groups which users belong to
  snapshotDiff         diff two snapshots of a directory or diff the
                       current directory contents with a snapshot
  lsSnapshottableDir   list all snapshottable dirs owned by the current user
						Use -help to see options
  portmap              run a portmap service
  nfs3                 run an NFS version 3 gateway
  cacheadmin           configure the HDFS cache
  crypto               configure HDFS encryption zones
  storagepolicies      list/get/set block storage policies
  version              print the version

Most commands print help when invoked w/o parameters.
#+END_EXAMPLE
** DONE hdfs dfs help usage
  CLOSED: [2016-04-18 Mon 18:36]
#+BEGIN_EXAMPLE
]0;root@myhadoopaio: /usr/local/hadooproot@myhadoopaio:/usr/local/hadoop# bin/hdfs dfs -help
bin/hdfs dfs -help
Usage: hadoop fs [generic options]
	[-appendToFile <localsrc> ... <dst>]
	[-cat [-ignoreCrc] <src> ...]
	[-checksum <src> ...]
	[-chgrp [-R] GROUP PATH...]
	[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
	[-chown [-R] [OWNER][:[GROUP]] PATH...]
	[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]
	[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
	[-count [-q] [-h] <path> ...]
	[-cp [-f] [-p | -p[topax]] <src> ... <dst>]
	[-createSnapshot <snapshotDir> [<snapshotName>]]
	[-deleteSnapshot <snapshotDir> <snapshotName>]
	[-df [-h] [<path> ...]]
	[-du [-s] [-h] <path> ...]
	[-expunge]
	[-find <path> ... <expression> ...]
	[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
	[-getfacl [-R] <path>]
	[-getfattr [-R] {-n name | -d} [-e en] <path>]
	[-getmerge [-nl] <src> <localdst>]
	[-help [cmd ...]]
	[-ls [-d] [-h] [-R] [<path> ...]]
	[-mkdir [-p] <path> ...]
	[-moveFromLocal <localsrc> ... <dst>]
	[-moveToLocal <src> <localdst>]
	[-mv <src> ... <dst>]
	[-put [-f] [-p] [-l] <localsrc> ... <dst>]
	[-renameSnapshot <snapshotDir> <oldName> <newName>]
	[-rm [-f] [-r|-R] [-skipTrash] <src> ...]
	[-rmdir [--ignore-fail-on-non-empty] <dir> ...]
	[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]
	[-setfattr {-n name [-v value] | -x name} <path>]
	[-setrep [-R] [-w] <rep> <path> ...]
	[-stat [format] <path> ...]
	[-tail [-f] <file>]
	[-test -[defsz] <path>]
	[-text [-ignoreCrc] <src> ...]
	[-touchz <path> ...]
	[-truncate [-w] <length> <path> ...]
	[-usage [cmd ...]]

-appendToFile <localsrc> ... <dst> :
  Appends the contents of all the given local files to the given dst file. The dst
  file will be created if it does not exist. If <localSrc> is -, then the input is
  read from stdin.

-cat [-ignoreCrc] <src> ... :
  Fetch all files that match the file pattern <src> and display their content on
  stdout.

-checksum <src> ... :
  Dump checksum information for files that match the file pattern <src> to stdout.
  Note that this requires a round-trip to a datanode storing each block of the
  file, and thus is not efficient to run on a large number of files. The checksum
  of a file depends on its content, block size and the checksum algorithm and
  parameters used for creating the file.

-chgrp [-R] GROUP PATH... :
  This is equivalent to -chown ... :GROUP ...

-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH... :
  Changes permissions of a file. This works similar to the shell's chmod command
  with a few exceptions.

  -R           modifies the files recursively. This is the only option currently
               supported.
  <MODE>       Mode is the same as mode used for the shell's command. The only
               letters recognized are 'rwxXt', e.g. +t,a+r,g-w,+rwx,o=r.
  <OCTALMODE>  Mode specifed in 3 or 4 digits. If 4 digits, the first may be 1 or
               0 to turn the sticky bit on or off, respectively.  Unlike the
               shell command, it is not possible to specify only part of the
               mode, e.g. 754 is same as u=rwx,g=rx,o=r.

  If none of 'augo' is specified, 'a' is assumed and unlike the shell command, no
  umask is applied.

-chown [-R] [OWNER][:[GROUP]] PATH... :
  Changes owner and group of a file. This is similar to the shell's chown command
  with a few exceptions.

  -R  modifies the files recursively. This is the only option currently
      supported.

  If only the owner or group is specified, then only the owner or group is
  modified. The owner and group names may only consist of digits, alphabet, and
  any of [-_./@a-zA-Z0-9]. The names are case sensitive.

  WARNING: Avoid using '.' to separate user name and group though Linux allows it.
  If user names have dots in them and you are using local file system, you might
  see surprising results since the shell command 'chown' is used for local files.

-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst> :
  Identical to the -put command.

-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst> :
  Identical to the -get command.

-count [-q] [-h] <path> ... :
  Count the number of directories, files and bytes under the paths
  that match the specified file pattern.  The output columns are:
  DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME or
  QUOTA REMAINING_QUOTA SPACE_QUOTA REMAINING_SPACE_QUOTA
        DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME
  The -h option shows file sizes in human readable format.

-cp [-f] [-p | -p[topax]] <src> ... <dst> :
  Copy files that match the file pattern <src> to a destination.  When copying
  multiple files, the destination must be a directory. Passing -p preserves status
  [topax] (timestamps, ownership, permission, ACLs, XAttr). If -p is specified
  with no <arg>, then preserves timestamps, ownership, permission. If -pa is
  specified, then preserves permission also because ACL is a super-set of
  permission. Passing -f overwrites the destination if it already exists. raw
  namespace extended attributes are preserved if (1) they are supported (HDFS
  only) and, (2) all of the source and target pathnames are in the /.reserved/raw
  hierarchy. raw namespace xattr preservation is determined solely by the presence
  (or absence) of the /.reserved/raw prefix and not by the -p option.

-createSnapshot <snapshotDir> [<snapshotName>] :
  Create a snapshot on a directory

-deleteSnapshot <snapshotDir> <snapshotName> :
  Delete a snapshot from a directory

-df [-h] [<path> ...] :
  Shows the capacity, free and used space of the filesystem. If the filesystem has
  multiple partitions, and no path to a particular partition is specified, then
  the status of the root partitions will be shown.

  -h  Formats the sizes of files in a human-readable fashion rather than a number
      of bytes.

-du [-s] [-h] <path> ... :
  Show the amount of space, in bytes, used by the files that match the specified
  file pattern. The following flags are optional:

  -s  Rather than showing the size of each individual file that matches the
      pattern, shows the total (summary) size.
  -h  Formats the sizes of files in a human-readable fashion rather than a number
      of bytes.

  Note that, even without the -s option, this only shows size summaries one level
  deep into a directory.

  The output is in the form
  	size	name(full path)

-expunge :
  Empty the Trash

-find <path> ... <expression> ... :
  Finds all files that match the specified expression and
  applies selected actions to them. If no <path> is specified
  then defaults to the current working directory. If no
  expression is specified then defaults to -print.

  The following primary expressions are recognised:
    -name pattern
    -iname pattern
      Evaluates as true if the basename of the file matches the
      pattern using standard file system globbing.
      If -iname is used then the match is case insensitive.

    -print
    -print0
      Always evaluates to true. Causes the current pathname to be
      written to standard output followed by a newline. If the -print0
      expression is used then an ASCII NULL character is appended rather
      than a newline.

  The following operators are recognised:
    expression -a expression
    expression -and expression
    expression expression
      Logical AND operator for joining two expressions. Returns
      true if both child expressions return true. Implied by the
      juxtaposition of two expressions and so does not need to be
      explicitly specified. The second expression will not be
      applied if the first fails.

-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst> :
  Copy files that match the file pattern <src> to the local name.  <src> is kept.
  When copying multiple files, the destination must be a directory. Passing -p
  preserves access and modification times, ownership and the mode.

-getfacl [-R] <path> :
  Displays the Access Control Lists (ACLs) of files and directories. If a
  directory has a default ACL, then getfacl also displays the default ACL.

  -R      List the ACLs of all files and directories recursively.
  <path>  File or directory to list.

-getfattr [-R] {-n name | -d} [-e en] <path> :
  Displays the extended attribute names and values (if any) for a file or
  directory.

  -R             Recursively list the attributes for all files and directories.
  -n name        Dump the named extended attribute value.
  -d             Dump all extended attribute values associated with pathname.
  -e <encoding>  Encode values after retrieving them.Valid encodings are "text",
                 "hex", and "base64". Values encoded as text strings are enclosed
                 in double quotes ("), and values encoded as hexadecimal and
                 base64 are prefixed with 0x and 0s, respectively.
  <path>         The file or directory.

-getmerge [-nl] <src> <localdst> :
  Get all the files in the directories that match the source file pattern and
  merge and sort them to only one file on local fs. <src> is kept.

  -nl  Add a newline character at the end of each file.

-help [cmd ...] :
  Displays help for given command or all commands if none is specified.

-ls [-d] [-h] [-R] [<path> ...] :
  List the contents that match the specified file pattern. If path is not
  specified, the contents of /user/<currentUser> will be listed. Directory entries
  are of the form:
  	permissions - userId groupId sizeOfDirectory(in bytes)
  modificationDate(yyyy-MM-dd HH:mm) directoryName

  and file entries are of the form:
  	permissions numberOfReplicas userId groupId sizeOfFile(in bytes)
  modificationDate(yyyy-MM-dd HH:mm) fileName

  -d  Directories are listed as plain files.
  -h  Formats the sizes of files in a human-readable fashion rather than a number
      of bytes.
  -R  Recursively list the contents of directories.

-mkdir [-p] <path> ... :
  Create a directory in specified location.

  -p  Do not fail if the directory already exists

-moveFromLocal <localsrc> ... <dst> :
  Same as -put, except that the source is deleted after it's copied.

-moveToLocal <src> <localdst> :
  Not implemented yet

-mv <src> ... <dst> :
  Move files that match the specified file pattern <src> to a destination <dst>.
  When moving multiple files, the destination must be a directory.

-put [-f] [-p] [-l] <localsrc> ... <dst> :
  Copy files from the local file system into fs. Copying fails if the file already
  exists, unless the -f flag is given.
  Flags:

  -p  Preserves access and modification times, ownership and the mode.
  -f  Overwrites the destination if it already exists.
  -l  Allow DataNode to lazily persist the file to disk. Forces
         replication factor of 1. This flag will result in reduced
         durability. Use with care.

-renameSnapshot <snapshotDir> <oldName> <newName> :
  Rename a snapshot from oldName to newName

-rm [-f] [-r|-R] [-skipTrash] <src> ... :
  Delete all files that match the specified file pattern. Equivalent to the Unix
  command "rm <src>"

  -skipTrash  option bypasses trash, if enabled, and immediately deletes <src>
  -f          If the file does not exist, do not display a diagnostic message or
              modify the exit status to reflect an error.
  -[rR]       Recursively deletes directories

-rmdir [--ignore-fail-on-non-empty] <dir> ... :
  Removes the directory entry specified by each directory argument, provided it is
  empty.

-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>] :
  Sets Access Control Lists (ACLs) of files and directories.
  Options:

  -b          Remove all but the base ACL entries. The entries for user, group
              and others are retained for compatibility with permission bits.
  -k          Remove the default ACL.
  -R          Apply operations to all files and directories recursively.
  -m          Modify ACL. New entries are added to the ACL, and existing entries
              are retained.
  -x          Remove specified ACL entries. Other ACL entries are retained.
  --set       Fully replace the ACL, discarding all existing entries. The
              <acl_spec> must include entries for user, group, and others for
              compatibility with permission bits.
  <acl_spec>  Comma separated list of ACL entries.
  <path>      File or directory to modify.

-setfattr {-n name [-v value] | -x name} <path> :
  Sets an extended attribute name and value for a file or directory.

  -n name   The extended attribute name.
  -v value  The extended attribute value. There are three different encoding
            methods for the value. If the argument is enclosed in double quotes,
            then the value is the string inside the quotes. If the argument is
            prefixed with 0x or 0X, then it is taken as a hexadecimal number. If
            the argument begins with 0s or 0S, then it is taken as a base64
            encoding.
  -x name   Remove the extended attribute.
  <path>    The file or directory.

-setrep [-R] [-w] <rep> <path> ... :
  Set the replication level of a file. If <path> is a directory then the command
  recursively changes the replication factor of all files under the directory tree
  rooted at <path>.

  -w  It requests that the command waits for the replication to complete. This
      can potentially take a very long time.
  -R  It is accepted for backwards compatibility. It has no effect.

-stat [format] <path> ... :
  Print statistics about the file/directory at <path>
  in the specified format. Format accepts filesize in
  blocks (%b), type (%F), group name of owner (%g),
  name (%n), block size (%o), replication (%r), user name
  of owner (%u), modification date (%y, %Y).
  %y shows UTC date as "yyyy-MM-dd HH:mm:ss" and
  %Y shows milliseconds since January 1, 1970 UTC.
  If the format is not specified, %y is used by default.

-tail [-f] <file> :
  Show the last 1KB of the file.

  -f  Shows appended data as the file grows.

-test -[defsz] <path> :
  Answer various questions about <path>, with result via exit status.
    -d  return 0 if <path> is a directory.
    -e  return 0 if <path> exists.
    -f  return 0 if <path> is a file.
    -s  return 0 if file <path> is greater than zero bytes in size.
    -z  return 0 if file <path> is zero bytes in size, else return 1.

-text [-ignoreCrc] <src> ... :
  Takes a source file and outputs the file in text format.
  The allowed formats are zip and TextRecordInputStream and Avro.

-touchz <path> ... :
  Creates a file of zero length at <path> with current time as the timestamp of
  that <path>. An error is returned if the file exists with non-zero length

-truncate [-w] <length> <path> ... :
  Truncate all files that match the specified file pattern to the specified
  length.

  -w  Requests that the command wait for block recovery to complete, if
      necessary.

-usage [cmd ...] :
  Displays the usage for given command or all commands if none is specified.

Generic options supported are
-conf <configuration file>     specify an application configuration file
-D <property=value>            use value for given property
-fs <local|namenode:port>      specify a namenode
-jt <local|resourcemanager:port>    specify a ResourceManager
-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster
-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.
-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.

The general command line syntax is
bin/hadoop command [genericOptions] [commandOptions]

#+END_EXAMPLE
** DONE [#A] run mapreduce job                                    :IMPORTANT:
  CLOSED: [2016-04-18 Mon 18:50]
su hdfs

cd /usr/local/hadoop
bin/hdfs dfs -mkdir /user/

bin/hdfs dfs -mkdir /user/root

# prepare input
bin/hdfs dfs -put etc/hadoop /user/root/input

# run mapr job
bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.0.jar grep /user/root/input output 'dfs[a-z.]+'

# get output
bin/hdfs dfs -get output output
cat ./output/*
*** run mapr
#+BEGIN_EXAMPLE
]0;root@myhadoopaio: /usr/local/hadooproot@myhadoopaio:/usr/local/hadoop# bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.0.jar grep input output 'dfs[a-z.]+'
op-mapreduce-examples-2.7.0.jar grep input output 'dfs[a-z.]+'
16/04/18 10:47:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/04/18 10:47:44 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/04/18 10:47:45 INFO input.FileInputFormat: Total input paths to process : 30
16/04/18 10:47:45 INFO mapreduce.JobSubmitter: number of splits:30
16/04/18 10:47:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1460969835862_0002
16/04/18 10:47:46 INFO impl.YarnClientImpl: Submitted application application_1460969835862_0002
16/04/18 10:47:46 INFO mapreduce.Job: The url to track the job: http://myhadoopaio:8088/proxy/application_1460969835862_0002/
16/04/18 10:47:46 INFO mapreduce.Job: Running job: job_1460969835862_0002
16/04/18 10:47:55 INFO mapreduce.Job: Job job_1460969835862_0002 running in uber mode : false
16/04/18 10:47:55 INFO mapreduce.Job:  map 0% reduce 0%
16/04/18 10:48:08 INFO mapreduce.Job:  map 3% reduce 0%
16/04/18 10:48:09 INFO mapreduce.Job:  map 20% reduce 0%
16/04/18 10:48:19 INFO mapreduce.Job:  map 33% reduce 0%
16/04/18 10:48:20 INFO mapreduce.Job:  map 40% reduce 0%
16/04/18 10:48:29 INFO mapreduce.Job:  map 47% reduce 0%
16/04/18 10:48:30 INFO mapreduce.Job:  map 57% reduce 0%
16/04/18 10:48:32 INFO mapreduce.Job:  map 57% reduce 19%
16/04/18 10:48:38 INFO mapreduce.Job:  map 60% reduce 19%
16/04/18 10:48:39 INFO mapreduce.Job:  map 73% reduce 19%
16/04/18 10:48:41 INFO mapreduce.Job:  map 73% reduce 24%
16/04/18 10:48:46 INFO mapreduce.Job:  map 77% reduce 24%
16/04/18 10:48:47 INFO mapreduce.Job:  map 83% reduce 24%
16/04/18 10:48:48 INFO mapreduce.Job:  map 87% reduce 24%
16/04/18 10:48:49 INFO mapreduce.Job:  map 90% reduce 24%
16/04/18 10:48:50 INFO mapreduce.Job:  map 90% reduce 30%
16/04/18 10:48:53 INFO mapreduce.Job:  map 100% reduce 30%
16/04/18 10:48:54 INFO mapreduce.Job:  map 100% reduce 100%
16/04/18 10:48:55 INFO mapreduce.Job: Job job_1460969835862_0002 completed successfully
16/04/18 10:48:55 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=409
		FILE: Number of bytes written=3572684
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=81943
		HDFS: Number of bytes written=513
		HDFS: Number of read operations=93
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters
		Launched map tasks=30
		Launched reduce tasks=1
		Data-local map tasks=30
		Total time spent by all maps in occupied slots (ms)=261849
		Total time spent by all reduces in occupied slots (ms)=34971
		Total time spent by all map tasks (ms)=261849
		Total time spent by all reduce tasks (ms)=34971
		Total vcore-seconds taken by all map tasks=261849
		Total vcore-seconds taken by all reduce tasks=34971
		Total megabyte-seconds taken by all map tasks=268133376
		Total megabyte-seconds taken by all reduce tasks=35810304
	Map-Reduce Framework
		Map input records=2108
		Map output records=26
		Map output bytes=650
		Map output materialized bytes=583
		Input split bytes=3594
		Combine input records=26
		Combine output records=15
		Reduce input groups=13
		Reduce shuffle bytes=583
		Reduce input records=15
		Reduce output records=13
		Spilled Records=30
		Shuffled Maps =30
		Failed Shuffles=0
		Merged Map outputs=30
		GC time elapsed (ms)=2230
		CPU time spent (ms)=15060
		Physical memory (bytes) snapshot=9440362496
		Virtual memory (bytes) snapshot=27172757504
		Total committed heap usage (bytes)=6076497920
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters
		Bytes Read=78349
	File Output Format Counters
		Bytes Written=513
16/04/18 10:48:55 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/04/18 10:48:55 INFO input.FileInputFormat: Total input paths to process : 1
16/04/18 10:48:55 INFO mapreduce.JobSubmitter: number of splits:1
16/04/18 10:48:55 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1460969835862_0003
16/04/18 10:48:55 INFO impl.YarnClientImpl: Submitted application application_1460969835862_0003
16/04/18 10:48:55 INFO mapreduce.Job: The url to track the job: http://myhadoopaio:8088/proxy/application_1460969835862_0003/
16/04/18 10:48:55 INFO mapreduce.Job: Running job: job_1460969835862_0003
16/04/18 10:49:07 INFO mapreduce.Job: Job job_1460969835862_0003 running in uber mode : false
16/04/18 10:49:07 INFO mapreduce.Job:  map 0% reduce 0%
16/04/18 10:49:13 INFO mapreduce.Job:  map 100% reduce 0%
16/04/18 10:49:18 INFO mapreduce.Job:  map 100% reduce 100%
16/04/18 10:49:18 INFO mapreduce.Job: Job job_1460969835862_0003 completed successfully
16/04/18 10:49:18 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=355
		FILE: Number of bytes written=230063
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=643
		HDFS: Number of bytes written=245
		HDFS: Number of read operations=7
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=3796
		Total time spent by all reduces in occupied slots (ms)=3601
		Total time spent by all map tasks (ms)=3796
		Total time spent by all reduce tasks (ms)=3601
		Total vcore-seconds taken by all map tasks=3796
		Total vcore-seconds taken by all reduce tasks=3601
		Total megabyte-seconds taken by all map tasks=3887104
		Total megabyte-seconds taken by all reduce tasks=3687424
	Map-Reduce Framework
		Map input records=13
		Map output records=13
		Map output bytes=323
		Map output materialized bytes=355
		Input split bytes=130
		Combine input records=0
		Combine output records=0
		Reduce input groups=5
		Reduce shuffle bytes=355
		Reduce input records=13
		Reduce output records=13
		Spilled Records=26
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=49
		CPU time spent (ms)=1260
		Physical memory (bytes) snapshot=533241856
		Virtual memory (bytes) snapshot=1774747648
		Total committed heap usage (bytes)=356515840
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters
		Bytes Read=513
	File Output Format Counters
		Bytes Written=245
#+END_EXAMPLE
*** mapr output
#+BEGIN_EXAMPLE
]0;root@myhadoopaio: /usr/local/hadooproot@myhadoopaio:/usr/local/hadoop# cat output/*
cat output/*
6	dfs.audit.logger
4	dfs.class
3	dfs.server.namenode.
2	dfs.audit.log.maxbackupindex
2	dfs.period
2	dfs.audit.log.maxfilesize
1	dfsmetrics.log
1	dfsadmin
1	dfs.servers
1	dfs.replication
1	dfs.file
1	dfs.datanode.data.dir
1	dfs.namenode.name.dir
]0;root@myhadoopaio: /usr/local/hadooproot@myhadoopaio:/usr/local/hadoop#
#+END_EXAMPLE
*** manual run
#+BEGIN_EXAMPLE
]0;root@myhadoopaio: /usr/local/hadooproot@myhadoopaio:/usr/local/hadoop# grep -RE 'dfs[a-z.]+' etc/hadoop
grep -RE 'dfs[a-z.]+' etc/hadoop
etc/hadoop/hadoop-metrics.properties:dfs.class=org.apache.hadoop.metrics.spi.NullContext
etc/hadoop/hadoop-metrics.properties:#dfs.class=org.apache.hadoop.metrics.file.FileContext
etc/hadoop/hadoop-metrics.properties:#dfs.period=10
etc/hadoop/hadoop-metrics.properties:#dfs.fileName=/tmp/dfsmetrics.log
etc/hadoop/hadoop-metrics.properties:# dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext
etc/hadoop/hadoop-metrics.properties:# dfs.class=org.apache.hadoop.metrics.ganglia.GangliaContext31
etc/hadoop/hadoop-metrics.properties:# dfs.period=10
etc/hadoop/hadoop-metrics.properties:# dfs.servers=localhost:8649
etc/hadoop/hadoop-policy.xml:    dfsadmin and mradmin commands to refresh the security policy in-effect.
etc/hadoop/hadoop-env.cmd:set HADOOP_NAMENODE_OPTS=-Dhadoop.security.logger=%HADOOP_SECURITY_LOGGER% -Dhdfs.audit.logger=%HDFS_AUDIT_LOGGER% %HADOOP_NAMENODE_OPTS%
etc/hadoop/hadoop-env.cmd:set HADOOP_SECONDARYNAMENODE_OPTS=-Dhadoop.security.logger=%HADOOP_SECURITY_LOGGER% -Dhdfs.audit.logger=%HDFS_AUDIT_LOGGER% %HADOOP_SECONDARYNAMENODE_OPTS%
etc/hadoop/hdfs-site.xml:   <name>dfs.replication</name>
etc/hadoop/hdfs-site.xml:   <name>dfs.namenode.name.dir</name>
etc/hadoop/hdfs-site.xml:   <name>dfs.datanode.data.dir</name>
etc/hadoop/log4j.properties:hdfs.audit.logger=INFO,NullAppender
etc/hadoop/log4j.properties:hdfs.audit.log.maxfilesize=256MB
etc/hadoop/log4j.properties:hdfs.audit.log.maxbackupindex=20
etc/hadoop/log4j.properties:log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}
etc/hadoop/log4j.properties:log4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false
etc/hadoop/log4j.properties:log4j.appender.RFAAUDIT.MaxFileSize=${hdfs.audit.log.maxfilesize}
etc/hadoop/log4j.properties:log4j.appender.RFAAUDIT.MaxBackupIndex=${hdfs.audit.log.maxbackupindex}
etc/hadoop/log4j.properties:#log4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=DEBUG
etc/hadoop/hadoop-env.sh:export HADOOP_NAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS"
etc/hadoop/hadoop-env.sh:export HADOOP_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_SECONDARYNAMENODE_OPTS"
#+END_EXAMPLE
** DONE Environment of Hadoop Daemons
  CLOSED: [2016-04-18 Mon 21:50]
http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/ClusterSetup.html
| Daemon                        | Environment Variable          |
|-------------------------------+-------------------------------|
| NameNode                      | HADOOP_NAMENODE_OPTS          |
| DataNode                      | HADOOP_DATANODE_OPTS          |
| Secondary NameNode            | HADOOP_SECONDARYNAMENODE_OPTS |
| ResourceManager               | YARN_RESOURCEMANAGER_OPTS     |
| NodeManager                   | YARN_NODEMANAGER_OPTS         |
| WebAppProxy                   | YARN_PROXYSERVER_OPTS         |
| Map Reduce Job History Server | HADOOP_JOB_HISTORYSERVER_OPTS |
** DONE Hadoop modules
  CLOSED: [2016-04-19 Tue 07:00]
http://hadoop.apache.org

The project includes these modules:

Hadoop Common: The common utilities that support the other Hadoop modules.
Hadoop Distributed File System (HDFS™): A distributed file system that provides high-throughput access to application data.
Hadoop YARN: A framework for job scheduling and cluster resource management.
Hadoop MapReduce: A YARN-based system for parallel processing of large data sets.
Other Hadoop-related projects at Apache include:

Ambari™: A web-based tool for provisioning, managing, and monitoring Apache Hadoop clusters which includes support for Hadoop HDFS, Hadoop MapReduce, Hive, HCatalog, HBase, ZooKeeper, Oozie, Pig and Sqoop. Ambari also provides a dashboard for viewing cluster health such as heatmaps and ability to view MapReduce, Pig and Hive applications visually alongwith features to diagnose their performance characteristics in a user-friendly manner.
Avro™: A data serialization system.
Cassandra™: A scalable multi-master database with no single points of failure.
Chukwa™: A data collection system for managing large distributed systems.
HBase™: A scalable, distributed database that supports structured data storage for large tables.
Hive™: A data warehouse infrastructure that provides data summarization and ad hoc querying.
Mahout™: A Scalable machine learning and data mining library.
Pig™: A high-level data-flow language and execution framework for parallel computation.
Spark™: A fast and general compute engine for Hadoop data. Spark provides a simple and expressive programming model that supports a wide range of applications, including ETL, machine learning, stream processing, and graph computation.
Tez™: A generalized data-flow programming framework, built on Hadoop YARN, which provides a powerful and flexible engine to execute an arbitrary DAG of tasks to process data for both batch and interactive use-cases. Tez is being adopted by Hive™, Pig™ and other frameworks in the Hadoop ecosystem, and also by other commercial software (e.g. ETL tools), to replace Hadoop™ MapReduce as the underlying execution engine.
ZooKeeper™: A high-performance coordination service for distributed applications.
** DONE get hadoop version
  CLOSED: [2016-04-18 Mon 18:28]
#+BEGIN_EXAMPLE
root@myhadoopaio:/usr/local/hadoop/bin# /usr/local/hadoop/bin/hadoop version
/usr/local/hadoop/bin/hadoop version
Hadoop 2.7.0
Subversion https://git-wip-us.apache.org/repos/asf/hadoop.git -r d4c8d4d4d203c934e8074b31289a28724c0842cf
Compiled by jenkins on 2015-04-10T18:40Z
Compiled with protoc 2.5.0
From source with checksum a9e90912c37a35c3195d23951fd18f
This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.0.jar
#+END_EXAMPLE
** DONE customize: change namenode data path
  CLOSED: [2016-04-21 Thu 19:49]
http://stackoverflow.com/questions/27271970/hadoop-hdfs-name-is-in-an-inconsistent-state-storage-directoryhadoop-hdfs-data
http://stackoverflow.com/questions/16713011/hadoop-namenode-is-not-starting-up

cd /etc/hadoop/conf

cat > hdfs-site.xml<<EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- generated by Chef for ccccce86c70d, changes will be overwritten -->

<configuration>
  <property>
    <name>dfs.data.dir</name>
    <value>/app/hadoop/tmp/dfs/name/data</value>
    <final>true</final>
  </property>
  <property>
    <name>dfs.name.dir</name>
    <value>/app/hadoop/tmp/dfs/name</value>
    <final>true</final>
  </property>
  <property>
    <name>dfs.datanode.max.transfer.threads</name>
    <value>4096</value>
  </property>
</configuration>
EOF
** DONE dfs commands
  CLOSED: [2016-04-21 Thu 22:11]
bin/hdfs dfs -help

su hdfs

| Name  | Command                                        |
|-------+------------------------------------------------|
| mkdir | bin/hdfs dfs -mkdir /user/denny                |
| ls    | bin/hdfs dfs -ls /user                         |
| touch | bin/hdfs dfs -touchz /user/denny/abc           |
| put   | bin/hdfs dfs -put /etc/hosts /user/denny/hosts |
| tail  | bin/hdfs dfs -tail /user/denny/hosts           |
** Hadoop versions
http://drcos.boudnik.org/2014/03/hadoop-genealogy-continued-4.html

[[file:/Users/mac/baidu/百度云同步盘/private_data/emacs_stuff/images/hadoop-vers.png]]
** hadoop deployment consideration
http://arkzoyd.com/hadoop4dba3/

- You can separate the Name Node and Resource Manager from the other
  slave servers. However, in order to have a large-enough
  configuration, Data Nodes and Node Managers are running on every
  server
** DONE hadoop YARN: split up resource management and job scheduling/monitoring into separate daemons.
  CLOSED: [2016-04-26 Tue 11:50]
http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html

The idea is to have a global ResourceManager (RM) and per-application ApplicationMaster (AM).
An application is either a single job or a DAG of jobs.

ResourceManager, NodeManager, ApplicationMaster
** DONE hadoop-mapreduce-historyserver: fail to start: folder permission issue
  CLOSED: [2016-04-26 Tue 22:04]
hdfs dfs -mkdir -p /tmp/hadoop-yarn/staging/history/done
hdfs dfs -mkdir -p /tmp/hadoop-yarn/staging/history/done_intermediate

root@kitchen-hadoop-3nodes-node2:/var/log/hadoop/mapreduce# tail -n 150 mapred-mapred-historyserver-kitchen-hadoop-3nodes-node2.out
...
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=mapred, access=WRITE, inode="/tmp/hadoop-yarn/staging/history/done":hdfs:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:319)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:292)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:213)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1771)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1755)
...
** DONE hdp hadoop jps: process information unavailable
  CLOSED: [2016-04-27 Wed 09:28]
sudo -u hdfs jps | grep -v Jps
sudo -u yarn jps | grep -v Jps
sudo -u zookeeper jps | grep -v Jps
sudo -u mapred jps | grep -v Jps
sudo -u oozie jps | grep -v Jps
sudo -u hive jps | grep -v Jps
sudo -u hbase jps | grep -v Jps

#+BEGIN_EXAMPLE
root@kitchen-hadoop-3nodes-node2:/# jps
2976 Jps
1865 -- process information unavailable
1723 -- process information unavailable
2844 -- process information unavailable
1966 -- process information unavailable
894 -- process information unavailable
#+END_EXAMPLE
** DONE [#A] HDP Components
  CLOSED: [2016-04-27 Wed 11:41]
http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.0-Win/bk_HDP_Install_Win/content/iug_hdp_components.html
#+BEGIN_EXAMPLE
The Hortonworks Data Platform consists of three layers:

Core Hadoop 2: The basic components of Apache Hadoop version 2.x.

Hadoop Distributed File System (HDFS): A special purpose file system designed to provide high-throughput access to data in a highly distributed environment.

YARN: A resource negotiator for managing high volume distributed data processing. Previously part of the first version of MapReduce.

MapReduce 2 (MR2): A set of client libraries for computation using the MapReduce programming paradigm and a History Server for logging job and task information. Previously part of the first version of MapReduce.

Essential Hadoop: A set of Apache components designed to ease working with Core Hadoop.

Apache Pig: A platform for creating higher level data flow programs that can be compiled into sequences of MapReduce programs, using Pig Latin, the platform's native language.

Apache Hive: A tool for creating higher level SQL-like queries using HiveQL, the tool's native language, that can be compiled into sequences of MapReduce programs.

Apache HCatalog: A metadata abstraction layer that insulates users and scripts from how and where data is physically stored.

WebHCat (Templeton): A component that provides a set of REST-like APIs for HCatalog and related Hadoop components.

Apache HBase: A distributed, column-oriented database that provides the ability to access and manipulate data randomly in the context of the large blocks that make up HDFS.

Apache ZooKeeper: A centralized tool for providing services to highly distributed systems. ZooKeeper is necessary for HBase installations.

Supporting Components: A set of components that allow you to monitor your Hadoop installation and to connect Hadoop with your larger compute environment.

Apache Oozie: A server based workflow engine optimized for running workflows that execute Hadoop jobs.

Apache Sqoop: A component that provides a mechanism for moving data between HDFS and external structured datastores. Can be integrated with Oozie workflows.

Apache Flume: A log aggregator. This component must be installed manually.

Apache Mahout: A scalable machine learning library that implements several different approaches to machine learning.

Apache Knox: A REST API gateway for interacting with Apache Hadoop clusters. The gateway provides a single access point for REST interactions with Hadoop clusters.

Apache Storm: A distributed, real-time computation system for processing large volumes of data.

Apache Spark: An in-memory data processing engine with access to development APIs to enable rapid execution of streaming, machine learning or SQL workloads requiring iterative access to datasets.

Apache Phoenix: A relational database layer on top of Apache HBase.

Apache Tez: An extensible framework for building high performance batch and interactive data processing applications, coordinated by YARN in Apache Hadoop. For additional information, see the Hortonworks website.

Apache Falcon: A framework for simplifying and orchestrating data management and pipeline processing in Apache Hadoop. For additional information, see the Hortonworks website.

Apache Ranger: The Hadoop cluster security component. Range provides centralized security policy administration for authorization, auditing, and data protection requirements.

Apache DataFu: A library for user defined functions for common data analysis task.

Apache Slider: A YARN-based framework to deploy and manage long running or always-on data access applications.
#+END_EXAMPLE
** [#A] HDP Documentation Version 2.4: http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/index.html
** DONE hadoop tcp ports: http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_HDP_Reference_Guide/content/reference_chap2.html
   CLOSED: [2016-04-27 Wed 16:43]
** TODO enable hbase feature: http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_installing_manually_book/content/ch_installing_hbase_chapter.html
http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_importing_data_into_hbase_guide/content/ch_importing_data_into_hbase_chapter.html
** TODO enable secondary namenode
http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_installing_manually_book/content/create_directories.html
** TODO [#A] enable Apache hive: http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_installing_manually_book/content/ch_installing_hive_hcat_chapter.html
** #  --8<-------------------------- separator ------------------------>8--
** TODO hadoop update region
  :PROPERTIES:
  :ID:       D80D46B0-6674-4D7C-A716-07C7645CB882
  :END:
http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html
http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CommandsManual.html
http://stackoverflow.com/questions/9104444/updating-a-hadoop-hdfs-file
http://hadoop.apache.org/docs/current/
** HBase - Hadoop Database是Google Bigtable的开源实现
*** HBase -- 建在HDFS上的NoSql DB
**** Who use HBase
   Adobe - 內部使用 (Structure data)
   Kalooga - 圖片搜尋引擎 http://www.kalooga.com/
   Meetup - 社群聚會網站 http://www.meetup.com/
   Streamy - 成功從 MySQL 移轉到 Hbase http://www.streamy.com/
   Trend Micro - 雲端掃毒架構 http://trendmicro.com/
   Yahoo! - 儲存文件 fingerprint 避免重複 http://www.yahoo.com/
   More - http://wiki.apache.org/hadoop/Hbase/PoweredBy
**** HBase benefits than RDBMS
- No real indexes
- Automatic partitioning
- Scale linearly and automatically with new nodes
- Commodity hardware
- Fault tolerance
- Batch processing
**** # --8<-------------------------- §separator§ ------------------------>8--
**** TODO HMaster
**** TODO RegionServer主要负责响应用户I/O请求,向HDFS文件系统中读写数据,是HBase中最核心的模块.
HRegionServer也会把自己以Ephemeral方式注册到Zookeeper中,使得HMaster可以随时感知到各个HRegionServer的健康状态.
**** TODO HStore
每个HStore对应了Table中的一个Column Family的存储,可以看出每个Column Family其实就是一个集中的存储单元
** TODO how to upgrade hadoop
http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_upgrading_hdp_manually/content/ch_upgrade_2_2.html

* Hadoop HBase                                           :noexport:IMPORTANT:

- commands
| Item                                                  | Summary                                                         |
|-------------------------------------------------------+-----------------------------------------------------------------|
| create 'test', 'cf'                                   | Create a table named test with a single column family named cf. |
| list 'test'                                           |                                                                 |
| put 'test', 'row1', 'cf:a', 'value1'                  | insert is at row1, column cf:a with a value of value1           |
| scan 'test'                                           |                                                                 |
| get 'test', 'row1'                                    |                                                                 |
| disable 'test'                                        |                                                                 |
| drop 'test'                                           |                                                                 |
|-------------------------------------------------------+-----------------------------------------------------------------|
| ./bin/hbase org.jruby.Main ./bin/get-active-master.rb | Run jruby script                                                |

- port: hbase-0.94.1/src/main/resources/hbase-default.xml
http://hbase.apache.org/book/config.files.html
|  Port | Summary                                                                          |
|-------+----------------------------------------------------------------------------------|
|  2181 | hbase.zookeeper.property.clientPort, The port at which the clients will connect. |
| 60010 | hbase.master.info.port, The port for the HBase Master web UI.                    |
| 60030 | hbase.regionserver.info.port, The port for the HBase RegionServer web UI         |
| 39321 |                                                                                  |
| 57063 |                                                                                  |

- The heirarchy of objects in hbase
http://hbase.apache.org/book/regions.arch.html
| Name      | Summary                                                                |
|-----------+------------------------------------------------------------------------|
| Table     | HBase table                                                            |
| Region    | Regions for the table                                                  |
| Store     | Store per ColumnFamily for each Region for the table                   |
| MemStore  | MemStore for each Store for each Region for the table                  |
| StoreFile | StoreFiles for each Store for each Region for the table                |
| Block     | Blocks within a StoreFile within a Store for each Region for the table |

- Data structure
| Name          | Summary                                                                                              |
|---------------+------------------------------------------------------------------------------------------------------|
| Column Family | Physically, all column family members are stored together on the filesystem.                         |
| -ROOT-        | keeps track of where the .META. table is                                                             |
| .META.        | keeps a list of all regions in the system.                                                           |
| Region server | It is responsible for serving and managing regions.                                                  |
| Master server | It monitors all RegionServer instances in the cluster, and is the interface for all metadata changes |
** http://hbase.apache.org/book/regions.arch.html
** http://hbase.apache.org/book/arch.bulk.load.html
** [#B] http://hbase.apache.org/book/ops_mgt.html
** [#A] [question] all update requests are sync write in hbase?   :IMPORTANT:
** [#A] [question] deploy hbase in a pseduo-distributed mode      :IMPORTANT:
   http://hbase.apache.org/book/standalone_dist.html

   http://hbase.apache.org/book/configuration.html#basic.prerequisites
Distributed modes require an instance of the Hadoop Distributed File System (HDFS).

HBase will lose data unless it is running on an HDFS that has a durable sync implementation.
** [question] How is hbase region's loadbalance calculated?
   http://hbase.apache.org/book/master.html#master.processes.loadbalancer
** [question] estimation for resource(memory, CPU, and disk) consumption of hbase
** [#C] [question] data recovery mechanism for hbase
** [question] why hbase server is listening on port 2181, which is also zookeeper communication port?
** # --8<-------------------------- separator ------------------------>8--
** basic use
*** installation of HBase
   sudo apt-get install openjdk-6-jdk

- hbase-site.xml
#+begin_example
<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>file:///home/denny/hbase</value>
  </property>
</configuration>
#+end_example
- hbase-env.sh
export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64
- /etc/hosts
127.0.1.1 denny-Vostro-1014 --> 127.0.0.1 denny-Vostro-1014
*** HBase hello world
   http://hbase.apache.org/book/quickstart.html
#+begin_example
Connect to your running HBase via the shell.

$ ./bin/hbase shell
HBase Shell; enter 'help<RETURN>' for list of supported commands.
Type "exit<RETURN>" to leave the HBase Shell
Version: 0.90.0, r1001068, Fri Sep 24 13:55:42 PDT 2010

hbase(main):001:0>
Type help and then <RETURN> to see a listing of shell commands and options. Browse at least the paragraphs at the end of the help emission for the gist of how variables and command arguments are entered into the HBase shell; in particular note how table names, rows, and columns, etc., must be quoted.

Create a table named test with a single column family named cf. Verify its creation by listing all tables and then insert some values.

hbase(main):003:0> create 'test', 'cf'
0 row(s) in 1.2200 seconds
hbase(main):003:0> list 'test'
..
1 row(s) in 0.0550 seconds
hbase(main):004:0> put 'test', 'row1', 'cf:a', 'value1'
0 row(s) in 0.0560 seconds
hbase(main):005:0> put 'test', 'row2', 'cf:b', 'value2'
0 row(s) in 0.0370 seconds
hbase(main):006:0> put 'test', 'row3', 'cf:c', 'value3'
0 row(s) in 0.0450 seconds
Above we inserted 3 values, one at a time. The first insert is at row1, column cf:a with a value of value1. Columns in HBase are comprised of a column family prefix -- cf in this example -- followed by a colon and then a column qualifier suffix (a in this case).

Verify the data insert by running a scan of the table as follows

hbase(main):007:0> scan 'test'
ROW        COLUMN+CELL
row1       column=cf:a, timestamp=1288380727188, value=value1
row2       column=cf:b, timestamp=1288380738440, value=value2
row3       column=cf:c, timestamp=1288380747365, value=value3
3 row(s) in 0.0590 seconds
Get a single row

hbase(main):008:0> get 'test', 'row1'
COLUMN      CELL
cf:a        timestamp=1288380727188, value=value1
1 row(s) in 0.0400 seconds
Now, disable and drop your table. This will clean up all done above.

hbase(main):012:0> disable 'test'
0 row(s) in 1.0930 seconds
hbase(main):013:0> drop 'test'
0 row(s) in 0.0770 seconds
Exit the shell by typing exit.

hbase(main):014:0> exit
#+end_example
** DONE
*** [#B] HBase expects the loopback IP address to be 127.0.0.1.   :IMPORTANT:
   http://hbase.apache.org/book.html#loopback.ip
   http://blog.nemccarthy.me/?p=110
   http://stackoverflow.com/questions/6007725/hbase-error-assignment-of-root-failure
#+begin_example
HBase expects the loopback IP address to be 127.0.0.1. Ubuntu and some other distributions, for example, will default to 127.0.1.1 and this will cause problems for you.

/etc/hosts should look something like this:

            127.0.0.1 localhost
            127.0.0.1 ubuntu.ubuntu-domain ubuntu
#+end_example
** hbase configuration
   http://hbase.apache.org/book/perf.configurations.html
** TODO Hbase类似于svn,有版本控制
   :PROPERTIES:
   :ID:       8F02472F-F87E-4C43-958A-A1AA47E445E5
   :END:
** TODO Hbase类似于Membase
   :PROPERTIES:
   :ID:       51958AF2-1D6B-41D0-B493-14210F7CEF17
   :END:
** TODO Membase集中了Hbase和cassendra的优点
   :PROPERTIES:
   :ID:       6CF89FA8-AA90-4518-9FE2-682365214B8B
   :END:
** useful link
http://hbase.apache.org/book/book.html
* Grails                                                           :noexport:
| Name                              | Summary |
|-----------------------------------+---------|
| gvm install grails 2.2.0          |         |
| source /root/.gvm/bin/gvm-init.sh |         |
** DONE Use gvm (Groovy enVironment Manager) to install grails
  CLOSED: [2014-08-07 Thu 14:09]
https://www.digitalocean.com/community/tutorials/how-to-install-grails-on-an-ubuntu-12-04-vps

curl -s get.gvmtool.net | bash

source "/home/username/.gvm/bin/gvm-init.sh"

gvm install grails 2.2.0
** DONE Ubuntu upgrade grails to 2.2.4
  CLOSED: [2014-08-07 Thu 15:35]
https://grails.org/download
wget  http://dist.springframework.org.s3.amazonaws.com/release/GRAILS/grails-2.2.4.zip
export RAILS_HOME=/XXX/XX
export PATH="$PATH:$RAILS_HOME/bin"
*** useful link
https://gist.github.com/marcoVermeulen/901884
* LevelDB is a fast key-value storage library                      :noexport:
  http://code.google.com/p/leveldb/\\

  leveldb - A fast and lightweight key/value database library by Google. - Google Project Hosting
** python leveldb
http://code.google.com/p/leveldb/issues/detail?id=72
All of the error messages seem to be related to things not
in the leveldb distribution (either snappy or py-leveldb
issues).
*** 安装snappy
http://code.google.com/p/snappy/downloads/list
*** easy_install leveldb
*** AttributeError: 'module' object has no attribute 'LevelDB'
http://code.google.com/p/py-leveldb/issues/detail?id=20
#+begin_example
4 ># python
Python 2.7.3 (default, Apr  8 2013, 16:15:27)
[GCC 4.1.2 20080704 (Red Hat 4.1.2-50)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import leveldb
>>> db = leveldb.LevelDB('./db')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'module' object has no attribute 'LevelDB'
>>>
#+end_example
*** ImportError: libsnappy.so.1: cannot open shared object file: No such file or directory
#+begin_example
10 >#python ./test.py
Traceback (most recent call last):
  File "./test.py", line 1, in <module>
    import leveldb
ImportError: libsnappy.so.1: cannot open shared object file: No such file or directory
#+end_example

#+begin_example
[root@datacenter03.shopex.cn]  eth0 = 192.168.65.248
[12:09:48] PWD => ~
4 >#export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH

[root@datacenter03.shopex.cn]  eth0 = 192.168.65.248
[12:10:05] PWD => ~
5 >#python
Python 2.7.3 (default, Apr  8 2013, 16:15:27)
[GCC 4.1.2 20080704 (Red Hat 4.1.2-50)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import leveldb
>>> db = leveldb.LevelDB('./db')
>>>
>>> # single put
... db.Put('hello', 'world')
>>> print db.Get('hello')
#+end_example
** basic use
http://www.cnblogs.com/haippy/archive/2011/12/04/2276064.html

- LevelDb比较适合写操作多于读操作的应用场合

- LevelDb支持数据快照（snapshot）功能,使得读取操作不受写操作影响,可以在读操作过程中始终看到一致的数据

- 同level中SSTable文件,不仅是块内有序的,也是块间有序的. 如果是非level 0的话,那么该level的文件之间key是不重叠的

- leveldb的存储组织结构
| Name               | Comment                                                                                        |
|--------------------+------------------------------------------------------------------------------------------------|
| immutable memtable | 它的设计是为了避免memtable占用的内存太大                                                       |
| Memtable           |                                                                                                |
|--------------------+------------------------------------------------------------------------------------------------|
| Manifest           | 它记载了SSTable各个文件的管理信息,比如属于哪个Level,文件名称叫啥,最小key和最大key各自是多少 |
| Current文件        | 记载当前的manifest文件名.随着compaction, SStable文件会发生变化,从而manifest文件也会跟着变化  |
| Log文件            |                                                                                                |
| SStable            |                                                                                                |

- leveldb的Compaction操作
| Name             | Comment                                                                                      |
|------------------+----------------------------------------------------------------------------------------------|
| Minor compaction | 简单地将Immutable Memtable按key的大小顺序,导出到level0的sstable文件里                       |
| major compaction | 当某个level下的SSTable文件数目超过一定设置值后,会将本level的一个文件与上一级的level文件合并 |

- Memtable的核心数据结构是一个SkipList
** [question] leveldb有dameon server吗
** [question] 文件的删除是lazy delete, 但最终是如何hard delete的呢
** [question] leveldb在从SSTable文件中读到老数据后,是否会将其加入到MemTable中?
** [question] major compaction在多路归并时,　如何防止某些SSTable文件太大,而某些太小?
** [question] major compaction在多路归并时,　插入数据时,是否涉及到大量数据的移动?
** [question] current文件真有存在的必要吗:修改内容后,mv一下不就可以了吗？
** [question] leveldb真的支持versioning的功能吗?那老数据如何删除呢?
** [question] 有无可能把leveldb包装成一个高可用的kv db
** 已经完结
*** DONE [question] 一级一级的level如何界定的,都存哪些东西
   CLOSED: [2012-11-02 Fri 18:10]
http://www.cnblogs.com/haippy/archive/2011/12/04/2276064.html
#+begin_example
当某个level下的SSTable文件数目超过一定设置值后,levelDb会从这个level的
SSTable中选择一个文件（level>0）,将其和高一层级的level+1的SSTable文件
合并,这就是major compaction.
#+end_example
*** 文件在不断修改时,整体垃圾数据不就会越增越大吗: major compaction时,有可能会丢弃老的数据,从而实现一定程度的垃圾回收
http://www.cnblogs.com/haippy/archive/2011/12/04/2276064.html
#+begin_example
　　Major compaction的过程如下:对多个文件采用多路归并排序的方式,依次找出其中最小的Key记录,也就是
对多个文件中的所有记录重新进行排序.之后采取一定的标准判断这个Key是否还需要保存,如果判断没有保存价
值,那么直接抛掉,如果觉得还需要继续保存,那么就将其写入level L+1层中新生成的一个SSTable文件中.就
这样对KV数据一一处理,形成了一系列新的L+1层数据文件,之前的L层文件和L+1层参与compaction 的文件数据
此时已经没有意义了,所以全部删除.这样就完成了L层和L+1层文件记录的合并过程.

　　那么在major compaction过程中,判断一个KV记录是否抛弃的标准是什么呢？其中一个标准是:对于某个key
来说,如果在小于L层中存在这个Key,那么这个KV在major compaction过程中可以抛掉.因为我们前面分析过,
对于层级低于L的文件中如果存在同一Key的记录,那么说明对于Key来说,有更新鲜的Value存在,那么过去的
Value就等于没有意义了,所以可以删除.

#+end_example
*** leveldb的重启点: 实现key的压缩存储
http://www.cnblogs.com/haippy/archive/2011/12/04/2276064.html#cnblogs_post_body
#+begin_example
"重启点"是干什么的呢？我们一再强调,Block内容里的KV记录是按照Key大小有
序的,这样的话,相邻的两条记录很可能Key部分存在重叠,比如key i="the
Car",Key i+1="the color",那么两者存在重叠部分"the c",为了减少Key的存
储量,Key i+1可以只存储和上一条Key不同的部分"olor",两者的共同部分从
Key i中可以获得.记录的Key在Block内容部分就是这么存储的,主要目的是减少
存储开销."重启点"的意思是:在这条记录开始,不再采取只记载不同的Key部分,
而是重新记录所有的Key值,假设Key i+1是一个重启点,那么Key里面会完整存储
"the color",而不是采用简略的"olor"方式.Block尾部就是指出哪些记录是这
些重启点的.

#+end_example
* TODO [#A] Chef hadoop cluster                                    :noexport:
** DONE Setup Password-less SSH for all servers involved
   CLOSED: [2016-04-26 Tue 09:44]
https://letsdobigdata.wordpress.com/2014/01/13/setting-up-hadoop-1-2-1-multi-node-cluster-on-amazon-ec2-part-2/
** DONE ~/.profile- HADOOP_HOME, HADOOP_CONF_DIR,
  CLOSED: [2016-04-26 Tue 09:44]
http://insightdataengineering.com/blog/hadoopdevops/

export HADOOP_CONF=/home/ubuntu/hadoop/conf
export HADOOP_PREFIX=/home/ubuntu/hadoop
export HADOOP_HOME
** DONE [#A] node awareness: /etc/hosts for 3 nodes: ssh key issue: http://104.131.129.100:48080/job/DennyDockerDeployCookbooks/25/console
   CLOSED: [2016-04-26 Tue 16:02]
** DONE [#B] digitalocean: listen on hostsname, but /etc/hosts binds hostname to 127.0.0.1
   CLOSED: [2016-04-28 Thu 10:47]
** DONE timing issue: format namenode and start services in sequence
   CLOSED: [2016-04-28 Thu 11:56]
** DONE after deployment, nodemanager not up when the first time; when manually restart, it's fine
   CLOSED: [2016-04-28 Thu 11:56]
* [#A] atlassian                                                   :noexport:
- JIRA
| Name                           | Summary                                             |
|--------------------------------+-----------------------------------------------------|
| JIRA location                  | /opt/atlassian/jira                                 |
| Default location for JIRA data | /var/atlassian/application-data/jira                |
| JIRA port                      | 8080                                                |
|--------------------------------+-----------------------------------------------------|
| jira export                    | /var/atlassian/application-data/jira/export/        |
| jira import                    | /var/atlassian/application-data/jira/import/        |
|--------------------------------+-----------------------------------------------------|
| cofluence export               | /var/atlassian/application-data/confluence/backups/ |
| cofluence import               | /var/atlassian/application-data/confluence/restore/ |
|--------------------------------+-----------------------------------------------------|
| stop jira                      | cd /opt/atlassian/jira/bin; ./shutdown.sh           |
| start jira                     | cd /opt/atlassian/jira/bin; ./startup.sh            |
|--------------------------------+-----------------------------------------------------|
| jira daily backup              | /var/atlassian/application-data/jira/export         |

- WIKi
| Name                                 | Summary                                             |
|--------------------------------------+-----------------------------------------------------|
| CONFLUENCE location                  | /opt/atlassian/confluence                           |
| Default location for CONFLUENCE data | /var/atlassian/application-data/confluence          |
| CONFLUENCE port                      | HTTP: 8090, Control: 8000                           |
|--------------------------------------+-----------------------------------------------------|
| stop confluence                      | cd /opt/atlassian/confluence/bin; ./shutdown.sh     |
| start confluence                     | cd /opt/atlassian/confluence/bin; ./startup.sh      |
|--------------------------------------+-----------------------------------------------------|
| tomcat logfile                       | tail -f /opt/atlassian/confluence/logs/catalina.out |
|--------------------------------------+-----------------------------------------------------|
| confluence lib                       | cd /opt/atlassian/confluence/confluence/WEB-INF/lib |
| confluence daily backup              | /var/atlassian/application-data/confluence/backups  |

/var/atlassian/application-data/confluence/confluence.cfg.xml
** [#A] JIRA
https://confluence.atlassian.com/jira/installing-jira-on-linux-191501165.html

docker run -t -d --privileged -h localatlassian --name local-atlassian -p 5122:22 -p 8080:8080 -p 3306:3306 denny/atlassian:latest /usr/sbin/sshd -D
docker exec -it local-atlassian bash
cd /opt/atlassian/jira/bin; ./shutdown.sh
service mysql start
service mysql status
cd /opt/atlassian/jira/bin; ./startup.sh

ssh -i /home/denny/denny -N -p 10040 -f root@123.56.44.213 -L *:8080:localhost:8080 -n /bin/bash
http://jira.jinganiam.com:8080
*** TODO [#A] JIRA的破解: 10个用户每月交10刀; SaaS 5用户以内免费
http://bbs.51testing.com/thread-1021221-1-1.html
#+BEGIN_EXAMPLE
Description=JIRA: Commercial,
CreationDate=2015-12-03,
jira.LicenseEdition=ENTERPRISE,
Evaluation=false,
jira.LicenseTypeName=COMMERCIAL,
jira.active=true,
licenseVersion=2,
MaintenanceExpiryDate=2099-12-31,
Organisation=MyTest,
SEN=SEN-L6912335,
ServerID=BGYF-ADWI-9YB6-34CH,
jira.NumberOfUsers=-1,
LicenseID=LIDSEN-L5325351,
LicenseExpiryDate=2099-12-31,
PurchaseDate=2015-12-04
#+END_EXAMPLE

root@localatlassian:/opt/atlassian/jira# find -name "atlassian*" | grep manager
./atlassian-jira/WEB-INF/atlassian-bundled-plugins/atlassian-universal-plugin-manager-plugin-2.19.1.jar
root@localatlassian:/opt/atlassian/jira# find -name "atlassian-extras-2.2.2.jar"
./atlassian-jira/WEB-INF/lib/atlassian-extras-2.2.2.jar
root@localatlassian:/opt/atlassian/jira# pwd
/opt/atlassian/jira
**** license file
Description=JIRA: Commercial,
CreationDate=2015-12-03,
jira.LicenseEdition=ENTERPRISE,
Evaluation=false,
jira.LicenseTypeName=COMMERCIAL,
jira.active=true,
licenseVersion=2,
MaintenanceExpiryDate=2099-12-31,
Organisation=MyTest,
SEN=SEN-L6912335,
ServerID=BGYF-ADWI-9YB6-34CH,
jira.NumberOfUsers=-1,
LicenseID=LIDSEN-L5325351,
LicenseExpiryDate=2099-12-31,
PurchaseDate=2015-12-04
**** buy: https://www.atlassian.com/purchase/product/jira-core
https://www.atlassian.com/software/jira/try

https://info.seibert-media.net/display/Atlassian/JIRA+hosting+or+self-hosting
https://community.spiceworks.com/how_to/62855-hosted-jira-in-15-minutes-for-20-per-month-on-digital-ocean-updated
**** evaluation
https://www.atlassian.com/licensing/purchase-licensing#evaluations-1
You can evaluate a product for 30 days (can be extended to a maximum of 90 days) before purchase
**** crack
http://fred-han.iteye.com/blog/1938735
http://alwaysyunwei.blog.51cto.com/3224143/1370288
http://jadethao.iteye.com/blog/1924432
http://www.cnblogs.com/upwifi/p/4492320.html
#+BEGIN_EXAMPLE
2`破解jira
  2.1. 下载破解补丁(jira_crack.zip)(http://download.csdn.net/source/2812878)
  2.2.用破解补丁中的JiraLicenseStoreImpl.class文件覆盖%JIRA_HOME%\atlassian-jira\WEB-INF\classes\com\atlassian\jira\license/JiraLicenseStoreImpl.class文件
  2.3. 用破解补丁中的Version2LicenseDecoder.class文件覆盖%JIRA_HOME%\atlassian-jira\WEB-INF\lib\atlassian-extras-2.2.2.jar包中的atlassian-extras-2.2.2.jar\com\atlassian\extras\decoder\v2\Version2LicenseDecoder.class文件
  2.4 启动JIRA,输入网址http://localhost:8080/,即可进入JIRA配置页面,
配置共分三步,在第一步的最后有输入授权码的区域,此时只要输入如下代码即可完全破解:

Java代码  收藏代码
 Description=JIRA\: COMMERCIAL
CreationDate=2012-11-30
ContactName=xxx@126.com
jira.LicenseEdition=ENTERPRISE
ContactEMail=xxx@126.com
Evaluation=false
jira.LicenseTypeName=COMMERCIAL
jira.active=true
licenseVersion=2
MaintenanceExpiryDate=2015-10-30
Organisation=haha
jira.NumberOfUsers=-1
ServerID=BL7P-KJJY-06XC-R277
LicenseID=LID
LicenseExpiryDate=2015-10-30
PurchaseDate=2012-11-30

其他选项,按需求填写,注意配置用户名`密码要记住,不然无法登陆进入.
这样jira就配置好了,现在你就可以使用了,不过现在登陆都是英文界面.
#+END_EXAMPLE
*** #  --8<-------------------------- separator ------------------------>8--
*** install JIRA
http://linoxide.com/linux-how-to/setup-jira-ubuntu-15-04/
http://blog.163.com/s_w_wang/blog/static/1716092212014726101210574/

# install jdk 8

wget http://downloads.atlassian.com/software/jira/downloads/atlassian-jira-6.4.10-x64.bin
chmod +x atlassian-jira-6.4.10-x64.bin
./atlassian-jira-6.4.10-x64.bin

# install mysql
sudo apt-get install mysql-server mysql-client libmysqlclient-dev
service mysql start
service mysql status

设置数据库MYSQL
mysql -u root -p
create database jira character set UTF8;
create user jira identified by "123456";
grant all on jira.* to "jira"@"%" identified by "123456" with grant option;
grant all on jira.* to "jira"@"localhost" identified by "123456" with grant option;
FLUSH PRIVILEGES;
quit

# restart
cd /opt/atlassian/jira/bin
./shutdown.sh
./startup.sh

ssh -4 -N -i /home/denny/denny -p 22 -f root@172.17.0.2 -L 45.55.17.65:28081:172.17.0.2:8080 -n /bin/bash
http://45.55.17.65:28081

https://confluence.atlassian.com/jira064/connecting-jira-to-mysql-720411769.html
*** more output                                                    :noexport:
#+BEGIN_EXAMPLE
root@jenkins:/#  ./atlassian-jira-6.4.10-x64.bin
Unpacking JRE ...
Starting Installer ...
Dec 03, 2015 4:30:47 AM java.util.prefs.FileSystemPreferences$1 run
INFO: Created user preferences directory.

This will install JIRA 6.4.10 on your computer.
OK [o, Enter], Cancel [c]

Choose the appropriate installation or upgrade option.
Please choose one of the following:
Express Install (use default settings) [1], Custom Install (recommended for advanced users) [2, Enter], Upgrade an existing JIRA installation [3]


Where should JIRA 6.4.10 be installed?
[/opt/atlassian/jira]

Default location for JIRA data
[/var/atlassian/application-data/jira]

Configure which ports JIRA will use.
JIRA requires two TCP ports that are not being used by any other
applications on this machine. The HTTP port is where you will access JIRA
through your browser. The Control port is used to Startup and Shutdown JIRA.
Use default ports (HTTP: 8080, Control: 8005) - Recommended [1, Enter], Set custom value for HTTP and Control ports [2]

JIRA can be run in the background.
You may choose to run JIRA as a service, which means it will start
automatically whenever the computer restarts.
Install JIRA as Service?
Yes [y, Enter], No [n]
#+END_EXAMPLE
*** DONE Could not find driver with class name: com.mysql.jdbc.Driver
   CLOSED: [2015-12-03 Thu 14:41]
http://stackoverflow.com/questions/18617108/how-to-install-com-mysql-jdbc-driver-could-not-find-driver-with-class-name-com
apt-get install libmysql-java
cp /usr/share/java/mysql-connector-java-5.1.28.jar /opt/atlassian/jira/lib/
*** DONE JIRA get tickets resolved in this week
  CLOSED: [2015-11-09 Mon 12:36]
https://answers.atlassian.com/questions/126711/how-to-search-issues-created-in-the-last-week-in-jira-issue-navigator
project = DEVOPS AND status = Done AND resolution = Fixed and updatedDate > -7d order by updatedDate desc
*** #  --8<-------------------------- separator ------------------------>8--
*** Current Open Tickets Of DevOps
https://totvslab.atlassian.net/wiki/display/MDMP/Current+Open+Tickets+Of+DevOps
project = MDM AND status in (Open, "In Progress", Reopened, "To Do") AND resolution = Unresolved AND labels = devops_mustfix ORDER BY key DESC
*** Resolved Tickets Of DevOps
https://totvslab.atlassian.net/wiki/display/MDMP/Resolved+Tickets+Of+DevOps
project = MDM AND status in (QA, Done) AND labels = devops_mustfix order by updatedDate
*** DevOps Tickets we get experience
https://totvslab.atlassian.net/wiki/display/MDMP/DevOps+Tickets+we+get+experience
project = MDM AND  labels = experience and labels = devops_mustfix  order by key desc
